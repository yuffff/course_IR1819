
    -----
    full notes below
        ppt-1-Einfurung --over --c
            was ist IR? 
                Information Retrieval (IR) is 
                    -finding material in
                    -an unstructured text , 
                    -satisfies an information needs
                    -Searches can be based large collections (usually stored on computers)
                personal::
                    everything in our daily life , around us can not be searched without this knowledge/technique
            unstructured nature (usually text)? & - ‰∏â‰∏™level
                public 
                    articles -- 
                        scientific articals 
                        from press news  
                        Webseiten
                --
                Office
                    documents (eg. Spreadsheets)
                    e mails 
                --
                personal 
                    multimedia material (z.B. Fotos und Videos)
                    user information  (z.B. on Facebook)
            information need--Informationsbed√ºrfnis des Benutzers
                Vage (z.B. Was kann man in Berlin unternehmen?)
                not Pr√§zise Anfragen in (relational en) Datenbanken
            ÊØîËæÉÔºåAbgrenzung IR und Datenbank & - 
                p31
            Gro√üe Datenmengen mit rapidem Wachstum
            Typische Anwendung des Text-Retrievals sind heute Web-Suchmaschinen
            Was ist Information? 
                what is Daten, Wissen Information &

                    1.data : coded information that computer can understand   ÊúÄÂéüÂ≠êÁöÑ‰∏Ä‰∏™Ê¶ÇÂøµ
                    2.knowledge : data with semantic ÂæàÂ§ßÁöÑ‰∏Ä‰∏™ËåÉÂõ¥
                    3.informaiton : part of knowledge , that can answer a specific question . useful knowledge  ‰ªã‰∫é‰∏≠Èó¥

                    1.DatenÔºöMaschinell verarbeitbare, kodierte Informationen ÂÖ∑ÊúâÁªìÊûÑÁöÑÊï∞ÊçÆ 1999-11-05 ÔÉ†Zeichenkettenformat: ZZZZ-ZZ-ZZ
                    2.Wissen ist Daten mit SemantikÔºöGesamtheit aller Kenntnisse eines Sachgebietes Âá†Âè•ËØùÔºåÂÖ∑ÊúâËØ≠‰πâ‰ø°ÊÅØÁöÑÊï∞ÊçÆ 
                        "I need one Euro for the car park.‚Äú, "I will
                        be home about 6.30 if I can catch the 4.54.
                    3.Information ÔºöTeilmenge des Wissens um spezielle Aufg
                        Information ist nutzbares Wissen   Ôºå Wann war ich in Berlin?
            Aufbau eines IR-Systems  Á§∫ÊÑèÂõæ p15  & 
                    DokumentverarbeitungÂåÖÂê´‰ªÄ‰πàÔºü 
                    Anfragen ÂåÖÂê´Âï• 
                    Matching und Ergebnisliste
                    Feedback
                IRÁ≥ªÁªüÁöÑÊ¶ÇÂøµÊ®°ÂûãÊòØÁ≥ªÁªüÁöÑÂü∫Êú¨ÊñπÊ≥ï
                VektorraummodellÂêëÈáèÁ©∫Èó¥Ê®°Âûã
                Probabilistisches RetrievalÊ¶ÇÁéáÊ®°Âûã
                    Wahrscheinlichkeit
            compare IR and databank &
                ‰∫î‰∏™ÊñπÈù¢
                    matching , Model , query language - query fault (miss spelling)
                    result 

                    corpus:
                        exact match ... probability accuracy ...
                        natural langu. ... structured lan. .. relevant ranking ... 
                ‰∏Ä‰∏™ÈáçÁÇπ:ÂØπË±°‰∏çÂêå
                    Êü•ËØ¢ÁöÑÊòØunstructured text and structured resources 
            Elementare Fragestellungen 
                Âü∫Á°ÄÈóÆÈ¢ò p23 &
                Informationsbed√ºrfnis?
                DokumenteÂª∫Ê®°Ôºü
                effizient?
                G√ºte?
            Anwendungsbeispiele & x2
                Bildersuche
                Suche in Open-Source-Projekten
                Stack Overflow ,Programming QA site
                Soundcloud , Musikempfehlungen
        ppt-2-Bool --over --c
            Dokument ist entweder relevant oder nicht
            Suchbegriffe k√∂nnen durch boolsche Operatoren verkn√ºpft werden ÂèØ‰ª•ËøûÊé•
            Boolean Retrieval
                the simplest model
                The search engine returns all documents that satisfy the Boolean expression
                Why is grep not the solution & p6 --
                    can not deal with negation expression 
                    can not support near operations 
                    slow on large collections

                    ÔÇß Slow (for large collections) Â§ßÈáèÊï∞ÊçÆÊó∂ÂÄôÂæàÊÖ¢
                    ÔÇß line-oriented, IR is document-oriented Èù¢ÂêëË°åÁöÑÊ£ÄÁ¥¢
                    ÔÇß can not express negation [NOT CALPURNIA] is non-trivial
                    ÔÇß do not support near operations (e.g., find the word ROMANS near COUNTRYMAN ) not feasible ÂØπ‰∫éÂÖ∂‰ªñÊìç‰ΩúÊ≤°ÊúâÊîØÊåÅ
                ÊèêÂâç‰∏∫ÊñáÊ°£Âª∫Á´ãÁ¥¢Âºï
                Term-document incidence matrix 
                    ÊòØÂê¶Âá∫Áé∞ÔºåÂá∫Áé∞Â∞±ÊòØ1 Âê¶ÂàôÊòØ0ÁöÑ‰∏Ä‰∏™Áü©Èòµ
                    Entry is 1 if term occurs 
                    Incidence vectors ‰∫ã‰ª∂ÂêëÈáè
                        Ê¶ÇÂøµ : 0/1 vector for each term
                    To answer the query with help of Term-document incident matrix Ôºü example & -
                        Do a (bitwise) AND on the three vectors
                    why not used? & - 
                        Bigger collections Can‚Äôt build the incidence matrix! **
                        needs huge storage 
                        ÊòØ‰∏Ä‰∏™Á®ÄÁñèÁü©Èòµ
                Inverted Index
                    ideaÔºöWe only record the position of 1s
                    why inverted ? Inverted Index & 
                        Normally, we want to index form docs to terms . But now , we use terms as dictionary and docID as posting list
                        ÂÄíÊéíÔºü‰∏ÄËà¨ÊòØÊñáÊ°£Á¥¢ÂºïÂà∞ÂçïËØçÁöÑÔºåËøôÈáåÊòØ‰ªéËØçÈ°πÂèçÂêëÊò†Â∞ÑÂà∞ÊñáÊ°£ÁöÑ For each term t *
                        ::For each term t, we store a list of all documents that contain t
                    postingsÔºü Á¥¢ÂºïÊù°ÁõÆ
                    what is posting list? & - p11     
                        For each term t, we store a list of all documents that contain t       
                    posting list Âª∫Á´ãËøáÁ®ã 3‰∏™Ê≠•È™§ & p14
                        ÁîªÂõæ  - 1,2,3,4   4ÁöÑÊ≠•È™§Êúâ‰∏â‰∏™
                        TokenizeÂèòÊàêËØçÊù° ËØçÊ±áÂàáÂàÜ
                        Do linguistic preprocessingÂ§ßÂ∞èÂÜôËΩ¨Êç¢‰πãÁ±ªÁöÑ
                        ÂÆû‰æã‰∏≠‰ªÖ‰ªÖÊòØÂ§ßÂ∞èÂÜôËΩ¨Êç¢ Ôºü 
                        Sort posting Ôºü ÊåâÁÖßÂ≠óÊØçÈ°∫Â∫èÊéíÂ∫è
                        Create postings lists, determine doc freq
                        Ê®™ÁùÄ‰∏â‰∏™È°πÁõÆcolumnÔºöterm doc.freq. posting lists  
                    ÂÖ∂‰ªñÁªÜËäÇÔºö
                        ‰∏§‰∏™Êñá‰ª∂Split the result into dictionary and postings file
                            dicÂíåpostingÂ¶Ç‰ΩïÂ≠òÂÇ® & -
                                ::
                                    we want to keep dictionary in memory 
                                    and store postings as files on disk
                        Size of postings much larger than size of dictionary ÔÉ† dictionary is commonly kept in memory, postings on disk
                        ÂéüÁêÜËßÑÂæãÔºöÂÖ∂ÂÆûÊñáÁ´†‰∏≠Áî®ËØçÂπ∂‰∏çÂ§öÔºå‰∫∫Á±ªÁ§æ‰ºöÂ≠¶Áé∞Ë±°
                        How much space do we need for dictionary and index? 
                        index compression: how can we efficiently store and process indexes for large collections?
                Queries 
                    ÂíåÂª∫Á´ãËøáÁ®ãÁõ∏ÂØπ
                        Intersecting two posting lists
                            For each of the terms, get its postings list, then AND them together
                            complicity:This is linear in the length of the postings lists 
                            postings lists should be sorted
                        do intersection for two postings & -
                            ÁîªÂõæ‰πãÂêéÂÜçÊèèËø∞
                            pseudocode 
                            code 
                Query Optimization
                        What is the best order for processing this query? & 
                            Start with the shortest postings list, then keep cutting further
                            more general , Process in increasing order of or sizes
                        Optimized with sort 
                                pseudocode 
                                code 
                            RecallÂõûÂøÜ  
                        using skip lists how ? &
                            we do better than this (sub-linear time)?
                                Skip pointers allow us to skip postings that will not figure in the search results
                                This makes intersecting postings lists more efficient
                            Intersection with Skip Pointers 
                                pseudocode & -
                                code 
                                For postings list of length pÔºå use Ê†πÂè∑p evenlyspaced skip pointers
                                harder in a dynamic environment because of updates
                                Â¶ÇÊûúËøá‰∫ÜÊÄé‰πàÂäûÔºü
                                NEXT
                            why skip lists are not used now &
                                1.cpus are faster 
                                2.can be slow when the list is always in changing 
                Boolean Search
                    can answer Boolean expression
                    precise: Document matches condition or notÁ≤æÁ°ÆÁöÑ
                    Primary commercial retrieval method for three decades‰∏âÂçÅÂπ¥Êù•‰∏ªË¶ÅÈááÁî®ÂïÜ‰∏öÊ£ÄÁ¥¢ÊñπÊ≥ï
                    Many professional searchers (e.g., lawyers) still like Boolean queries
                    You know exactly what you are getting
                    Many search systems you use are also Boolean: spotlight, email, intranet etc
                        email searching : when you searching an email content ,it use bool search at usual 
                    Westlaw
                        Commercially successful Boolean retrieval: Westlaw
                        Largest commercial legal search service
                        The service was started in 1975
                        ÁâπÊÄß & -
                            Proximity operators
                            1.Space is disjunction, not conjunction! This was the default in search pre-Google
                            2.Long, precise queries
                                Precision, transparency, control
                            Boolean queries return set of matching documents 
                                order the returned results, scoring function
                            Search for compounds or phrases needs to increase the number of the index
                Phrase Queries ÂØπ‰∫éËØçÁªÑÁöÑÊîØÊåÅ
                    ‚ÄúThe inventor Stanford Ovshinsky never went to university‚Äù should not be a match
                    About 10% of web queries are phrase queries
                    It is no longer suffices to store docIDs in postings lists
                    Two ways of extending the inverted index
                        Bi-word index 
                            explain & -
                                Index every consecutive pair of terms in the text as a phrase
                                Each of these bi-words is now a vocabulary term
                                add to dictionary (larger scale )
                                change the queries to AND form to do Phrase queries:: not only Two-word queries  
                            bi-words as vocabulary terms
                            A long phrase like ‚Äústanford university palo alto‚Äù 
                                --can be divided to some Bi-words with AND between them 
                            can be occasional false positives
                            but:we should do 
                                post-filtering 
                                of hits to identify subset actually 4-word phrase?
                        Extended Bi-words
                                key idea & -
                                    Bucket the terms into  Nouns (N) and Articles/prepositions (X) ***
                                    use it to match larger words , with arbitry length
                                    more efficient as Bi words 
                                Parse each document and perform partof-speech tagging
                                Classify words as nouns, verbs, etc.
                                    Now deem any string of terms of the form
                                    ÔøΩÔøΩ‚àóÔøΩ to be an extended bi-word
                            DIS:
                                very large term vocabulary   
                        why Bi words not used ? &
                            1.false positives
                            2.large term vocabulary 
                        what is Positional index &
                            use the positional informaiton to determine a phrase query 
                            commonly used 
                            a more efficient alternative to 
                            Each posting is a docID and a list of positions
                            Example in page 38: & -
                                pay attention to positon: 
                                    in the 4th doc, TO has a position of 16
                                    and BE has a positoin of 17
                                    this means that they are close to each other in doc 4
                                    and it can be used as resault 
                            Proximity search :: k word proximity search ‰∏¥ËøëÊ£ÄÁ¥¢
                                E.g. Find all documents that contain EMPLOYMENT and PLACE within 4 words of each other
                                +stop words: a the in or ....
                                ‚ÄúProximity‚Äù intersection algo 
                                    pseudocode & -
                        Combination scheme
                            how & -
                                Include frequent bi-words as vocabulary terms in the index
                                Do all other phrases by positional intersection
                            extremely frequent :: Bi-words
                            Many bi-words are extremely frequent: ‚ÄúMichael Jackson‚Äù, ‚ÄúBarack Obama‚Äù etc
                            For these bi-words, increased speed compared to positional postings intersection is substantial
                            What are ‚Äúgood‚Äù bi-words & -
                                Phrase where the 
                                    individual words are common 
                                    but::the desired phrase is comparatively rare
                                        explain as follow :
                                        Adding ‚ÄúBarack Obama‚Äù as a phrase index entry may
                                            only give a speedup factor to that query of
                                        about 3 (most documents that mention either
                                            word are valid results) whereas‚Ä¶
                                        - ‚Ä¶adding ‚ÄúThe Who‚Äù as a phrase index entry may speed up that query by a factor of 1,000
        ppt-3-Getting Term --over --c
            How to get terms out of documents
                challenges
                    General and Non-english
                        What format is it in (pdf, word, excel, html etc.)?
                        Reading direction: from left to right, right to left, column-wise
                        What character set is in use? - UTF-8, ASCII, ISO-8859-1,
                        determine lan? & -
                            use frequent words (German: der, die, das, und, ein, einer,
                            Englisch: the, a, and, or, one)
                        What is the document unit
                            Answering the question ‚Äúwhat is a document?‚Äù is not trivial 
                        Definitions  
                            Term,Morphem,Inflection & - NEXT
                            Derivation:Forming a new word from an existing word
                            Kompositum: consists of more than one stem ËØçÂπ≤
                            Noun Phrase (NP):noun as its head word
                    Tokenization problems
                        string into words or tokens
                        Needed to apply further processing, e.g. stemming and lemmatization
                        promblems for reading word? & - NEXT
                        promblems for reading words in IR? & *5
                            //
                                numbers 
                                no white space between words.
                                diff. meaning of words . eg Bush 
                            Short Forms
                            Orthographic word: string of characters with ‚Äòwhitespace‚Äô at each end;
                            Word form: have, has, had, having are word forms of the lemma have
                            One word or two or several
                                ÔÇß Hewlett-Packard
                                ÔÇß State-of-the-art
                                ÔÇß Co-education
                            Numbers
                                ÔÇß 3/20/91
                                ÔÇß 20/3/91
                                ÔÇß Mar 20, 1991
                            Apostrophes can be 
                                part of a word, 
                                a part of a possesive 
                                just a mistake
                            Capitalized words: different meanings
                            chinese:No whitespaces between words
                            chinese:Ambiguous segmentation
                            japanese:End user can express query entirely in hiragana!
                    Normalization
                        in indexed text as well as query terms into the same form
                            eg:We want to match U.S.A. and USA
                        we can 
                            do asymmetric expansion,but less efficient
                            Normalization and language detection interact
                        some Terminology & -
                            Grammatical markings
                                bakes is a (grammatically) inflected form
                            Stemming:ÊâæËØçÂπ≤
                                algorithms work by cutting off the end or the beginning of the word
                            LemmatizationÔºöËØçÂΩ¢ËøòÂéüÔºåÂçïÂ§çÊï∞ 
                                Êõ¥Âä†ÂÆåÊï¥Ôºå‰∏ç‰ºö‰ªÖ‰ªÖÂéªÊéâÂâçÁºÄ
                            Case Folding
                                Reduce all letters to lower case
                            Stop words = extremely common words
                        More Classing techÔºü
                            phonetic equivalentsÔºöBeijing and Peking
                            ThesauriÔºöSemantic equivalence, car = automobile
                            calcu Soundex & -  
                                Ê†πÊçÆËØ≠Ê≥ïËßÑÂàôÂÜô‰∏Ä‰∏≤Êï∞Â≠ó
                                ËÄåÂêéÊòØ‰∏™stepÁöÑÁº©ÂáèÂ∑•‰ΩúÔºàÊ∂àÈô§ÈáçÂ§çÔºåÔºåÔºâ
                    stemming algorithms
                        ÔÇß Table lookup approach 
                            &explain -
                            root forms and inflected forms wrote in a table 
                            Building lookup tables is very labour-intensive
                            High probability that these tables may miss out some exceptional cases
                        ÔÇß Successor Variety 
                            idea 
                                a method to do stemming .
                                find the boundaries of the words with the help of the Varieties of the suffix .
                            &calcu -
                            distribution of phonemes 
                            Successor variety of substrings of a term will decrease as more characters are added ----> boundry is reached 
                            based on Corpus to find characters following 
                            see Pic P28
                            ÊòØËØçÂπ≤ÁöÑÊ†áÂøóÔºösharply increase **
                            Example
                                ÔÇß Test Word: READABLE
                                ÔÇß Corpus: ABLE, APE, BEATABLE, FIXABLE, READ,
                                READABLE, READING, READS, RED, ROPE, RIPE
                        ÔÇß n-gram stemmers
                            based on shared unique n-grams 
                                to calculate Association measures 
                            Dice‚Äôs coefficient Dice‚Äôs coefficient (similarity)
                            &calcu -
                            ÂÆûÁé∞stemming: **
                                Once such a similarity matrix is available
                                terms are clustered using a single link clustering method
                        ÔÇß Affix Removal Stemmers
                        Porter's Stemmer
                            idea 
                                use a rule table to remove suffix in words
                            explain & -
                            ÂéüÁêÜÂæàÁÆÄÂçïÔºåÂÖ∂ÂÆûÂ∞±ÊòØ‰∏Ä‰∏™ rule table **
                            Most used in IR, probably because of its
                                balance between simplicity and accuracy
                                (1980)
                            the defacto standard algorithm used for English stemming
                            Porter extended his work by developing Snowball, a
                                framework for writing stemming algorithms 
                                http://snowball.tartarus.org
                            Rules are composed of a pattern (left) and a string (right side)
                            
                            alternate vowel-consonant sequences
                            5 phases of reduction rules applied sequentially **
                                Step 1 deals with plurals and past participles
                                Steps 2 to 5 deal with English-specific suffixes
                                ÁÆÄÂçïËÆ°ÁÆó
                                    ÈúÄË¶ÅÁü•ÈÅìCVCVÂÖ¨Âºè
                                    ÈúÄË¶ÅÁü•ÈÅìmÊï∞ÂÄºËøõË°åÂà§Êñ≠
                                    ËßÅ‰æãÈ¢ò & -
                    Lemmatization
                        to base form
                            am, are, is ‚Üí be
                        Inflectional morphology vs. derivational morphology Êõ≤ÊäòÂΩ¢ÊÄÅÂ≠¶vsÊ¥æÁîüÂΩ¢ÊÄÅÂ≠¶
                        ‰∏∫Âï•‰∏∫Ê£ÄÁ¥¢Â∏¶Êù•ÁöÑÂ•ΩÂ§ÑÈùûÂ∏∏ÊúâÈôê & - NEXT
                        vs Stemmer:: 
                            Stm just cutting affix.
                            Lemmatization adds some alphabets.
                    Stemming?or not &
                        ‰∏ÄËà¨Êù•ËØ¥ÔºåËØçÂπ≤Âåñ‰ºöÊèêÈ´òÊüê‰∫õÊü•ËØ¢ÁöÑÊïàÁéáÔºåËÄåÈôç‰ΩéÂÖ∂‰ªñÊü•ËØ¢ÁöÑÊïàÁéá
                        Benefits of stemming depend on the language Â±àÊäòËØ≠Êõ¥ÊúâÂ•ΩÂ§Ñ
                        Danger‚Äôs of stemmingÔºöÔºö[information retrieval] vs. [information on Golden Retrievers] ÈîôËØØÂæóÂ§ÑÁêÜ‰∫Ü‰∏Ä‰∫õÊÉÖÂÜµ
                            increses the probability of false positiv
                    Part-of-Speech Tagging &
                        Labeling each word 
                        Different approaches
                            ÔÇß Rule-based Tagger
                            ÔÇß Stochastic POS Tagger
                            - Simplest stochastic Tagger
                                Each word is assigned its most frequent tag 
                                Learning from examples : we can training it 
                                WSJ (Wall Street Journal) in the Penn Treebank around 1.2 Million tokens
                                Wall Street Journal corpus:p42 as a example 
                                    a conditional probability = 0.019
                                    Áªô‰∏Ä‰∏™Êù°‰ª∂Ê¶ÇÁéáË°®ÔºåËøõË°åPOS & -
                            - HMM Tagger
                    ÁºñËæëË∑ùÁ¶ª
                        https://nlp.stanford.edu/IR-book/html/htmledition/img152.png
                        ÂÜôÂá∫‰º™‰ª£Á†ÅÂ∞±ÈÉΩÊáÇ‰∫Ü & -
                            Âä®ÊÄÅËßÑÂàí‰øùËØÅÊúÄÂêéÊòØÊúÄ‰ºòÁöÑ
                            Ê®™ÂêëÂèòÂä®ÊòØdelÊìç‰ΩúÔºåÁ∫µÂêëÂèòÂä®ÊòØinsertÊìç‰Ωú
                            Ë¶Å‰ºöÊ†πÊçÆË°®Ê†ºÂÜôÂÖ∑‰ΩìÊìç‰Ωú**
                                ‰∏ÄÂÖ±ÂõõÁßçÊìç‰Ωú
                            ppt4-p21
        ppt-4-Tolerant Retrieval  --over --c
            +BG 
                ÂâçÈù¢ÁöÑÂ∑•‰ΩúÊó†Ê≥ïÊîØÊåÅetst Êü•ËØ¢
                ÂÆπÈîôÂºèÊ£ÄÁ¥¢‰πüÈúÄË¶ÅÊîØÊåÅ‰ªªÊÑèÂ§öÂ≠óÁ¨¶ÂåπÈÖç* &
                k-gram Á¥¢ÂºïÁªìÊûÑÂ¶Ç‰ΩïÊîØÊåÅÈÄöÈÖçÁ¨¶ÁöÑÊ£ÄÁ¥¢
                ËØçÂÖ∏ÁöÑÊï∞ÊçÆÁªìÊûÑ &
                    ÂèØ‰ª•ÈááÂèñÂâçÁºÄÊï∞ 
                    ‰∏∫‰∫ÜÂä†Âø´ÂØπÂ≠óÂÖ∏ÁöÑÊ£ÄÁ¥¢ÈÄüÂ∫¶
                    https://blog.csdn.net/lsjseu/article/details/12234769
                    Á§∫ÊÑèÂõæ
                Áî®Êà∑ÊúâÊó∂ÂÄôÂØπÂçïËØçÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåÂèØËÉΩ‰ºöËæìÂÖ•‰∏Ä‰∫õÈÄöÈÖçÁ¨¶Êìç‰Ωú
                    ÈááÁî®BÊ†ë
                        Â≠óÁ¨¶‰∏≤ÂâçÁºÄÊêúÁ¥¢‰∏ÄÁõ¥ÊòØBÊ†ëÁöÑ‰ºòÁÇπÔºåËÄåÂ≠óÁ¨¶‰∏≤ÂêéÁºÄÊêúÁ¥¢Ôºå
                        Êàë‰ª¨ÂèØ‰ª•ÈááÁî®ÂèçÂêëBÊ†ëÔºåÈÇ£‰πàÂØπ‰∫éËøôÁßçab*cdÔºåÈÇ£‰πàÊàëÂ∞±ÂèØ‰ª•ÈÄöËøáBÊ†ëÂíåÂèçÂêëBÊ†ëÁöÑÁªìÂêàÔºåÊúÄÂêéÂèñ‰∏Ä‰∏™‰∫§ÈõÜ
                    ËΩÆÊéíÁ¥¢ÂºïËß£ÂÜ≥ÊñπÊ°à
                        ÂØπ‰∏Ä‰∏™ÂçïËØçÂª∫Á´ãËΩÆÊéíÁ¥¢Âºï„ÄÇÂÅáÂ¶ÇÊúâhelloÔºåÊàë‰ª¨Âª∫Á´ã‰∏ãÂàóËΩÆÊéíÂçïËØçË°®ÔºåËÄÉËôëÂêÑÁßçÊÉÖÂÜµ
                        ËΩÆÊéíÁ¥¢ÂºïÁöÑÁº∫ÁÇπÂæàÊòæÁÑ∂ÔºöËØçÂÖ∏ÈõÜÂêàÂ§™Â§ß„ÄÇ
                    kgram
                        ‰∏æ‰∏™‰æãÂ≠êÔºåÂØπ‰∫é‰∏Ä‰∏™ÂçïËØçcastleÔºåÂÆÉÁöÑ3-gramÂåÖÊã¨Ôºö$ca„ÄÅcas„ÄÅast„ÄÅstlÂíåtl$„ÄÇÔºàÂú®ÂºÄÂßãÂíåÁªìÂ∞æÂ§ÑÊ∑ªÂä†$Ôºâ
                        Âú®k-gram‰∏≠ÔºåÂÖ∂ËØçÂÖ∏Áî±ËØçÊ±áË°®‰∏≠ÊâÄÊúâÂçïËØçÁöÑk-gramÂΩ¢ÂºèÁªÑÊàê„ÄÇÊúÄÂêé‰∏éÊìç‰Ωú‰∏Ä‰∏ãÔºåÊ≥®ÊÑè‰∏ãÂõæÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂêéËøáÊª§ÁöÑËøáÁ®ãÔºàÂâîÈô§‰∏Ä‰∫õ‰∏çÊª°Ë∂≥Êù°‰ª∂ÁöÑÂçïËØçÂêßÔºâ„ÄÇ
                        k-gramÁ¥¢ÂºïÂèØËÉΩ‰ºöËøîÂõûÂæàÂ§ö‰º™Ê≠£‰æãÔºåÈúÄË¶ÅÂÅöËøáÊª§Â§ÑÁêÜ„ÄÇ
                    ÊÑüËßâÊúÄÂ•ΩÈááÁî®bÊ†ë
                Áî®Êà∑ÂèØËÉΩÂ≠òÂú®ÈîôËØØËæìÂÖ•ÔºåËøôÊó∂ÔºåÊàë‰ª¨ÈúÄË¶ÅÂØπÁî®Êà∑ÁöÑËæìÂÖ•ÁªôÂá∫‰∏Ä‰∫õÊ†°Ê≠£ÁöÑÊèêÁ§∫ÊñπÊ°à
                    edit distance ::
                        idea of it :
                            measure the steps converting from one word to an other.
                            -based on inserting or deleting methods 
                    https://blog.csdn.net/lsjseu/article/details/12234769
                    Âü∫‰∫éÁºñËæëË∑ùÁ¶ªÁöÑÊ†°Ê≠£
                        ËøêÁî®DP &
                        Âú®ÈîÆÁõò‰∏äÊää‚Äúa‚ÄùÊï≤Êàê‚Äús‚ÄùÁöÑÂèØËÉΩÊÄßÂ§ß‰∫éÔºåÊää‚Äúa‚ÄùÊï≤Êàê‚Äúu‚ÄùÔºåÂõ†‰∏∫aÂíåsÈù†ÂæóÂæàËøëÔºå
                            ËøôÊ†∑Êàë‰ª¨Â∞±ÂèØ‰ª•Âú®Ê±ÇÂ≠óÁ¨¶‰∏≤ÁºñËæëË∑ùÁ¶ªÁöÑÊó∂ÂÄôÔºåÁªôÂÆö‰∏çÂêåÁöÑÊùÉÈáç„ÄÇ
                        Âõ†‰∏∫Êàë‰ª¨Ë¶ÅÂêåÂ≠óÂÖ∏ÁöÑÊØè‰∏™ÂçïËØçËøõË°åÊ±ÇÁºñËæëË∑ùÁ¶ª„ÄÇËøôÊòØÊàë‰ª¨
                            Â∞±ÂèØ‰ª•ÈÄöËøáÁªôÂÆö‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÊù°‰ª∂Êù•ÂáèÂ∞ëÂåπÈÖçÂçïËØçÁöÑ‰∏™Êï∞ÔºåÊØîÂ¶ÇÊàë‰ª¨ÂÅáËÆæÈ¶ñÂ≠óÊØçËÇØÂÆöË¶ÅÂØπ„ÄÇ
                    k-gramÁöÑÊ†°Ê≠£ÊñπÊ≥ï
                    ‰∏ä‰∏ãÊñáÊïèÊÑüÁöÑÊãºÂÜôÊäÄÊúØ
                    Âü∫‰∫éÂèëÈü≥ÊäÄÊúØÁöÑÊ†°Ê≠£
                        Âü∫‰∫éÂèëÈü≥ÁöÑÊ†°Ê≠£ÊäÄÊúØÔºàSoundexÁÆóÊ≥ïÔºâ

            ---------------------------------------------------------------------------------------
            Dictionaries
                Êï∞ÊçÆÁªìÊûÑÔºü & -
                    Some IR systems use hashes, some use trees 
                    ‰∏§ËÄÖÂ¶Ç‰ΩïÈÄâÁî®Ôºü
                    will it keep growing„ÄÇ„ÄÇ„ÄÇ
                hash:
                    Each vocabulary term is hashed into an integer
                    Lookup in a hash is faster than lookup in a tree
                    BUT:
                        No prefix, infix or suffix search 
                        no tolerant retrieval
                        Need to rehash everything periodically if vocabulary keeps growing
                tree:
                    B-trees mitigate the rebalancing problem
                    Search is slightly slower than in hashes: O(logM), only on balanced trees 
            ÈÄöÈÖçÁ¨¶Êü•ËØ¢Wildcard queries
                1.using B-tree immedately ::intersect
                    ‰∏âÁßçÊÉÖÂÜµ & -
                        Trailing wildcard query-----B-tree
                        Leading wildcard query-----inverse B-tree
                        middle ------intersect the two term sets 
                    +btree 
                        Â§öË∑ØÊêúÁ¥¢Ê†ë
                        Êµ∑ÈáèÊï∞ÊçÆÊêúÁ¥¢
                        Á£ÅÁõòÔºåÊï∞ÊçÆÂ∫ì
                    
                2.permuted index ËΩÆÊéíÁ¥¢Âºï
                    ÂéüÁêÜ & -
                        Store each of these rotations in the dictionary, in a B-tree 
                        hello--Â¢ûÂä†4‰∏™term
                        add terms to dictionary 
                    where $ is a special symbol
                    Problem : quadruples the size of the dictionary
                        compared to a regular B-tree (empirical number)
                    Permuterm index doesn‚Äôt require post-filtering
                    [hel*o] look up & -   ****
                        [o$hel*]
                        ÂÖ∑‰ΩìÊü•ËØ¢Ë¶Å‰ºöÁî®xË°®Á§∫ & -
                3.kgram 
                    More space-efficient than permuterm index
                    ‰æãÂ≠ê & -
                        Query [mon*] can now be run as [$m AND mo AND on]  ****
                        do postings-filter
                        but also many ‚Äúfalse positives‚Äù like ‚ÄúMoon‚Äù 
                        change the queries 
                    execute a large number of Boolean queries
                ÊØîËæÉ & -
                    k-gram index should do postings filter
                    permuted index tasks huge anount of spaces to store dic.
            Spelling Correction
                26%: Web queries (Wang et al. 2003)
                Correcting documents and queries
                The general philosophy in IR is: do not change the documents
                we use the the smallest distance to the misspelled word
                using :A standard dictionary (Webster‚Äôs, OED etc.)
                basic operations that convert s1 to s2
                based on Distance 
                    Edit distance 
                        The edit distance between string s1 and
                            string s2 is the minimum number of basic
                            operations that convert s1 to s2
                        +Damerau-Levenshtein 
                            includes transposition as a fourth possible operation
                        Computation & -
                        Algorithm & -
                    Weighted edit distance
                        Typewriter distanceÔºöÂü∫‰∫éÈîÆÁõò
                        Confusion Matrix for Spelling Errors p31 
                        Key Idea: we seek the string(s) in S of least edit distance from q
                    Problem:
                        Computing the edit distance from q to each string in S is inordinately expensive
                Invoke the k-gram index to assist --with low edit distance to the query
                    Âü∫‰∫ékgramÁöÑÊãºÂÜôÊ£ÄÊü• & -
                    1.Â∞ÜqueryÊãÜÂàÜkgram 
                    2.Ê†πÊçÆkgramÈòàÂÄºÂÅö‰∫§Âèâ Ôºö only vocabulary terms that differ by at most 3 k-grams p33
                    3.then, Jaccard coefficient 
                        Declare a match if ùêΩùëéùëêùëêùëéùëüùëë ùëû, ùë° > ùë°‚Ñéùëüùëíùë†‚Ñéùëúùëôùëë
                Context-sensitive spelling correction
                    hit-based spelling correction--Âü∫‰∫é‰ª•ÂæÄÊü•ËØ¢
                        The correct query ‚Äúflew from munich‚Äù has the most hits
                        BUTÔºö we have to test 7 √ó 20 √ó 3 different variants -- too many 
                        SOLVE: we can log queries
                Issues when using Spelling correction 
                    & - 
                    Spelling correction is potentially expensive
                    Avoid running on every query?
                        Maybe just on queries that match few documents
                how to write 
                    & - NEXT
                    Spelling corrector in 21 lines Python code
                    Peter Norvig: How to write a spelling corrector (http://norvig.com/spell-correct.html)
            Soundex
                key idea of Sondex & 
                    for each word , we can calcu. a Sondex code ,
                        and we can use this code to check the similarity of their pronunciation
                Especially applicable to searches on the names of people **
                Soundex algorithm 
                    ËΩ¨Êç¢ÊàêÂõõ‰Ωç‰ª£Á†Å
                    ËÆ°ÁÆó & -
                    words with same pronunciation will generate the same code 
                    ÂéüÁêÜ 
                        Vowels are viewed as interchangeable **
                        Consonants with similar sounds (e.g., D and T) are put in equivalence classes
                        ÔºöÂèëÈü≥Á±ª‰ººÁöÑËæÖÈü≥ÊîæÂú®‰∏Ä‰∏™Á≠â‰ª∑Á±ª‰∏≠
                        Better alternatives for phonetic matching in IR
        ----!!constructing!!
        ppt-5-Scoring, Term Weighting --over --c
            +btree:ÊòØ‰∏ÄÁßçËá™Âπ≥Ë°°ÁöÑÊ†ëÔºåËÉΩÂ§ü‰øùÊåÅÊï∞ÊçÆÊúâÂ∫è
                BÊ†ëÔºåÊ¶ÇÊã¨Êù•ËØ¥ÊòØ‰∏Ä‰∏™‰∏ÄËà¨ÂåñÁöÑ‰∫åÂèâÊü•ÊâæÊ†ëÔºàbinary search treeÔºâ‰∏Ä‰∏™ËäÇÁÇπÂèØ‰ª•Êã•ÊúâÊúÄÂ∞ë2‰∏™Â≠êËäÇÁÇπ
                BÊ†ëÈÄÇÁî®‰∫éËØªÂÜôÁõ∏ÂØπÂ§ßÁöÑÊï∞ÊçÆÂùóÁöÑÂ≠òÂÇ®Á≥ªÁªüÔºå‰æãÂ¶ÇÁ£ÅÁõò„ÄÇBÊ†ëÂáèÂ∞ëÂÆö‰ΩçËÆ∞ÂΩïÊó∂ÊâÄÁªèÂéÜÁöÑ‰∏≠Èó¥ËøáÁ®ãÔºå‰ªéËÄåÂä†Âø´Â≠òÂèñÈÄüÂ∫¶„ÄÇ
                BÊ†ëËøôÁßçÊï∞ÊçÆÁªìÊûÑÂèØ‰ª•Áî®Êù•ÊèèËø∞Â§ñÈÉ®Â≠òÂÇ®„ÄÇËøôÁßçÊï∞ÊçÆÁªìÊûÑÂ∏∏Ë¢´Â∫îÁî®Âú®Êï∞ÊçÆÂ∫ìÂíåÊñá‰ª∂Á≥ªÁªüÁöÑÂÆûÁé∞‰∏ä
            
            compare: & -
                    Boolean--
                        too many results::Most users don‚Äôt want to wade through 1000s of results
                            Feast or famine
                            Not good for the majority of users
                            it‚Äôs too much work
                        skillsÔºöÔºöa lot of skill to come up with a query 
                    Why rank &ranked--
                        With ranking, large result sets are not an issue
                            Doesn‚Äôt overwhelm the user
                        doesnt need too many skills 
                            more relevant results are ranked higher than less relevant results
            the most basic approach to scoring -- Jaccard coefficient to Scoring
                ‰∫§ÈõÜÂ§Ñ‰ª•Âπ∂ÈõÜÂêàÔºà‰∏™Êï∞Ôºâ
                calcu  & -
                    one query : [ides of March]
                    and two docs
                problems: & -
                    Normalization with Ê†πÂè∑
                    it Doesn‚Äôt consider term freq
                    it Doesn‚Äôt consider how rare items
            verbesserung : tf-idf weighting how & as vectors
                    what is tf idf &
                    basic calculations  
                        BOW:Bag of Words : do not consider order 
                        tf: number that t in doc d 
                            count vector
                            formular & -
                                p19
                            sum over terms to Score for a document-query pair
                            The score is 0 if none of the query terms found in d
                            challenge & -
                                Relevance does not increase proportionally with term frequency
                            ->should be promoted with log function 
                            we can score it only using the tf .
                        idf : number of docs that t occurs in 
                            where comes from the idea ? 
                                explain & - p21
                            ->should be used with log func 
                            to ‚Äúdampen‚Äù the effect of idf
                            calcu p27 & -
                            idf affects the ranking of documents for queries with at least two terms
                            idf has no effect on ranking for one-term queries
                            why not collection frequency ? & -
                                Which word is a better search term (and should get a higher weight)?
                                that means , this term should appear in some docs with more freq and less freq in others 
                                so we can combine the collection freq at the same time to get a better score for queries  
                        we use the log10 transformation for both term frequency and document frequency
                        tf-idf:Best known weighting schema in information retrieval
                            Combines local (term frequency) and global (inverse document frequency) weights with product 
                                of its tf weight and its idf weight
                            character 
                                ÔÇß Increases with the number of occurrences within a document (term frequency)
                                ÔÇß Increases with the rarity of the term in the collection (inverse document frequency)
                                ÔÇß Is 0 if tf = 0 or df = N
                        calculate scores 
                            Assign a tf-idf weight for each term t in each document d
                            vectors are generated
                            calcu & - P28
                        tf-idf compare &
                            idf is a global weight whereas tf is a local weight
                    The vector space model
                        at first : 0/1 
                        then: Ê¨°Êï∞ count 
                        now : tf-idf weights 
                            document vectors with weights

                        it was sparse too ,most entries are zero
                        we should rank relevant documents higher than non-relevant documents
                        Vector space similarity
                            why Euclidean distance is a bad idea & -
                                because Euclidean distance is large for vectors of different lengths
                                Use angle instead of distance
                                Ë∑ùÁ¶ªËøë‰ΩÜÊòØ‰∏çÁõ∏‰ººÔºåÁúãËßíÂ∫¶Êõ¥ÂêàÁêÜ
                            should be calculated with angle cos 
                            normalization
                            Cosine similarity : equivalently, the cosine of the angle between
                            after calculations , we can get Cosine Metric
                            calcu & - ‰ªép41ÂºÄÂßãËÆ°ÁÆó
                    Practical implemention
                        algo p46 & - NEXT
                    Variant tf-idf functions
                        tf 
                            augmented
                        df 
                            prob idf
                        Normalization
                            pivoted unique
                            byte size
                    tf-idf example: Inc.Itn 
                        use different weightings for queries and documents & -
                        ËÆ°ÁÆóÈ¢ò‰∏≠‰ΩøÁî®ltn.bnnËÆ°ÁÆó 
                        NEXT
                            ÂàÜÂà´ÊòØÔºöÔºö term document normalization
                            term::log boolean(b)
                            doc:: idf(t) none(n)
                            normal:: cosine(c) none(n) 
        ppt-6-index Compression --over --c
            +corpus 
                unary code -
                    key steps : calculate offset and length and combine them as one code 
                    unary code can be applied to any distribution 
                œí codes always has odd length
                Gamma code is universal


            ---
            effizient Dokumente findet?
            compress the postings component
            Hardware    
                Disk I/O is block-based
                Block sizes: 8 KB to 256 KB
            WHY ? & -
                Use less disk space ÔÉ† saves money
                Keep more stuff in memory - increases speed
                    Decompression algorithms should be fast
            Lossy vs. lossless compression & -
                Discard some information in Losy compression
                All information is preserved in lossless
                    we mostly do in index compression
                    index--- dictionary + posting lists
            Term statistics
                Heaps‚Äô law
                    Estimates vocabulary size M as a function of collection size number:
                    M is the size of the vocabulary, T is the number of tokens in the collection
                    Typical values for the parameters k and b are: 30 ‚â§ k ‚â§ 100 and b ‚âà 0.5
                    Heaps‚Äô law is linear in log-log space (line with a slop of about 0.5)
                    it was a Empirical law
                    formular & -
                    Tokens can repeat 
                    For Reuters : das functioniert
                    fit is good in general
                    including numbers and spelling errors affect the result 
                    it means & -
                        no maximum vocabulary size reached    
                        the size of the dictionary is quite large for large collections
                Zipf‚Äôs law
                    formular & -
                    it means & -
                    In natural language, there are a few very frequent terms and many very rare terms
                    formular 
            Dictionary compression
                at first :for Reuters: (20+4+4) √ó 400,000 = 11.2 MB
                resolve Fixed-width bad 
                    explain & -
                        Most of the bytes in the term column are wasted
                        We can‚Äôt handle larger strings
                        Average length of a term in English: 8 characters 
                    2.Dictionary as a string & -
                        ÁîªÂõæ **
                        need to know how to caklcu 3 here 
                        400,000 √ó (4 + 4 + 3 + 8) = 7.6 MB 
                    3.Dictionary as a string with blocking & -
                        We eliminate k ‚àí 1 term pointers
                        We save 4 √ó 3 ‚àí (3 + 4 √ó 1) = 5 bytes per block
                        Total savings: 400,000/4 ¬∑ 5 = 0.5 MB
                        from 7.6 MB to 7.1 MB
                        block‰∏çÂèØ‰ª•Â§™Â§ßÁöÑÂéüÂõ† & -
                            Slightly slower
                            we should Tradeoff between compression and the speed of term lookup
                    4.Front coding & -
                        A sequence of terms with identical prefix (e.g. ‚Äúautomat‚Äù) 
                        Consecutive entries: common prefixes
                        the first byte of each entry encodes the number of characters
                        with blocking and front coding: 5.9MB
                        ÂÜôÂá∫Á§∫ÊÑè‰∏≤‰∏≤ & -
                            Ê≥®ÊÑè‰∏§‰∏™Á¨¶Âè∑‰∏çÂêåÔºÅ
                            Ë¢´Âøò‰∫ÜÊï∞Â≠óÔºÅ
            Postings compression
                The postings file is much larger than the dictionary
                For Reuters (800,000 documents), we would use 32 bits per docID when using 4-byte integers
                Alternatively, we can use log2 800,000 ‚âà 19.6 < 20 bits per docID
                goal : less than 20 bits per docID 
                Âü∫Êú¨ÊÄùË∑ØÔºöusing gaps 
                    Postings for frequent terms are close together
                    Gaps between postings are short
                    Postings list using gaps: COMPUTER - 283154, 5, 43, ‚Ä¶
                Variable length encoding 
                    we need a variable encoding method that uses fewer bits for short gaps
                    Two solutions
                        ÔÇß Bytewise compression - Variable length encoding 
                            Dedicate 1 bit (high bit) to be a continuation bit c
                            At the end c=1 others c=0
                            Postings are stored as a byte concatenation see P34
                            still : For a small gap (e.g. 5) VB use a whole byte this can be promoted later 
                            the algorithm & - NEXT
                            exmaple p34 & -
                            do not sensitiv to computer memory alignment matches 
                        ÔÇß Bitwise compression - Elias gamma encoding 
                            Gamma Codes for gap encoding
                            Unary code ÔºöRepresent n as n 1s with a final 0  & -
                                3= 1110
                            Gamma Codes 
                                calcu & -
                                    13? p39
                                    a pair of length and offset
                                    13 ‚Üí 1101 ‚Üí 101 = offset
                                    Encode length in unary code: 1110
                                    length = 1110 and offset = 101 --„Äã 1110101
                                    Length of the entire code is 2 x ‚åälog2 G‚åã + 1 bits
                                    œí codes are always of odd length
                                    steps :
                                        1.write as Binary code 
                                        2.calcu. offset 
                                        3.calcu. len of offset as unary
                                        4.combine them together 
                                some advantages & - explain -
                                    3.prefix-free
                                    2.optimal within a factor of 2
                                    1.universal
                                        independent of the distribution of gaps
                                        it is parameter-free
                                    
                                Gamma seldom used in practice & - explain
                                    1.it is not aligned and thus potentially less efficient
                                postings, gamma encoded 101MB
            p42 ÂõûÈ°æ
            less reality ? NEXT
        ppt-7-Index Construction --over --c
            +corpus 
                    -distributed index construct system
                how can we construct index efficiently ?
                Performance characteristics typical of systems in 2007
                to motivate IR system
                Access to data in memory is much faster than access to data on disk
                    -we can access the data faster when we keep it in mem. 
                takes a few clock cycles    
                    -when we want ot access data on disk , a few clock cycles can be taken  
                    -it takes a few clock cycles to access it 
                We call the technique of keeping frequently used disk data in main memory caching
                    -the cashing technique will be used in this expriment. 
                if it is stored as one chunk
                    -when the data are localted as the same chunk , ...
                Operating systems generally read and write entire blocks. Thus, reading 
                    a single byte from disk can take as much time as reading the entire block.
                    -it takes the same time to ...
                We call the part of main memory where a block being read or written is stored a buffer
                    -buffer is part of 


            Âª∫Á´ãÂÄíÊéíÁ¥¢Âºïeinen invertierten Index daf√ºr erstellen
            Á¥¢ÂºïÊûÑÂª∫ÁÆóÊ≥ïÁöÑËÆæËÆ°ÂèóÁ°¨‰ª∂ÁöÑÈÖçÁΩÆÊâÄÂà∂Á∫¶
            ÁªÉ‰π†È¢òÁõÆÂíåËøô‰∏™‰∏çÊòØÂæàÂ§ßÂÖ≥Á≥ªÔºåÊâÄ‰ª•Êõ¥Ë¶ÅÁúãpptÔºÅÔºÅÔºÅÔºÅÔºÅ
            Static Indexing
                Êìç‰ΩúÁ≥ªÁªüÂæÄÂæÄ‰ª•Êï∞ÊçÆÂùó‰∏∫Âçï‰ΩçËøõË°åËØªÂÜô„ÄÇÂõ†Ê≠§Ôºå‰ªéÁ£ÅÁõòËØªÂèñ‰∏Ä‰∏™Â≠óËäÇÂíåËØªÂèñ‰∏Ä‰∏™Êï∞ÊçÆÂùó
                    ÊâÄËÄóË¥πÁöÑÊó∂Èó¥ÂèØËÉΩ‰∏ÄÊ†∑Â§ö„ÄÇÊï∞ÊçÆÂùóÁöÑÂ§ßÂ∞èÈÄöÂ∏∏‰∏∫ 8 KB„ÄÅ 16 KB„ÄÅ 32 KB Êàñ 64 KB„ÄÇÊàë‰ª¨
                    Â∞ÜÂÜÖÂ≠ò‰∏≠‰øùÂ≠òËØªÂÜôÂùóÁöÑÈÇ£ÂùóÂå∫ÂüüÁß∞‰∏∫ÁºìÂÜ≤Âå∫Ôºà bufferÔºâ
                Na√Øve Indexierung
                    Termvorkommen im Hauptspeicher halten und dort sortieren
                    Wie gro√ü darf eine Dokumentensammlung sein,mit 8 GB Hauptspeicher
                        calcu & - NEXT
                        134.216 Dokumente 13 wan< 80 wan
                        unter Zuhilfenahme von Sekund√§rspeicher
                        Sortieren mit Sekund√§rspeicher 
                            Zahl der Ebene ? & 
                    External Memory Sort &
                Âü∫‰∫éÂùóÁöÑÊéíÂ∫èÁ¥¢ÂºïÊñπÊ≥ïBlocked Sort-Based Indexing (BSBI)
                    idea & - NEXT
                        ÁîªÂõæ
                        1.ÂàÜÂùóËØªÂÖ•
                        2.sortierte Teilfolgen von Termvorkommen,als
                            Ebene 0 von External Memory Sort in zweitem Durchlauf
                        3.ÊúÄÂêé‰∏ÄÊ≠•Ôºö
                            ÂÜÖÂ≠ò‰∏≠Áª¥Êä§‰∫Ü‰∏∫ 10 ‰∏™ÂùóÂáÜÂ§áÁöÑËØªÁºìÂÜ≤Âå∫Âíå‰∏Ä‰∏™‰∏∫ÊúÄÁªàÂêàÂπ∂Á¥¢ÂºïÂáÜÂ§áÁöÑÂÜôÁºìÂÜ≤Âå∫
                            ÊØèÊ¨°Ëø≠‰ª£‰∏≠ÔºåÂà©Áî®‰ºòÂÖàÁ∫ßÈòüÂàóÔºàÂç≥Â†ÜÁªìÊûÑÔºâÊàñËÄÖÁ±ª‰ººÁöÑÊï∞ÊçÆÁªìÊûÑÈÄâÊã©ÊúÄÂ∞èÁöÑÊú™Â§ÑÁêÜËØçÈ°π ID (term id)ËøõË°åÂ§ÑÁêÜ
                            ÂêàÂπ∂ÁªìÊûúÂÜôÂõûÁ£ÅÁõò‰∏≠
                            ÈúÄË¶ÅÊó∂ÔºåÂÜçÊ¨°‰ªéÊñá‰ª∂‰∏≠ËØªÂÖ•Êï∞ÊçÆÂà∞ÊØè‰∏™ËØªÁºìÂÜ≤Âå∫„ÄÇ
                        algo & - NEXT
                    ÂÆûÈôÖ‰∏≠ÁöÑÂæàÂ§öËØ≠ÊñôÂ∫ìËøúÊØîËøô‰∏™ËØ≠ÊñôÂ∫ìË¶ÅÂ§ßÔºåÂ∞±ÈúÄË¶ÅËøôÁßçÊäÄÊúØÂª∫Á´ãÁ¥¢Âºï
                    ÈóÆÈ¢òÔºö
                        ‰ΩÜÊòØÈúÄË¶Å‰∏ÄÁßçÂ∞ÜËØçÈ°πÊò†Â∞ÑÊàêÂÖ∂ ID ÁöÑÊï∞ÊçÆÁªìÊûÑ„ÄÇ
                        ‰∏≠Èó¥Êñá‰ª∂ÂæàÂ§ß intermediate files
                ÂÜÖÂ≠òÂºèÂçïÈÅçÊâ´ÊèèÁ¥¢ÂºïÊûÑÂª∫ÊñπÊ≥ïSingle-Pass In-Memory Indexing (SPIMI)
                    ‰∏ÄÁßçÊõ¥ÂÖ∑Êâ©Â±ïÊÄßÁöÑÁÆóÊ≥ï
                    posting list Âä®ÊÄÅÂ¢ûÈïøÁöÑ‰∏ÄÁßçÁ≠ñÁï•ÔºåÂú®ÂÜÖÂ≠òÊª°‰∫Ü‰ª•ÂêéÔºåÊéíÂ∫èÂπ∂ÂÜôÂÖ•Á£ÅÁõò„ÄÇÂêéÁª≠ÂêàÂπ∂‰∏ébased on block are the same 
                    pseudocode & - NEXT
                    ‰ºòÂäøÔºö
                        ËäÇÁúÅ‰∫ÜidÂ≠òÂÇ®ÁöÑÂºÄÈîÄ
                        ËäÇÁúÅÂÜÖÂ≠ò
                    ‰∏∫‰ΩøÂÄíÊéíËÆ∞ÂΩïË°®ÊåâÁÖßËØçÂÖ∏È°∫Â∫èÊéíÂ∫èÊù•Âä†Âø´ÊúÄÂêéÁöÑÂêàÂπ∂ËøáÁ®ãÔºåË¶ÅÂØπËØçÈ°πËøõË°åÊéíÂ∫èÊìç‰ΩúÔºàÁ®ãÂ∫èÁ¨¨ 11 Ë°åÔºâ
                    Áî±‰∫é‰∫ãÂÖàÂπ∂‰∏çÁü•ÈÅìÊØè‰∏™ËØçÈ°πÁöÑÂÄíÊéíËÆ∞ÂΩïË°®Â§ßÂ∞èÔºåÁÆóÊ≥ï‰∏ÄÂºÄÂßã‰ºöÂàÜÈÖç‰∏Ä‰∏™ËæÉÂ∞èÁöÑÂÄíÊéíËÆ∞ÂΩïË°®Á©∫Èó¥ÔºåÊØèÊ¨°ÂΩìËØ•Á©∫Èó¥ÊîæÊª°ÁöÑÊó∂ÂÄôÔºåÂ∞±‰ºöÁî≥ËØ∑Âä†ÂÄçÁöÑÁ©∫Èó¥
                    ÂéãÁº©ÂØπ‰∫éËøôÁßçÊòØÊúâÊïàÁöÑ
                    ÁÆóÊ≥ïÂ§çÊùÇÂ∫¶ÊòØÁ∫øÊÄßÁöÑÂÖ≥‰∫éTÔºöpairs that can be held into main memory
                    assign a small space at first . Then, dopple it when necessary.
                    Fill main memory as a block
                Caching
                    reduziert dadurch die Antwortzeiten eines IR-Systems
                    & -
                    ÔÇß Least Recently Used (LRU) schafft Platz f√ºr neues Element, indem es das am l√§ngsten unbenutzte
                        Element entfernt
                    ÔÇß Least Frequently Used (LFU) schafft Platz f√ºr neues Element, indem es das am seltensten benutzte
                        Element entfernt
                    Cache-Hit-RatioÔºö die aus dem Cache beantwortet werden k√∂nnen
                Verteilte IR-Systeme
                    Fargen:
                        auf mehreren Rechnern abzulegen
                        invertierten Index schneller aufzubauen
                    zwei Arten von Rechner‚ÄêKnoten :  & -
                        Master und Slaves 
                        Other structures :Grid
                    Mathods of Distributing ***
                        Term-Partitionierter:
                            jeder Rechner-Knoten speichert -- eine Teilmenge der Terme
                        Dokument-Partitionierter:
                            jeder Rechner-Knoten speichert -- Teilmenge der Dokumente
                        compare and see the advantages or dis- & - NEXT
                        Vorteil und Nachteil nennen 
                            Term:
                                NEXT
                            Document:
                                Â¶ÇÊûúSlaveÂÆïÊú∫ÔºåÂá†‰πé‰∏çÂèóÂΩ±Âìç
                                ÂíåÊØè‰∏™SlaveÈÉΩË¶Å‰∫§ÊµÅ
                    parallelisierbar? & - NEXT Grund dafur  explain
                        Ohne Probleme parallel auf verschiedenen Rechnern stattfindenÔºü 
                        Grund nennen -- P39
                    BUT: & - explain
                        Character of a distributed computer: 
                            cluster Can unpredictably slow down or fail
                    Master machine assigns each task to an idle machine from a pool
                    MapReduce programming model
                        users modify map and a reduce function
                        MapReduce Architecture & - NEXT
                        NEXT 
                        MapReduceÊòØÂèØ‰ª•‰∏≤ËÅîÁöÑ
            Âä®ÊÄÅÁ¥¢ÂºïÊûÑÂª∫ÊñπÊ≥ï
                How to keep the index up-to-date as the collection changes
                Drei Methode 
                    1.Na√Øver Ansatz
                        jedes neue anzupassen 
                        Problem:
                            Lange Zugriffszeit
                    2.Re-Indexierung
                        Regelm√§√üige vollst√§ndiger Neuaufbau 
                        Problem :
                            Es geht nur F√ºr kleine oder wenig dynamische Dokumentensammlungen
                            Ineffizienz
                            hoche Speicherbedarf
                    3.Delta-Index
                        -r
                            zus√§tzlichen invertierten Index
                            in an additional inverted index store 
                            use this additional index to deal with queries 
                            put the delta index to disk when necessary 
                            or do it regularly

                        beschreibung:
                            in einem zus√§tzlichen invertierten Index im Hauptspeicher
                            Gel√∂schte Dokumente werden in einer Liste
                            W√§hrend der Anfragebearbeitung : Haupt- und Delta‚ÄêIndex verschmolzen
                            put the delta index to Disk:
                                gesamt n Terme in einem Document und T ist die Lange des Indexes 
                            bis zu T/n mal miteinander verschmolzen?
                                Complxity &
                        Logarithmisches Verschmel
                            put the delta index to Disk: with a flexible length 2^m*n
                            NEXT
        ppt-8-QueryProcessing --over --c
            Â¶Ç‰ΩïÂä†ÈÄüÊØîËæÉÂá∫Top-kÁöÑÁÆóÊ≥ï
            ÂÆûÈôÖ‰∏äÁöÑÊØîËæÉËøáÁ®ã,Âà©Áî®Á¥ØÂä†Âô®
            ÂΩìÊó∂Âú®Âõæ‰π¶È¶ÜÁúãÊáÇÁöÑ
            Wie kann man effizient :
                wert ermittelnÔºöÂèØ‰ª•Âà©Áî®Á¥ØÂä†Âô®Ôºå‰ºòÂÖàÈòüÂàóÂä†ÈÄüËøô‰∏™ËøáÁ®ã
                die Top-k Dokumente bestimmen
            ‰∏ÄË°å‰∏ÄË°åTermËØªÂèñÔºå‰ΩÜÊòØËøôÈáåÊú™ÂøÖË¶ÅËØªÂà∞ÊúÄÂêé
            1.Term-at-a-Time (TAAT): die in Vorlesung 5 gegeben 
                ‰∏ÄË°å‰∏ÄË°åTermËØªÂèñÔºå‰ΩÜÊòØËøôÈáåÊú™ÂøÖË¶ÅËØªÂà∞ÊúÄÂêé
                ‰∏çÁî®ÊéíÂ∫è
                ÂøÖÈ°ªÂÖ®ËØªÂÆå
                p8ÂõûÈ°æËÆ°ÁÆó
                calcu & -

            2.DAAT
                ‰∏ÄÊ¨°‰∏Ä‰∏™Document
                p10ÂõûÈ°æËÆ°ÁÆó
                calcu & -
            NRA Ôºö No Random Accesses
                explain & -
                    NRA ist ein allgemeines Verfahren
                    fr√ºhzeitig beenden  and korrekten Top-k Dokumenten finden
                    monotonen Aggregationsfunktionen fÂçïË∞ÉËÅöÂêàÂáΩÊï∞
                posting listÈúÄË¶ÅÊéíÂ∫èÈôçÂ∫è
                ÊúÄÂ∑ÆÔºöÂΩìÂâçÊï∞ÂÄº
                best(d)ÔºöÂΩìÂâçÊï∞ÂÄº+ÈùûÂΩìÂâçdocÁöÑÂàóÁöÑ‰∏§‰∏™/n‰∏™Êï∞ÂÄº‰πãÂíå 
                unseen = ÂΩìÂâçÂàóÊï∞ÂÄº‰πãÂíå
                Êù°‰ª∂Ôºö
                    unseen ‚â§ mink: Êú™Áü•ÁöÑÈÉΩ‰∏çË°å‰∫ÜÔºåÁé∞Âú®Â∑≤ÁªèÂá∫Êù•topk‰∫Ü
                    best ‚â§ minkÔºöÂΩìÂâçdoc‰∏çË°å‰∫ÜÔºå‰∏çÂèØËÉΩÂÖ•ÈÄâ‰∫Ü
                p27ËÆ°ÁÆó & -
                p29ÂÖ¨Âºè & -
                ÊàëÁöÑÁΩë‰∏äÁÖßÁâá‰æãÂ≠ê &ËÆ°ÁÆó - NEXT
        ppt-9-Evaluation --over --c
            Unranked evaluation
                ÁªôÂÆöÊµãËØïÈõÜÔºö
                ÔÇß User Happiness
                    However, the key measure for a search engine is user happiness
                    how to Measure the happiness ? & - p5  explain
                        individual users:
                            relevant quality
                            speed of response time ,
                            size of index, 
                            uncluttered UI

                        for Web search engine:
                        for Ecommerce:
                        ...
                ÔÇß Precision and Recall
                    explain & - NEXT explain
                        Ê≠£Á°ÆÁéáÔºöÁªìÊûú‰∏≠Áõ∏ÂÖ≥ÊñáÊ°£ÊâÄÂç†ÁöÑÊØî‰æãÔºåÊòØ‰∏çÊòØÊúâ‰∏çÁõ∏ÂÖ≥ÁöÑÔºü
                        Âè¨ÂõûÁéáÔºöÊòØÊâÄÊúâÁõ∏ÂÖ≥ÈÉΩËøîÂõû‰∫ÜÂêóÔºüËøîÂõûÁöÑÁõ∏ÂÖ≥ÊñáÊ°£Âç†ÊâÄÊúâÁõ∏ÂÖ≥ÊñáÊ°£ÁöÑÊØî‰æã
                    Web Ê£ÄÁ¥¢Áî®Êà∑Â∏åÊúõÁ¨¨‰∏ÄÈ°µÁöÑÊâÄÊúâÁöÑÁªìÊûúÈÉΩÊòØÁõ∏ÂÖ≥ÁöÑÔºå
                        ‰πüÂ∞±ÊòØËØ¥‰ªñ‰ª¨ÈùûÂ∏∏ÂÖ≥Ê≥®È´òÊ≠£Á°ÆÁéáÔºå
                        ËÄåÂØπÊòØÂê¶ËøîÂõûÊâÄÊúâÁöÑÁõ∏ÂÖ≥ÊñáÊ°£Âπ∂Ê≤°ÊúâÂ§™Â§ßÁöÑÂÖ¥Ë∂£
                    Áõ∏ÂèçÂú∞Ôºå‰∏Ä‰∫õ‰∏ì‰∏öÁöÑÊêúÁ¥¢‰∫∫Â£´ÔºàÂ¶ÇÂæãÂ∏àÂä©Êâã„ÄÅÊÉÖÊä•ÂàÜÊûêÂ∏àÁ≠âÔºâ
                        Âç¥ÂæÄÂæÄÈáçËßÜÈ´òÂè¨ÂõûÁéáÔºåÊúâ
                        Êó∂ÁîöËá≥ÂÆÅÊÑøÂøçÂèóÊûÅ‰ΩéÁöÑÊ≠£Á°ÆÁéá‰πüË¶ÅËé∑ÂæóÈ´òÁöÑÂè¨ÂõûÁéá
                    ÂØπÊú¨Êú∫Á°¨ÁõòËøõË°åÊêúÁ¥¢ÁöÑ‰∏™‰∫∫Áî®Êà∑‰πüÂ∏∏Â∏∏ÂÖ≥Ê≥®Âè¨ÂõûÁéá
                    ÂÖ¨Âºè p9 & -
                    ËÆ°ÁÆóp10 & -
                ÔÇß F-Measure
                    Precision/recall tradeoff
                    Â∞±ÊòØ‰∏Ä‰∏™Ë∞ÉÂíåÂπ≥ÂùáÂÄºÔºåÂÖ¨Âºè & -
                    increase recall by returning more documents
                    A system that returns all documents has 100% recall!
                    It‚Äôs easy to get high precision for very low recall
                    use harmonic mean
                    ‰∏Ä‰∏™ËûçÂêà‰∫ÜÊ≠£Á°ÆÁéáÂíåÂè¨ÂõûÁéáÁöÑÊåáÊ†áÊòØ F ÂÄºÔºà F measureÔºâÔºåÂÆÉÊòØÊ≠£Á°ÆÁéáÂíåÂè¨ÂõûÁéáÁöÑË∞ÉÂíåÂπ≥ÂùáÂÄº
                        ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåÊ≠£Á°ÆÁéáÂíåÂè¨ÂõûÁéáÁöÑÊùÉÈáçÁõ∏Á≠â
                        Œ≤ < 1 Ë°®Á§∫Âº∫Ë∞ÉÊ≠£Á°ÆÁéáÔºåËÄå Œ≤ > 1 Ë°®Á§∫Âº∫Ë∞ÉÂè¨ÂõûÁéá
                        Values of Œ≤ < 1 emphasize precision, while values
                            of Œ≤ > 1 emphasize recall
                        ‰∏∫‰ªÄ‰πà‰ΩøÁî®Ë∞ÉÂíåÂπ≥ÂùáËÄå‰∏çÊòØÂÖ∂‰ªñÁÆÄÂçïÁöÑÂπ≥ÂùáÊñπÊ≥ïÔºàÂ¶ÇÁÆóÊúØÂπ≥ÂùáÔºâÊù•ËÆ°ÁÆó F ÂÄºÂë¢ & - NEXT
                            Ê≤°ÊúâÊúÄÂ§ßÊúÄÂ∞èÊï∞ÂÄºÈôêÂà∂
                            Ë∞ÉÂíåÂπ≥ÂùáÂÄºÂæÄÂæÄÂ∞è‰∫éÁÆóÊúØÂπ≥ÂùáÂíåÂá†‰ΩïÂπ≥ÂùáÂÄºÔºåÂπ∂‰∏îÂ∏∏Â∏∏‰∏é‰∏§‰∏™Êï∞ÁöÑËæÉÂ∞èÂÄºÊõ¥Êé•Ëøë
                            a kind of smooth minimum
                        Example & - p13
                ÔÇß Cost-based Measure
                    As an alternative to precision and recall
                    ÂûÉÂúæÈÇÆ‰ª∂ÁöÑËÆ°ÁÆóÔºåÂä†ÊùÉÁöÑÂíå
                    example & -
                ÔÇß Accuracy
                    Á≤æÁ°ÆÁéáÔºöÊ†πÊçÆ‰∏äËø∞‰∏§‰∏™Ôºå‰ª•ÂèäË°®Ê†ºÔºöÁ≤æÁ°ÆÁéáÊåáÊ†áÂú®ÂæàÂ§öÊú∫Âô®Â≠¶‰π†ÈóÆÈ¢ò‰∏≠ÁöÑ‰ΩøÁî®ÈùûÂ∏∏ÊôÆÈÅç
                            ‰ΩÜÊòØÔºö‰∏çÂùáË°°ÊÄßÔºåÊØîÂ¶ÇÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåË∂ÖËøá 99.9%ÁöÑÊñáÊ°£ÈÉΩÊòØ‰∏çÁõ∏ÂÖ≥ÊñáÊ°£„ÄÇ
                            TP TN Âç†Áî®ÁöÑÊØî‰æã ÂÖ¨Âºè & -
                            Why is accuracy not a useful measure
                                Normally over 99.99% of the documents are in the non-relevant category
                                explain &
                            ÈóÆÈ¢òbugÔºöHow to build a 99.9999% accurate search 
                                engine on a low budget
                                ÔÇß The Snoogle search engine 
                                below always returns 0 results (‚Äú0 matching results found‚Äù),
                                regardless of the query
                Macro average (precision) and Micro average (precision) 
                    p19 calcu & ÂÖ¨ÂºèÊúâÈóÆÈ¢ò  
            Ranked evaluation
                ÔÇß Precision-Recall-Curve
                    ROC Curve
                    ROC Êõ≤Á∫øÈÄöÂ∏∏Ëµ∑‰∫éÂ∑¶‰∏ãËßíËÄåÈÄêÊ∏êÂêëÂè≥‰∏äËßíÂª∂‰º∏„ÄÇ‰∏Ä‰∏™Â•ΩÁöÑÁ≥ªÁªüÔºåÊõ≤Á∫øÂõæÁöÑÂ∑¶ÈÉ®‰ºöÊØîËæÉÈô°Â≥≠
                    Âú®ÂæàÂ§öÈ¢ÜÂüüÔºå‰∏Ä‰∏™ÊôÆÈÅç‰ΩøÁî®ÁöÑÊåáÊ†áÊòØËÆ°ÁÆó ROC Êõ≤Á∫ø‰∏ãÁöÑÈù¢ÁßØ
                    ËøëÂπ¥Êù•ÔºåÂæÄÂæÄÂ∫îÁî®Âú®Âü∫‰∫éÊú∫Âô®Â≠¶‰π†ÁöÑÊéíÂ∫èÊñπÊ≥ï‰∏≠ÔºàÂèÇËÄÉ 15.4 ËäÇÔºâÁöÑÊåáÊ†á‚Äî‚Äî
                    CGÔºà cumulative gainÔºåÁ¥ØÁßØÂ¢ûÁõäÔºâ
                    first k elements in the result list
                    For each such sets (with size k), 
                        precision and recall values can be plotted
                    ÂëàÈîØÈΩøÂΩ¢ÁöÑÂéüÂõ†? & -
                        k+1ÁØáÊñáÊ°£‰∏çÁõ∏ÂÖ≥ÔºåÂè¨ÂõûÁéá‰∏çÂèòÔºåÊ≠£Á°ÆÁéá‰∏ãÈôç
                        Â¶ÇÊûúËøîÂõûÁöÑÁ¨¨(k+1)ÁØáÊñáÊ°£Áõ∏ÂÖ≥ÔºåÈÇ£‰πàÊ≠£Á°ÆÁéáÂíåÂè¨ÂõûÁéáÈÉΩ‰ºöÂ¢ûÂ§ß

                        in top-k hits ::„ÄÄ
                            assume that k+1 is not relevant .
                            from k to k+1 , recall will not be changed , but precision has decreased
                ÔÇß 11-point interpolated average precision
                    ÂÆö‰πâ‰∏Ä‰∏™ 11 ÁÇπÊèíÂÄºÂπ≥ÂùáÊ≠£Á°ÆÁéáÔºå
                        Áî®‰∫éÊµìÁº©‰ø°ÊÅØ ***
                    transform this information down to a few numbers
                    Each point corresponds to a result for the top k ranked hits
                    ÈÅçÂéÜrecallËé∑Âæóprecision ËÄåÂêéÂèñÂπ≥ÂùáÂÄº ***
                from multiple queries ÔºüÊúâÁöÑinformation needÂÆπÊòìÊúâÁöÑÈöæ
                    TREC‰∏≠ÊúÄÂ∏∏ËßÑÁöÑÊåáÊ†áÊòØMAPÔºà mean average precisionÔºåÂπ≥ÂùáÊ≠£Á°ÆÁéáÂùáÂÄºÔºâ
                    MAPË¢´ËØÅÊòéÂÖ∑ÊúâÈùûÂ∏∏Â•ΩÁöÑÂå∫Âà´ÊÄßÔºà discriminationÔºâÂíåÁ®≥ÂÆöÊÄßÔºà stabilityÔºâ
                    MAPÂèØ‰ª•Á≤óÁï•Âú∞ËÆ§‰∏∫ÊòØÊüê‰∏™Êü•ËØ¢ÈõÜÂêàÂØπÂ∫îÁöÑÂ§öÊù°Ê≠£Á°ÆÁéá‚ÄîÂè¨ÂõûÁéáÊõ≤Á∫ø‰∏ãÈù¢ÁßØÁöÑÂπ≥ÂùáÂÄº„ÄÇ
                    MAP: Average of several average precision values
                        Gold standardÔºüÔºü
                            ÂèØËÉΩÂ∞±ÊòØ‰º∞ËÆ°Êúâ50%ÊòØÁõ∏ÂÖ≥ÁöÑ
                            ‰πüÂ∞±ÊòØËØ¥Ëøô‰∫õÂà§ÂÆöÊûÑÊàêÊâÄË∞ìÁöÑ‚Äú Ê†áÂáÜÁ≠îÊ°à‚Äù ÈõÜÂêà** ‰∏ç‰∏ÄÂÆöÂ§öÂ∞ëÔºåÊòØÈ¢òÁõÆÊù°‰ª∂
                        p28ËÆ°ÁÆó & -
            Evaluation benchmarks
                to mwasure effectiveness
                ÔÇß Standard relevance benchmarks
                    what is that actually & -
                        documents
                        information needs
                        Human relevance assessments
                    eg
                        TREC:1.89 million documents, mainly newswire articles, 
                            450 information needs
                        Five largest classes in the Reuters-21578
                ËØÑ‰ª∑ËøáÁ®ã
                    PoolingÁ≠ñÁï•--ËØÑ‰ª∑ÊâÄÊúâÊñáÊ°£Â∑•‰ΩúÈáèÊòØÂæàÂ§ßÁöÑÔºåÂèØ‰ª•Âè™ËØÑ‰ª∑Ê£ÄÁ¥¢ÁöÑ
                    ÁªôÂÆö‰ø°ÊÅØÈúÄÊ±ÇÈõÜÂèäÊñáÊ°£ÈõÜÔºåÈúÄË¶ÅÁªôÂá∫ÂÆÉ‰ª¨‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßÂà§ÂÆöÊÉÖÂÜµÔºåËøôÊòØ‰∏ÄÈ°πÈúÄË¶Å‰∫∫Â∑•ÂèÇ‰∏é
                    ÁöÑË¥πÊó∂Ë¥πÂäõÁöÑÂ∑•‰Ωú
                ÔÇß relevance assessmentsÈúÄË¶Å‰∏ÄËá¥ÊÄß--Kappa statistics,ËÆ°ÁÆójudgeÊòØ‰∏çÊòØÂèò‰∫ÜÔºü
                    measure consistency .
                    Kappa Âè™ÊòØ‰∏Ä‰∏™ÁªüËÆ°ÈáèÔºåÂ≠òÂú®ÊäΩÊ†∑ËØØÂ∑Æ„ÄÇ
                    ÁªüËÆ°Â≠¶Ôºå‰∏ÄËá¥ÊÄßÊ£ÄÈ™å
                    http://sofasofa.io/forum_main_post.php?postid=1000321&
                    xiangqin ‰æãÂ≠ê 
                    ppt‰∏äËÆ°ÁÆó & p37 - ÁªôÂÆö‰∏§‰∏™Âà§Êñ≠ÔºåÊ±ÇKappaÁªüËÆ°
                    ÁΩëÈ°µ‰∏≠ÈÇ£‰∏™ËÆ°ÁÆó & - NEXT
                    Relevance assessments are only usable if they are consistent
                    As a rule of thumbÔºö 0.8 0.67Ôºü
                    Marginal relevanceÔºü 
                        ÊúÄÂ§ßËæπÁïåÁõ∏ÂÖ≥Ê≥ïÔºàMaximal Marginal RelevanceÔºâ & - NEXT
                ASK Consumer
                Consumer (Data) Science
                    Ongoing studies of user behavior in the lab
                    Âà©Áî®Áî®Êà∑‰ø°ÊÅØËøõË°åÈªëÁõíÊµãËØï
                    ÔÇß A/B Testing
                        two versions of the same product
                        ÂÖ∂ÂÆûÂ∞±ÊòØ‰∏§‰∏™ÁâàÊú¨ÁöÑ‰∫ßÂìÅ
                        ‰æãÂ¶ÇÂ••Â∑¥È©¨ÁöÑÁ´ûÈÄâÂãüÊçêÁΩëÁ´ô„ÄÇËøô‰∏™ÁΩëÁ´ôÊúÄÊ†∏ÂøÉÁöÑÁõÆÊ†áÊòØÔºöËÆ©ÁΩëÁ´ôÁöÑËÆøÂÆ¢ÂÆåÊàêÊ≥®ÂÜåÂπ∂ÂãüÊçêÁ´ûÈÄâËµÑÈáë
                        Ëøô‰∏™Âõ¢ÈòüÂΩìÊó∂ÂÅö‰∫Ü‰∏Ä‰∏™ÈùûÂ∏∏ÊàêÂäüÁöÑÂÆûÈ™åÔºöÈÄöËøáÂØπ6‰∏™‰∏çÂêåÈ£éÊ†ºÁöÑ‰∏ªÈ°µËøõË°åABÊµãËØïÔºå
                            ÊúÄ‰ºòÁöÑÁâàÊú¨Â∞ÜÁΩëÁ´ôÊ≥®ÂÜåËΩ¨ÂåñÁéáÊèêÂçá‰∫Ü40.6%Ôºå
                            ËÄåËøô40.6%ÁöÑÊñ∞Â¢ûÁî®Êà∑Áõ¥Êé•Â∏¶Êù•‰∫ÜÈ¢ùÂ§ñÁöÑ5700‰∏áÁæéÈáëÂãüÊçêËµÑÈáë
                        A/BÊµãËØïÂ∏¶Êù•ÁöÑÊî∂Áõä‰ºöËøúÈ´ò‰∫éA/BÊµãËØïÁöÑÂÆûÊñΩÊàêÊú¨
                        ‚ÄúÂÅáËÆæÊääÊ≥®ÂÜåÊµÅÁ®ã‰∏≠ÁöÑÂõæÁâáÊ†°È™åÁ†ÅÊñπÂºèÔºåÊîπÊàêÁü≠‰ø°Ê†°È™åÁ†ÅÁöÑÊñπÂºèÔºåÊàë‰ª¨ÁöÑÊ≥®ÂÜåËΩ¨ÂåñÁéáÂèØËÉΩÊèêÂçá10%‚Äù„ÄÇ
                        Âü∫‰∫éËøô‰∏™ÂÅáËÆæÔºåÊàë‰ª¨‰ºöËÆæËÆ°ÂØπÂ∫îÁöÑA/BÊµãËØï
                        widget variations
                    ÔÇß Interjudge agreement
                    ÔÇß Query Logs
                        Logs can be used to tune / evaluate search engines
                        Aggregate clicks to reduce noise 
                        Click deviation &ÈÅøÂÖçÈöèÊú∫ÁÇπÂáªÔºü
            Result summaries
                Presenting Results
                    as a list with discreption
                    what to descreption? & -
                        document title, url, some metadata
                    Two basic kinds result Summaries
                        static
                            NLPÔºà natural language processingÔºåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºâÈ¢ÜÂüü‰∏≠Â≠òÂú®Â§ßÈáèÊõ¥Â•ΩÁöÑÊñáÊú¨ÊëòË¶ÅÊñπÊ≥ï„ÄÇ
                            Âú®Êõ¥Â§çÊùÇÁöÑ NLP ÊñπÊ≥ï‰∏≠ÔºåÁ≥ªÁªüÂèØ‰ª•ÈÄöËøáËá™Âä®ÂÖ®ÊñáÁîüÊàê
                                ÊñπÂºèÊàñËÄÖÂØπÂéüÊñáÊ°£‰∏≠Âè•Â≠êËøõË°åÁºñËæëÊàñÁªÑÂêàÁöÑÊñπÊ≥ïÊù•Ëá™Âä®ÁîüÊàêÊëòË¶ÅÂè•Â≠ê
                            basic method: a subset of the document
                            better: Emphasizing sentences with key terms
                            or: complex NLP to synthesize/generate a summary
                        dynamic 
                            ÈÄöÂ∏∏Ëøô‰∫õÁ™óÂè£‰ºöÂåÖÂê´‰∏Ä‰∏™ÊàñËÄÖÂ§ö‰∏™Êü•ËØ¢ËØçÈ°π
                            Âä®ÊÄÅ‰∏≠ÊúâÈùôÊÄÅÂÜÖÂÆπÈÉ®ÂàÜ
                            keyword-in-context (KWIC) snippets
                            Ëß£ÂÜ≥ËÆ°ÁÆóÂ§çÊùÇÈóÆÈ¢òÔºöÔºöÈ¢ÑÂÖàËøõË°åÁ£ÅÁõòÁºìÂ≠ò
                            ÁîüÊàêÂä®ÊÄÅÊëòË¶ÅÁöÑÁõÆÊ†áÊòØÈÄâÂá∫Êª°Ë∂≥Â¶Ç‰∏ãÊù°‰ª∂ÁöÑÁâáÊÆµ & - NEXT p49
                                (i) Âú®ÊñáÊ°£‰∏≠ÊúÄÂ§ßÈôêÂ∫¶Âú∞ÂåÖÊã¨Ëøô‰∫õËØçÈ°πÁöÑ‰ø°ÊÅØÔºõshould contain several of the query terms as many as possible
                                (ii) ÂÜÖÂÆπË∂≥Â§üÂÆåÊï¥ÔºåÊñπ‰æøÁî®Êà∑ÈòÖËØªÁêÜËß£Ôºõas a phrase,should be complete sentences
                                (iii) Ë∂≥Â§üÁü≠ÔºåÊª°Ë∂≥ÊëòË¶ÅÂú®Á©∫Èó¥‰∏äÁöÑ‰∏•Ê†ºÈôêÂà∂„ÄÇshort enough , beacause the space is limited 
                            Summary should answer the query, so we don‚Äôt 
                                have to look at the document
                            Detail?
                                cannot construct a dynamic summary from the positional inverted index 
                                Ë∂ÖËøáÂõ∫ÂÆöÂâçÁºÄÈïøÂ∫¶ÁöÑÊñáÊ°£Âú®‰∫ßÁîüÂä®ÊÄÅÊëòË¶ÅÊó∂Âè™Âü∫‰∫éÊñáÊ°£ÂâçÁºÄÊù•ÂÆûÁé∞
                                +NEXT
                            We need to cache documents
        -----!!improving!!‰ø°ÊÅØÊ£ÄÁ¥¢‰∏≠ÊúÄÈáçË¶ÅÁöÑÁâàÂùó----ÊñáÊ°£ËØÑÂàÜÔºåËØÑÂàÜÂèòÊç¢ÔºåÈíàÂØπ‰∏çÂêåÁöÑÁ≥ªÁªüËØÑÂàÜ
        ppt-10-Relevance Feedback and Query Expansion Á¨¨ 9 Á´† Áõ∏ÂÖ≥ÂèçÈ¶àÂèäÊü•ËØ¢Êâ©Â±ï --over --c
            a quick review:
                Áî®Êà∑ÂØπÁõ∏ÂÖ≥ÊÄßÁªôÂá∫ÂèçÈ¶àÊÑèËßÅ
                ÈíàÂØπ‰∏Ä‰πâÂ§öËØçÁé∞Ë±° **
            Áõ∏ÂÖ≥ÂèçÈ¶à--‚Äúlocal‚Äù(relevance feedback)
                ÈÄöËøáÊü•ËØ¢ÁöÑÂàùÂßãÂåπÈÖçÊñáÊ°£ÂØπÂéüÂßãÊü•ËØ¢ËøõË°å‰øÆÊîπ
                Áõ∏ÂÖ≥ÂèçÈ¶à
                    Áî®Êà∑‰ºöÂØπÂàùÊ¨°Ê£ÄÁ¥¢ÁªìÊûúÁöÑÁõ∏ÂÖ≥ÊÄßÁªôÂá∫ÂèçÈ¶àÊÑèËßÅ
                    Êèê‰∫§ÂèçÈ¶àÂêéÁöÑÊ£ÄÁ¥¢ÁªìÊûúÔºåÂÖ∂Ê≠£Á°ÆÁéáÂæóÂà∞ÊòæËëóÊèêÈ´ò
                    Rocchio ÁÆóÊ≥ïÊòØÁõ∏ÂÖ≥ÂèçÈ¶àÂÆûÁé∞‰∏≠ÁöÑ‰∏Ä‰∏™ÁªèÂÖ∏ÁÆóÊ≥ï
                    Âü∫‰∫éÊ¶ÇÁéáÁöÑÁõ∏ÂÖ≥ÂèçÈ¶àÊñπÊ≥ï
                    Ad hoc retrievalËá™ÁªÑÁªáÊ£ÄÁ¥¢
                    four different examples with picture & -
                    ÂêéÊù•ÊéíÂêçÂêëÂâç‰∫Ü **
                    document : term vector 
                    ÔÇß Rocchio Algorithm
                        based on the vector space model
                        key idea & -
                            find a new query vector : The optimal query vector  :
                            make the  similarity with relevant documents  bigger ! 
                            change the q vector !!?
                        origionally , the query vector does not separate 
                            relevant / non-relevant ***
                        formular & - p17 NEXT
                            Dr and Dnr: 
                                sets of known relevant and 
                                non-relevant documents respectively
                            Why parameter & -
                                If we have a lot of judged documents, 
                                we want a higher Œ≤/Œ≥
                        ÊúâÂõæÊù•Ëß£ÈáäËøô‰∏™ÂÖ¨Âºè & -Ôºö‰∏§‰∏™Âõæ p29
                            ÂÅáËÆæ: we find document besed on query 
                            vector and the docs returned Âú® q 
                            ‰∏∫‰∏≠ÂøÉÁöÑÂúÜ‰∏≠Ôºö ËÆ°ÁÆóÁõ∏‰ººÂ∫¶ÁöÑÊó∂ÂÄôÊòØÊ¨ßÊ∞èË∑ùÁ¶ªÔºü 
                        it separates relevant and non-relevant documents p28
                    ÔÇß Positive versus negative feedback & -
                        Positive feedback is more valuable than negative feedback ***
                        Many systems (e.g. image search at the beginning) 
                        allow only positive feedback ÔÉ† is Œ≥ = 0
                        Most IR systems set Œ≥ < Œ≤
                    ÔÇß Assumptions / Evaluation / Problems
                        Assumptions Â∫îËØ•ÂÅöÂì™‰∫õÂÅáËÆæÔºü & - NEXT
                            sufficient knowledge to be able to make an initial query
                                can be various reasons why initial query may fail 
                                ÊòØ‰ªÄ‰πàÂéüÂõ†Âú®‰∏ÄÂºÄÂßãÁöÑÊêúÁ¥¢‰∏≠Â§±Ë¥•Ôºü & -
                                    Misspellings
                                    ..
                                    ..p30
                            Term distribution in all non-relevant documents will be different
                            At least five judged documents can give stable results 
                        Evaluation 
                            one round of relevance feedback is Ok & -
                            we can Pick one of the evaluation measures from last lecture 
                            Ê†πÊçÆ‰ª•ÂæÄÁªßÁª≠ËØÑ‰ª∑ & ! -
                            ‰æãÂ¶ÇÔºö
                                Compute p@10 for original query q0
                                Compute p@10 for modified relevance feedback query q1
                                ÁÑ∂ÂêéÂÜçÂ§ÑÁêÜËøô‰∏§‰∏™Êï∞ÊçÆÔºåÂæóÂà∞‰∏Ä‰∏™ËØÑ‰ª∑Êï∞ÊçÆ
                            Âà§Êñ≠È¢òÔºö & -
                                Fair evaluation must be on ‚Äúresidual‚Äù documents (not yet judged by user) ***
                                in most cases : q1 is spectacularly better than q0
                        problemsÔºå‰∏çËµ∑‰ΩúÁî®ÁöÑÂéüÂõ†  & - NEXT
                            ‚ÄúExcite web search engine‚Äù
                                :Initially provided full relevance feedback,However
                            no incentive to give feedback
                            users do not understand .
                                Web search users are only rarely concerned with increasing recall
                            this method creates long modified queries
                ‰º™Áõ∏ÂÖ≥ÂèçÈ¶à
                    it assumes that topk documents are relevant .
                    can improve quaity without interaction with users  
                    it automates the ‚Äúmanual‚Äù part of true relevance feedback
                    Áî®Êà∑‰∏çÈúÄË¶ÅËøõË°åÈ¢ùÂ§ñÁöÑ‰∫§‰∫íÂ∞±ÂèØ‰ª•Ëé∑ÂæóÊ£ÄÁ¥¢ÊÄßËÉΩÁöÑÊèêÂçá„ÄÇ
                    ÂÅáËÆæÊéíÂêçÈù†ÂâçÁöÑ k ÁØáÊñáÊ°£ÊòØÁõ∏ÂÖ≥ÁöÑÔºåÊúÄÂêéÂú®Ê≠§ÂÅáËÆæ‰∏äÂÉè‰ª•ÂæÄ‰∏ÄÊ†∑ËøõË°åÁõ∏ÂÖ≥ÂèçÈ¶à„ÄÇ
                    ÂÆÉ‰∏çÂèØËÉΩÂÆåÂÖ®ÈÅøÂÖçËá™Âä®ÂåñÊìç‰ΩúÊâÄÂ∏¶Êù•ÁöÑÈ£éÈô©
                        it can leads to  query drift & -
                    pseudo-relevance feedback is effective on average & -
                        use the notation : relevance feedback (PsRF)
                Èó¥Êé•Áõ∏ÂÖ≥ÂèçÈ¶à Indirect relevance feedback
                    -r     
                        implicit feedback from user data 
                            eg. location of users , time of the query ...

                    Less reliable than explicit feedback
                    users are often reluctant to provide explicit feedback 
                    understand it  & -
                        Web ÊêúÁ¥¢ÂºïÊìé‰∏ÄÊ†∑ÁöÑÂÖ∑ÊúâÈ´òËÆøÈóÆÈáèÁöÑÁ≥ªÁªü‰∏≠ÔºåÊî∂ÈõÜÁî®Êà∑ÁöÑÂ§ßÈáèÈöêÂºèÂèçÈ¶à‰ø°ÊÅØÊòØÂçÅÂàÜÂÆπÊòìÁöÑ
                        Â¶ÇÊûúÁî®Êà∑ÊµèËßàÁöÑÊ¨°Êï∞Ë∂äÂ§öÔºåÈÇ£‰πàÂÆÉÁöÑÊéíÂêç‰πüË∂äÈ´ò„ÄÇ
                        ËøôÂÆûÈôÖÊòØÁÇπÂáªÊµÅÊåñÊéòÔºà clickstream miningÔºâËøô‰∏™ÈÄöÁî®È¢ÜÂüüÁöÑ‰∏ÄÁßçÂΩ¢Âºè„ÄÇ
                        ‰∏Ä‰∏™ÈùûÂ∏∏Áõ∏ÂÖ≥ÁöÑÊñπÊ≥ïÁî®‰∫é‰∏é Web Êü•ËØ¢Áõ∏ÂåπÈÖçÁöÑÂπøÂëäÊéíÂ∫è
                    Áª¥Êä§‰ø°ÊÅØËøáÊª§Âô®ÔºàÂ¶ÇÊñ∞ÈóªËøáÊª§Âô®Ôºâ
            Êü•ËØ¢Êâ©Â±ï(Query expansion)
                Web ÊêúÁ¥¢ÂºïÊìé‰ºöÁªôÂá∫Áõ∏ÂÖ≥ÁöÑÊé®ËçêÊü•ËØ¢ÔºåÁÑ∂ÂêéÁî®Êà∑ÂèØ‰ª•ÈÄâÊã©ÂÖ∂‰∏≠ÁöÑÊüê‰∏™Êé®ËçêÊü•ËØ¢ËøõË°åÊêúÁ¥¢
                    "also try :"
                ‰ΩÜÊòØÊÄªÁöÑÊù•ËØ¥Êü•ËØ¢Êâ©Â±ï‰∏çÂ¶ÇÁõ∏ÂÖ≥ÂèçÈ¶àÊäÄÊúØÊàêÂäü„ÄÇÂΩìÁÑ∂ÔºåÂÆÉÁöÑ‰ºòÁÇπÊòØÊõ¥ÂÆπÊòì‰∏∫Áî®Êà∑ÊâÄÁêÜËß£
                    ‰ºÅÂõæËß£ÂÜ≥"ÁøªËØëËΩ¨Êç¢‚Äù ÁöÑÈóÆÈ¢òÔºåÂç≥Áî®Êà∑Â¶Ç‰ΩïÁü•ÈÅìÊñáÊ°£‰ΩøÁî®Âì™‰∫õËØçÈ°π
                types of it : & -
                    1.based on query log mining
                        example give 
                            example 1:
                                After issuing the query [herbs], 
                                users frequently search for [herbal remedies]
                            example 2:
                                Users searching for [flower pix] 
                                    frequently click on the URL photobucket.com/flower
                                Users searching for [flower clipart] 
                                    frequently click on the same URL
                    2.ÊâãÂ∑•ÁºñËæë
                        built up sets of synonymous 
                        Example for manual thesaurus: PubMed
                    3.Âü∫‰∫éÁªüËÆ°Â≠¶ÁöÑÁªìÊûúÔºåÁõ∏ÂÖ≥ÊÄßÁü©Èòµ
                        Automatically derived thesaurus (e.g., 
                            based on co-occurrence statistics)
                        idea & -
                        using term-document matrix A , 
                            we can generate co-occurrence statistics 
                        semantically related with t
                        Widely used in specialized search engines 
                            for science and engineering
                        we use global resource, i.e. a 
                            resource that is not query-dependent
                        ÊûÑÂª∫Âêå‰πâËØçËØçÂÖ∏Ôºöuse thesaurus
                        geneerate from 
                            Co-occurrence is more robust, grammatical relations are more accurate &
                            processÔºö   
                                A is a term-document matrix 
                                C = AA^T
                                cij is the number of times two terms ti and tj co-occur
                                For each ti, pick terms with high values in C
                                with a larger number being better
                                calcu & - NEXT p48 
                                we can calculate the 
                                    Nearest neighbors ** & -
                                    with this method 
                Query expansion:it may be as good as pseudo-relevance feedback
        ppt-11-Probabilistic Information Retrieval Á¨¨ 11 Á´† Ê¶ÇÁéáÊ£ÄÁ¥¢Ê®°Âûã --over --c
            file:///G:/i.Note-%E8%BF%87%E4%BA%94%E5%85%B3%E6%96%A9%E5%85%AD%E5%B0%86/2%E8%AF%BE%E4%B8%9A%E5%85%B3/TUD%E8%AF%BE%E7%A8%8B/WS1819%E7%AC%94%E8%AE%B0/%E8%87%AA%E5%AD%A6%E8%AF%BE%E4%BB%B6/pic/IR/ppt-11/Viewer.html
            ‰∏ÄÁßçÁõëÁù£ÁöÑÔºåÂü∫‰∫éÊ¶ÇÁéáÁöÑÔºåËøîÂõûÊéíÂêçÁöÑÊ£ÄÁ¥¢Á≠ñÁï•
                ËÆ≠ÁªÉÂàÜÁ±ªÂô®
            probability ranking principleÔºåÊ¶ÇÁéáÊéíÂ∫èÂéüÁêÜ
            ÊñáÊ°£ÂíåÊü•ËØ¢ÈÉΩË°®Á§∫‰∏∫ËØçÈ°πÂá∫Áé∞‰∏éÂê¶ÁöÑÂ∏ÉÂ∞îÂêëÈáè
            ËÆ∏Â§ö‰∏çÂêåÁöÑÊñáÊ°£ÂèØËÉΩÈÉΩÊúâÁõ∏ÂêåÁöÑÂêëÈáèË°®Á§∫„ÄÇ
            Áã¨Á´ãÊÄßÂÅáËÆæÂíåÂÆûÈôÖÊÉÖÂÜµÂæà‰∏çÁõ∏Á¨¶Ôºå‰ΩÜÂú®ÂÆûÈôÖ‰∏≠Â∏∏Â∏∏Âç¥ËÉΩÁªôÂá∫‰ª§‰∫∫Êª°ÊÑèÁöÑÁªìÊûú
            Sch√§tzung der Wahrscheinlichkeit, dass ein Dokument d 
                f√ºr eine Anfrage q relevant ist
            Beispiel
                1.Êî∂ÈõÜË°®Ê†º Âë®Âõ¥ÊòØÁªüËÆ°‰ø°ÊÅØ,Ê†πÊçÆÁî®Êà∑ÁªôÂá∫ÁöÑÂèçÈ¶àÔºåÂè™Ë¶ÅÊúâÂèçÈ¶àÂ∞±ÂèØ‰ª•
                2.Berechnung des Termgewichts
                    GewichtungsfunktionÔºöformular  & -
                3.ËÆ°ÁÆó Áä∂ÊÄÅÊ£ÄÁ¥¢ÂÄº Retrievalstatuswert
                    logÁõ∏Âä†
                    ËÆ°ÁÆó & -
            Âü∫Á°ÄÊ¶ÇÁéáÁü•ËØÜ
                    ‰∫ã‰ª∂ÁöÑ‰ºòÂäøÁéáÔºà oddsÔºâ   **
                        ÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèçÊò†Ê¶ÇÁéáÂ¶Ç‰ΩïÂèòÂåñÁöÑ‚Äú ÊîæÂ§ßÂô®‚Äù Ôºà multiplierÔºâ
                    Êó†ÂÖ≥ÊÄßÂà§Êñ≠ÔºåËÆ°ÁÆóÊù°‰ª∂Ê¶ÇÁéá 
                        ppt‰∏äÁöÑËÆ°ÁÆó
                    ct ÊòØÊü•ËØ¢ËØçÈ°πÁöÑ‰ºòÂäøÁéáÊØîÁéáÔºà odds ratioÔºâÁöÑÂØπÊï∞ÂÄº„ÄÇ
                        Áõ∏ÂÖ≥Âíå‰∏çÁõ∏ÂÖ≥Ôºå‰∏§‰∏™‰ºòÂäøÁéáÁöÑÊØîÂÄºÔºåÊúÄÂêéÂØπËøô‰∏™ÂÄºÂèñÂØπÊï∞
                    Â¶ÇÊûúËØçÈ°πÂú®Áõ∏ÂÖ≥Âíå‰∏çÁõ∏ÂÖ≥ÊñáÊ°£‰∏≠ÁöÑ‰ºòÂäøÁéáÁõ∏Á≠âÔºå ctÂÄº‰∏∫ 0„ÄÇ
                    Â¶ÇÊûúËØçÈ°πÊõ¥ÂèØËÉΩÂá∫Áé∞Âú®Áõ∏ÂÖ≥ÊñáÊ°£‰∏≠ÔºåÈÇ£‰πàËØ•ÂÄº‰∏∫Ê≠£„ÄÇ 
                    ct ÂÆûÈôÖ‰∏äÁªôÂá∫ÁöÑÊòØÊ®°Âûã‰∏≠ËØçÈ°πÁöÑÊùÉÈáçÔºåÁ≤íÂ∫¶ÊØîËæÉÂ∞èÔºåÊü•ËØ¢ÊñáÊ°£ÁöÑÂæóÂàÜÊòØRSVdÔºåÊòØctÁöÑÊ±ÇÂíå
                    ÂæóÂàÜÈ´òÂàôÁõ∏ÂÖ≥ÊÄßÂ§ßÔºåÊ†πÊçÆËØÑÂàÜÂèØ‰ª•ËøõË°åÊéíÂ∫èÂá∫k‰∏™È°πÁõÆ
                    Beispiel--Basic Model 
                    Einfache Wahrscheinlichkeit P(A) 
                    Theorem von Bayes 
                        Umkehren von SchlussfolgerungenÁõ∏Âèç
                        priori-WahrscheinlichkeitÂÖàÈ™åÊ¶ÇÁéá
                        Wahrscheinlichkeit f√ºr ein Ereignis B unter der BedingungÂêéÈ™åÊ¶ÇÁéá
                    Chance / Quote (Odds) statt Wahrscheinlichkeiten
                        Wenn P(A1) > P(A2), dann ist auch O(A1) > O(A2)
                    Unabh√§ngige EreignisseÁã¨Á´ãÊ¥ªÂä®
                        zwei W√ºrfel geworfen
                        Test auf Unabh√§ngigkeit &calcu - p
            Binary Independence Retrieval Model (BIR) **
                ‰∫åÂÄºÁã¨Á´ãÊ®°ÂûãBIR &
                Binary Independence Retrieval Model (BIR)
                a model to measure rank documents .give each document a weight ,value .
                ÔÇß Theorie und Definitionen
                    RSVdÔºöRetrievalstatuswert eines Dokuments
                    sim(dm,qk) = RSVÁõ∏‰ººÂ∫¶ËØÑÂàÜ
                    Anfrage-Vektor und Dokument-Vektor
                    Wahrscheinlichkeit der Relevanz, wenn eine Anfrage q und ein Dokument d gegeben sind?
                ÔÇß Retrievalstatuswert eines Dokuments (RSV)
                    Êé®ÂØºÔºÅÔºÅÔºÅ Ë¶ÅËÄÉÔºüÔºü & - Â§ßÊ¶ÇÂÖ∂Â∞±ÂèØ‰ª• NEXT ‰ª•ÂêéÁªÜÂåñ
                        ‰∫íË°•Ê¶ÇÁéáÁöÑËΩ¨Êç¢Komplement√§re Umformung der Wahrscheinlichkeit
                        NEXT
                        Anwendung eines Logarithmus
                            Grund? 
                ÔÇß Termgewichtungsfunktion
                    r : Wahrscheinlichkeit, dass der Term ti 
                        in einem f√ºr die Anfrage q relevanten Dokument d vorkommt
                    n : Wahrscheinlichkeit, dass der Term ti 
                        in einem f√ºr die Anfrage q nicht relevanten Dokument d vorkommt
                    ÂÖ∂ÂÆûÂ∞±ÊòØÂä†ÂíåËÄåÂ∑≤ÔºåÂêéÈù¢ÊâçÊúâÂèÇÊï∞‰øÆÊ≠£
                    +
                        Â∫îÁî®Ôºö  
                            Interaktives Relevance Feedback‰∫§‰∫íÁõ∏ÂÖ≥ÂèçÈ¶à
                            Ëá™Âä®Â§ÑÁêÜ
                        ‰ª•R ÈùûRÁöÑÂàÜÂ∏É‰∏∫Âü∫Á°Äauf der Basis der Verteilung 
                            in relevanten und nichtrelevanten Dokumenten
                ÔÇß Probabilistisches Relevance Feedback
                    ÂèØ‰ª•ÈÄöËøáÔºà‰º™ÔºâÁõ∏ÂÖ≥ÂèçÈ¶àÊäÄÊúØÔºå‰∏çÊñ≠Ëø≠‰ª£‰º∞ËÆ°ËøáÁ®ãÊù•Ëé∑Âæó ptÁöÑÊõ¥Á≤æÁ°ÆÁöÑ‰º∞ËÆ°ÁªìÊûú
                    Ëøô‰∏™ËøáÁ®ãÂè´ÂèÇÊï∞‰º∞ËÆ°ÔºöParametersch√§tzung durch Relevance Feedback (cont‚Äôd)
                    wenn es positiv ist ,in relevanten Dokumenten gr√∂√üer ist als in nicht relevanten Dokumenten
                    ÂÆûÈôÖ‰∏≠ÂæÄÂæÄË¶ÅÂØπ‰∏äËø∞‰º∞ËÆ°ËøõË°åÂπ≥ÊªëÔºåÊ≠§Êó∂ÂèØ‰ª•ÂØπÂåÖÂê´Âíå‰∏çÂåÖÂê´ËØçÈ°πÁöÑÊñáÊ°£Êï∞ÁõÆÈÉΩÂä†‰∏ä0.5 --Parameterkorrektur
                    ‰πüÂèØ‰ª•Âü∫‰∫é‰º™Áõ∏ÂÖ≥ÂèçÈ¶àÁöÑÊñπÊ≥ïÊù•ÂÆûÁé∞‰∏äËø∞ÁÆóÊ≥ï Rekursive Parametersch√§tzung
                        Áî®Êà∑ÂØπÊüê‰∏™ÊñáÊ°£Â≠êÈõÜ V ÁöÑÁõ∏ÂÖ≥ÊÄßÂà§Êñ≠
                        V ÂèØ‰ª•ÂàíÂàÜÊàê‰∏§‰∏™Â≠êÈõÜÔºö VR and VNR ÊòØÂê¶Áõ∏ÂÖ≥
                        ËØçÈ°π t Âá∫Áé∞Âú®Áõ∏ÂÖ≥ÊñáÊ°£ÁöÑÊØî‰æã
                        NEXT Ëø≠‰ª£ËøáÁ®ãÂÆû‰æãÁÆóÊ≥ï & - NEXT p47 ‰π¶‰∏≠p158
                            Anfangssch√§tzung Ôºü 
                        Erzeugung neuer Termgewichte
                        Korrekturwerte
                    ËøôÈáåÂØπlogÊòØÊï∞ÂÄºÊ±ÇÂíåÔºåËÄå‰∏çÊòØÊ±ÇÁßØÔºåtf-idfÊòØÊ±ÇÊú∫
            Okapi 
                +
                    Áî®Êù•ÂØπÂåπÈÖçÊñáÊ°£ËøõË°åÊéíÂ∫èÁöÑÂáΩÊï∞
                    BM ÊòØ Best Matching (ÊúÄ‰Ω≥ÂåπÈÖç) ÁöÑÁº©ÂÜô
                    Âú®Ê¶ÇÁéáÊêúÁ¥¢ÁöÑÊ°ÜÊû∂‰∏ãË¢´ÊèêÂá∫ÁöÑ„ÄÇOkapi ÊòØÁ¨¨‰∏Ä‰∏™‰ΩøÁî®ËøôÁßçÊñπÊ≥ïÁöÑ‰ø°ÊÅØËé∑ÂèñÁ≥ªÁªüÁöÑÂêçÁß∞
                    ÊñáÊ°£ËØÑÂàÜÁ∫µÂêëÊØîËæÉÔºö & -
                        1.tf-idf 
                        2.Âü∫‰∫éBIR‰∫åÂÄºÊ¶ÇÁéáÊ®°Âûã
                            Ê≤°ÊúâËÄÉËôëÊñáÊ°£ÈïøÂ∫¶
                        3.Âü∫‰∫éOkapi
                            Âü∫‰∫éËØçÈ°πÈ¢ëÁéá„ÄÅÊñáÊ°£ÈïøÂ∫¶Á≠âÂõ†Â≠êÊù•Âª∫Á´ãÊ¶ÇÁéáÊ®°ÂûãÁöÑ‰∏ÄÁßçÊñπÊ≥ï
                            Termgewichtungsfunktionen‰∏çÂêåÔºåÂºïÂÖ•Êõ¥Â§öÂèÇÊï∞
                    OkapiÊòØtf idf‰ª•Âèä‰∏Ä‰∫õÂèÇÊï∞ÁöÑÁªÑÂêàËÄåÂ∑≤
                    Â¶ÇÊûúÊêúÁ¥¢ËØç‰∏≠ÂåÖÂê´ÊØîËæÉÁã¨ÁâπÁöÑËØçÔºåÂàô‰ºöÊèêÂçáÂàÜÊï∞ÔºõÊêúÁ¥¢ËØçÂú®‰∏Ä‰∏™ÊñáÊ°£‰∏≠Âá∫Áé∞ÁöÑÊ¨°Êï∞Ë∂äÈ´òÔºåÂàÜÊï∞‰πü‰ºöÊõ¥È´òÔºõ‰ΩÜÊñáÊ°£Ë∂äÈïøÔºåÂàÜÊï∞‰ºöË∂ä‰Ωé„ÄÇ
                    BM25ËæÉÊñ∞ÂçáÁ∫ßÁâà
                        BM25FÔºåÊääÊñáÊ°£ÁªìÊûÑÂíåÈîöÊñáÊú¨‰πüËÄÉËôëËøõÊù•„ÄÇÂè¶‰∏Ä‰∏™Âè´ 
                        BM25+ÔºåÂè™ÊòØÂú®‰∏äÈù¢ÂÖ¨Âºè‰∏≠ÁöÑÊñπÊã¨Âè∑ÈáåÂä†‰∫Ü‰∏Ä‰∏™ Œ¥ÔºåÁî®Êù•Âº•Ë°•ÂéüÊù•ÂÖ¨ÂºèÂØπË∂ÖÈïøÊñáÊ°£ÁöÑ‰∏çÂÖ¨„ÄÇ
                        Erweiterung der BM25 Gewichtung f√ºr den Fall dass sehr lange Abfragen auftreten
                    Âè¶Â§ñÁöÑÊ®°ÂûãÔºö
                        ElasticSearch/Lucene ÁöÑÂàÜÊï∞ËÆ°ÁÆó
                            ElasticSearch Â∫ïÂ±ÇÈááÁî®‰∫Ü LuceneÔºå
                                ËÄå Lucene ÁöÑÂàÜÊï∞ËÆ°ÁÆóÁªºÂêà‰∫ÜÂ∏ÉÂ∞îÊ®°Âûã
                                (Boolean model), TF-IDF, ‰ª•ÂèäÁü¢ÈáèÁ©∫Èó¥Ê®°Âûã„ÄÇ
                    BM25ÊòØ‰∏ÄÁßçBOWÔºàbag-of-wordsÔºâÊ®°Âûã
                    BM25ÁÆóÊ≥ïÈ¶ñÂÖàÁî±OkapiÁ≥ªÁªüÂÆûÁé∞ÔºàOkapiÊòØ‰º¶Êï¶ÂüéÂ∏ÇÂ§ßÂ≠¶ÂÆûÁé∞ÁöÑ‰ø°ÊÅØÊ£ÄÁ¥¢Á≥ªÁªüÔºâÔºåÊâÄ‰ª•ÂèàÁß∞‰∏∫Okapi BM25
                    Âú®ËÆ°ÁÆóIDFÊó∂ÔºåÂ¶ÇÊûúË¢´Êü•ËØ¢ÁöÑËØçËØ≠‰∏çÂú®ËØ≠ÊñôÂ∫ì‰∏≠ & -
                        Â∞±‰ºöÂØºËá¥ÂàÜÊØç‰∏∫Èõ∂ÔºåÊâÄ‰ª•ÈÄöÂ∏∏‰ºöÂä†‰∏ä‰∏Ä‰∏™ËæÉÂ∞èÁöÑÊï∞‰ª•‰øùËØÅÂàÜÂ∏É‰∏ç‰∏∫Èõ∂
                    ÊòØÂØπtf-idfÁöÑÂçáÁ∫ßÔºöOkapi BM25 ÊòØÂà∞ÁõÆÂâç‰∏∫Ê≠¢Ë¢´ËÆ§‰∏∫ÊúÄÂÖàËøõÁöÑÊéíÂêçÁÆóÊ≥ï‰πã‰∏Ä
                    È•±ÂíåÂ∫¶Ôºö
                        Êó¢ÁÑ∂‰∏§‰∏™‰∏§‰∏™ÊñáÊ°£ÈÉΩÊòØÂ§ßÁØáÂπÖËÆ®ËÆ∫Ê£íÁêÉÁöÑÔºåÈÇ£‰πà‚ÄúÊ£íÁêÉ‚ÄùËøô‰∏™ËØçÂá∫Áé∞ 40 
                        Ê¨°ËøòÊòØ 80 Ê¨°ÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ‰∫ãÂÆû‰∏äÔºå30 Ê¨°Â∞±ËØ•Â∞ÅÈ°∂Âï¶ÔºÅ
                BM11, BM15, BM25ÈÉΩÂ±û‰∫éBM25ÁÆóÊ≥ïÂÆ∂Êóè
                Erweiterung der BM25 Gewichtung f√ºr den Fall dass sehr lange Abfragen auftreten
                ÂÖ¨Âºè*2 & - NEXT
                ÊÄªÁªìÊØîËæÉ 
                    Âú®Âì™Èáå‰ΩøÁî®Ôºü & - NEXT explain 
                        Suche √§hnlicher Dokumente
                        ohne besondere Verfahren multilingual
                    Vgl. der klassischen IR-Modelle & - NEXT explain
                    Besides the big error in estimating the
                        probabilities the classification is still
                        correct
                    Ê¶ÇÁéáIR‰ºòÁº∫ÁÇπ & - NEXT explain
                        vor 
                            it can be very quick 
                            good theoretical background 
                        nach 
                            Need to guess the initial ranking
                                user have to mark some of docs as relevant or not
                            the term frequency will be ignored
                            //Independence assumption
        ppt-12-Language Models for IR Á¨¨ 12 Á´† Âü∫‰∫éËØ≠Ë®ÄÂª∫Ê®°ÁöÑ‰ø°ÊÅØÊ£ÄÁ¥¢Ê®°Âûã  --over --c
            +lan corpus
                a double circle indicates a (possible) finishing state.
                    here can we find a double circle . it means the state is finished  
                idea: 
                    for each doc. we generate a LM
                    then ,we can judge which doc is likely to generate the query 
                    it is a method to measure the relevance of a document d and a query ...
                A language model is constructed -for each document -in the collection
                some of the strings in the language it generates    
                    these strings can be gen. by the LM here 
                After generating each word, we decide whether to stop or to loop around
                a mathematical model of computation
                The FSM can change from one state to another in response to some external inputs
                    with some input data , state can be changed from to an other
                    the number of stats has a limit
                    external input can cause transition
                initial state, and the conditions for each transition
                A combination lock is a type of locking device
                This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows)
                An automaton is a finite representation of a formal language that may be an infinite set
                Automata play a major role in theory of computation, compiler construction, artificial intelligence, parsing and formal verification. 
                to calcu. , we Multiply the probabilities together. 
                MATH
                    dividing is the opposite of multiplying
                    Numerator and denominator!!!
                    decimals, percentages and fractions. 
                    +https://www.cs.cmu.edu/~venkatg/teaching/codingtheory/




            +    
                idea & -
                ÊåâÁÖßÊ®°ÂûãÁîüÊàêÊü•ËØ¢ÁöÑÊ¶ÇÁéáÊù•ËøõË°åÊéíÂêç
                ÂØπÊØè‰∏™ÊñáÁ´†ÈÉΩÁîüÊàê‰∏Ä‰∏™ËØ≠Ë®ÄÊ®°Âûã & - exp -
                    ::
                        we gener. a Lan. Model for each doc.
                        the LM can be used to calcu. the relevant between doc and query 
                        then , we can rank the docs 
                ÊúâÁ©∑Ëá™Âä®Êú∫
                    ËØ≠Ë®ÄÔºöÊâÄÊúâÂèØËÉΩÁöÑÂ≠óÁ¨¶‰∏≤ÁöÑÂÖ®ÈõÜ & - exp
                        ::the lan. means all possible strings that LM can accept 
                    ÂèåÂúàËäÇÁÇπÂØπÂ∫îÁöÑÊòØÔºàÂèØËÉΩÁöÑÔºâÁªàÊ≠¢Áä∂ÊÄÅ
                ËØ≠Ë®ÄÊ®°ÂûãÔºö 
                        ÊØè‰∏™ËäÇÁÇπÈÉΩÊúâ‰∏Ä‰∏™ÁîüÊàê‰∏çÂêåËØçÈ°πÁöÑÊ¶ÇÁéáÂàÜÂ∏É
                        ËøòÈúÄË¶Å‰∏Ä‰∏™Âú®ÁªàÊ≠¢Áä∂ÊÄÅÂÅúÊ≠¢ÁöÑÊ¶ÇÁéá
                        ‰∏ÄÂÖÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÂçïÁä∂ÊÄÅÊúâÁ©∑Ëá™Âä®Êú∫ ‰π¶‰∏≠ see p164
                        ‰∏Ä‰∏™ÂÖ∑‰ΩìÁöÑÂ≠óÁ¨¶‰∏≤ÊàñÊñáÊ°£ÁöÑÊ¶ÇÁéáÂæÄÂæÄÈùûÂ∏∏Â∞è
                        ÂÅúÊ≠¢Ê¶ÇÁéáÔºö‰∏ç‰ºöÂΩ±ÂìçÊñáÊ°£ÁöÑÊéíÂ∫è
                        ‰∏ÄÂÖÉËØ≠Ë®ÄÊ®°Âûã 
                            ËÆ°ÁÆóÊù°‰ª∂Ê¶ÇÁéáÊó∂‰∏çËÄÉËôëÂâç‰∏Ä‰∏™ËØçÈ°πÁöÑÂá∫Áé∞ÊÉÖÂÜµ
                            ËÆ°ÁÆó‰æãÂ≠ê & - p165 ‰æã12-2
                            ‰æãÈ¢òÔºö
                                ‰ΩÜÊòØÈÄöÂ∏∏Âú®Ê¶ÇÁéáÂ∫îÁî®‰∏≠
                                    ÂÆûÈôÖ‰∏äÂæÄÂæÄÈááÁî®ÂØπÊï∞Ê±ÇÂíåÁöÑËÆ°ÁÆóÊñπÊ≥ï
                                    Âõ†‰∏∫ËÆ°ÁÆóÊú∫Á≤æÂ∫¶ÈóÆÈ¢ò
                                ‰∏§‰∏™‰∏ÄÂÖÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÈÉ®ÂàÜÊ¶ÇÁéáËµãÂÄº
                            Â§ßÈÉ®ÂàÜÂ∑•‰ΩúÈÉΩÂè™‰ΩøÁî®‰∫Ü‰∏ÄÂÖÉÊ®°Âûã
                            Âú®‰∏ÄÂÖÉËØ≠Ë®ÄÊ®°Âûã‰∏≠ÔºåËØçÂá∫Áé∞ÁöÑÂÖàÂêéÊ¨°Â∫èÊó†ÂÖ≥Á¥ßË¶Å
                        ‰∫åÂÖÉËØ≠Ë®ÄÊ®°Âûã 
                            ËÆ°ÁÆóÊù°‰ª∂Ê¶ÇÁéáÊó∂Âè™ËÄÉËôëÂâç‰∏Ä‰∏™ËØçÈ°πÁöÑÂá∫Áé∞ÊÉÖÂÜµ
                        Âü∫‰∫éÊñáÊ≥ïÁöÑËØ≠Ë®ÄÊ®°Âûã &
                            ÂØπ‰∫éËØ∏Â¶ÇËØ≠Èü≥ËØÜÂà´„ÄÅÊãºÂÜôÊ†°ÂØπ„ÄÅÊú∫Âô®ÁøªËØëÁ≠âÈúÄË¶Å
                                Ê†πÊçÆ‰∏ä‰∏ãÊñáÊù•Ê±ÇËØçÈ°πÊù°‰ª∂Ê¶ÇÁéáÁöÑÂ∫îÁî®ÈùûÂ∏∏ÈáçË¶Å
                            NEXT
                        +‰∫åÈ°πÂºèÂàÜÂ∏ÉÔºåÂ§öÈ°πÂºèÂàÜÂ∏É
                            ‰∫åÈ°πÂºèÔºöÊØè‰∏™‰∫ã‰ª∂ÂèñÂÄºÊòØ2ÔºåÁªÑÊàê‰∏Ä‰∏™Â∫èÂàóÁöÑÊ¶ÇÁéá
                            Â§öÈ°πÂºèÔºöÊØè‰∏™‰∫ã‰ª∂ÂèñÂÄºÊòØ1/NÔºåÁªÑÊàê‰∏Ä‰∏™Â∫èÂàóÁöÑÊ¶ÇÁéá
                            ‰∫åÈ°πÂºèÊ¶ÇÁéáÂíå‰∫åÂ∑∑Â±ïÂºÄÂºèÁ≥ªÊï∞ÂÖ≥Á≥ª &
                Einsatz im IR--Êü•ËØ¢‰ººÁÑ∂Ê®°Âûã
                    ËÄÉËôëÊü•ËØ¢Ôºü
                        Êàë‰ª¨ÂØπÊñáÊ°£ÈõÜ‰∏≠ÁöÑÊØèÁØáÊñáÊ°£ d ÊûÑÂª∫ÂÖ∂ÂØπÂ∫îÁöÑËØ≠Ë®ÄÊ®°Âûã Md
                        Êàë‰ª¨ÁöÑÁõÆÊ†áÊòØÂ∞ÜÊñáÊ°£ÊåâÁÖßÂÖ∂‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑ‰ººÁÑ∂ P(d|q) ÊéíÂ∫è„ÄÇ
                        ‰ΩøÁî®Ë¥ùÂè∂ÊñØÂÖ¨ÂºèÔºå Âè™ÈúÄË¶ÅÂØπP(q|d)ÊéíÂ∫è ÔºåÂÆÉÊòØÂú®ÊñáÊ°£ d ÂØπÂ∫îÁöÑËØ≠Ë®ÄÊ®°Âûã‰∏ãÁîüÊàê q ÁöÑÊ¶ÇÁéá
                        ÂÖ∑‰ΩìÊÄé‰πàÊü•ÔºåÊ¶ÇÁéá‰πòÁßØ & - exp    
                            ::
                                we multiply the probabilities together to estimate the result
                                it is usually approximated by considering each term from the retrieved document 
                                the model is unknown 
                            ÊØèÁØáÊñáÊ°£Âú®‰º∞ËÆ°‰∏≠ÈÉΩÊòØ‰∏ÄÈó®Áã¨Á´ãÁöÑ‚Äú ËØ≠Ë®Ä‚Äù
                            ‰∏Ä‰∏™ÂÖ∑‰ΩìÁöÑÂ≠óÁ¨¶‰∏≤ÊàñÊñáÊ°£ÁöÑÊ¶ÇÁéáÂæÄÂæÄÈùûÂ∏∏Â∞è 
                    +ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°   
                        Ê≠£ÊÄÅÂàÜÂ∏ÉÔºåÂèñÂæó‰∏Ä‰∫õÁÇπÔºåÂèçËøáÊù•‰º∞ËÆ°ÂèÇÊï∞Ôºå‰º∞ËÆ°ÂáΩÊï∞ÁöÑÁä∂ÂÜµ 
                        ‰∏ç‰∏ÄÂÆöÂ≠òÂú®Ôºå‰πü‰∏ç‰∏ÄÂÆöÂîØ‰∏Ä„ÄÇ
                    Âú® IR ÁéØÂ¢É‰∏ãÔºåËøòÊúâÂÖ∂‰ªñ‰ΩøÁî® LM ÁöÑÊÄùË∑Ø
                    Èõ∂Ê¶ÇÁéáÈóÆÈ¢ò
                        Áî±‰∫éÂæàÂ§öËØçÂú®ËÆ≠ÁªÉÊï∞ÊçÆ‰∏äÂ≠òÂú®Á®ÄÁñèÊÄßÔºåÊâÄ‰ª•Âú®È¢ÑÊµã‰∏ã‰∏Ä‰∏™ËØçÊó∂ÂèØËÉΩ‰ºöÈÅáÂà∞Èõ∂Ê¶ÇÁéáÈóÆÈ¢ò
                        ‰ΩøÁî®Âπ≥Êªë
                            ÁÆÄÂçïÂπ≥ÊªëÔºåÂΩí‰∏ÄÂåñ
                            ÂºïÂÖ•ÂèÇÁÖßÊ¶ÇÁéáÂàÜÂ∏ÉÂáΩÊï∞
            -------
                Einf√ºhrung
                    Statistische Sprachmodelle
                        Êú¨Ë¥®‰∏äÊòØÂï• & - exp
                            ::essentially a method to rank docu.
                        Áî®‰∫éÊÄªÁªìËøôÊñáÁ´†ÊòØ‰ªÄ‰πà‰∏™ËßÑÂæãÁîüÊàêÁöÑ
                        Ôºöbeschreibt die Erzeugung von Texten

                        Â∫îÁî®
                            ÔÇß Erkennung gesprochener Sprache
                            ÔÇß Part-of-Speech-Tagger
                            ÔÇß Digitalisierung handschriftlicher Texte
                    Funktionsweise
                        ÊèèËø∞ÊúâÈôêËá™Âä®Êú∫ EA & - explain
                            Doppelkreis kennzeichnet Ôºü
                                den (m√∂glichen) Endzustand desEA (=Ende der Zeichenkette)
                            ÁÆ≠Â§¥‰ª£Ë°®Âï•...
                            ::
                                stands for transformation(transition) 
                                double circle means finishing state
                        ËÆ°ÁÆó P(Katzen fangen M√§use) & - 
                        Mehrere Modelle M1, M2, ..., M Âà§Êñ≠ËÆ°ÁÆó & -
                            Â§ßOÂá†ÁéáÔºü
                            ppt‰∏äÊúâÁÇπÈóÆÈ¢ò
                        unigram language model
                            P(Hunde, fangen, Katzen) = P(Katzen, Hund, fangen) 
                        ‰∫åÂÖÉËØ≠Ë®ÄÊ®°Âûã‰∏∫Âï•‰∏çÁî® & - exp
                            Zu speziell, zu aufw√§ndig
                            ‰ΩøÁî®bedingte Wahrscheinlichkeiten
                            ::
                                it uses Conditional probability , complex/difficult to compute 

                    Modelle / Markow-Ketten
                        ‰ΩøÁî®È©¨Â∞îÁßëÂ§´Èìæ
                            ‰∏Ä‰∏™È©¨Â∞îÁßëÂ§´ËøáÁ®ãÊòØÁä∂ÊÄÅÈó¥ÁöÑËΩ¨Áßª‰ªÖ‰æùËµñ‰∫éÂâçn‰∏™Áä∂ÊÄÅÁöÑËøáÁ®ã„ÄÇËøô‰∏™ËøáÁ®ãË¢´Áß∞‰πã‰∏∫nÈò∂È©¨Â∞îÁßëÂ§´Ê®°Âûã
                            kann man LM durch Markow-Ketten darstellen & -
                    HorizontÔºàÂêëÂâçÁúãÁöÑËßÜÈáéÔºâ
                        durch Analyse n- vorangegangener W√∂rter
                        ËßÜÈáé n
                            - n = 2 : Trigramm LM
                            - n = 1 : Bigramm LM
                            - n = 0 : Unigramm LM
                Einsatz im IR
                    Ranking mit absteigender Wahrscheinlichkeit 
                    AberÔºöAnfrage q zu kurz f√ºr LM
                    Vereinfachungen, Annahmen
                        fest f√ºr Anfrage q (kann somit weggelassen werden) 
                        :ÂØπÊØè‰∏™qÊü•ËØ¢ÔºåÊòØÂõ∫ÂÆöÁöÑÔºåÂõ†Ê≠§ÂèØ‰ª•ÂéªÊéâ
                        F√ºr jedes Dokument d ein Sprachmodelle Md ableiten
                        ÔºöÊØè‰∏™ÂΩ¢Êàê‰∏Ä‰∏™ËØ≠Ë®ÄÊ®°Âûã & -
                        PÔºàd|qÔºâWird zum Zeitpunkt der Anfrage berechnet
                        BUT:Zu wenig Daten f√ºr komplexes Language Models
                            losung:‰ΩøÁî®‰∏ÄÂÖÉÊ®°Âûã
                        Multinomialverteilung 
                        ÔºöÂ§öÈ°πÂàÜÂ∏ÉÔºàÁî®‰∫éËÆ°ÁÆó‰∏ÄÂÖÉÊ®°ÂûãÊ¶ÇÁéáÔºâ
                    Sch√§tzen von Wahrscheinlichkeiten
                        Sch√§tzer √ºber relative H√§ufigkeit 
                            p20ÂÖ¨Âºè & -
                            ÂÖ∂ÂÆûÂ∞±ÊòØÁî®tf/d‰ª£ÊõøËá™Âä®Êú∫‰∏≠ÁöÑÊ¶ÇÁéá
                    Smoothing
                        um Nullwerte zu verhindern
                        1.‰ΩøÁî®Gesamtkorpus 
                            idea & -
                        2.Jelinek-Mercer Smoothing 
                            idea & -
                                Linearkombination 
                                ÔºöÂ±ÄÈÉ®ÁöÑÊúâÊó∂ÂÄôÊòØ0 ÊâÄ‰ª•ÂºïÂÖ•ÂÖ®Â±ÄÁöÑÂÅöÂπ≥Êªë
                            p23ÂÖ¨Âºè & -
                                ÂèÇÊï∞Â§ßÂ∞èÁöÑÂΩ±Âìç
                                Hoher Wert: kaum / kein Smoothing
                                Niedriger Wert: fast nur Korpuswerte
                                ËøôÈáåÁöÑÈõ∂Èò∂ÊåáÁöÑÊòØÂÖ®Â±ÄÊ¶ÇÁéá 
                                ‰∏Ä‰∏™documentÊú¨Êù•ÊòØÁõ∏ÂÖ≥ÁöÑÔºå‰ΩÜÊòØÂõ†‰∏∫Êü•ËØ¢ÈïøÔºåÊü•ËØ¢‰∏≠
                                    ÂæàÂ§öÂú®ÊñáÁ´†‰∏≠ÊúâÂèØËÉΩÂπ∂Ê≤°ÊúâÂá∫Áé∞„ÄÇ
                                    ÊâÄ‰ª•ËØ¥Âπ≥ÊªëÔºåÂØπ‰∫éÈïøÁöÑÊü•ËØ¢ÊòØÊúâÂà©ÁöÑ **
                                    Viele Anfrageterme: st√§rkeres Smoothing
                                    Wenige Anfrageterme: schwaches Smoothing
                            ËæìÂÖ•‰∏∫Tasse KanneÁöÑÈÇ£‰∏™ËÆ°ÁÆó & - NEXT
                                ÂéüÊù•ÁöÑË°®Ê†ºÔºåÊ®™ÂêëÂä†Ëµ∑Êù•ÊòØ1Ôºå‰ΩÜÊòØÊúâÂæàÂ§ö0
                            Alternative VorgehensweisenÔºüËøòÊúâÂï•Êõø‰ª£ÁöÑ‰∏ç
                                & NEXT
                        +ÂêÑÁßçÂπ≥ÊªëÊñπÊ≥ï
                            Âä†Ê≥ïÂπ≥Êªë
                                ÊúÄÊú¥Á¥†ÁöÑÊÄùÊÉ≥Â∞±ÊòØÂØπÊâÄÊúâ‰∫ã‰ª∂ÁöÑÂá∫Áé∞Ê¨°Êï∞Âä†1
                            Âè§Âæ∑-ÂõæÁÅµÔºàGood-TuringÔºâ‰º∞ËÆ°
                            KatzÂπ≥Êªë
                            Jelinek-MercerÂπ≥Êªë
                                Êãøbi-gram‰∏æ‰æãÔºåÂ¶ÇÊûú‰∏Ä‰∏™bi-gramÊ≤°ÊúâÂá∫Áé∞ËøáÔºå
                                ‰∏Ä‰∏™Êú¥Á¥†ÁöÑÊÄùÊÉ≥ÊòØÈÄÄ‰∏ÄÊ≠•ÔºåÁúã‰∏Ä‰∏ãuni-gramÁöÑÊ¶ÇÁéáÔºå
                                ÁÑ∂ÂêéÂ§ßÊ¶ÇÊé®Êµã‰∏Ä‰∏ãbi-gramÁöÑÊ¶ÇÁéá
                            Ë¥ùÂè∂ÊñØÂπ≥Êªë
                    ÂÆûÈôÖÂ∫îÁî®
                        Multiplikation kleiner Werte F√ºhrt zu Rechenungenauigkeiten & -
                            Summe statt Produkt
                            Logarithmieren des Wertes rechnen
                            :using log function to calcu. **
                        Term-at-a-time
                            ‰∏ÄË°å‰∏ÄË°åÁ¥¢ÂºïË°®ËøõË°åÂ§ÑÁêÜ
                            Akkumulatoren f√ºr die Dokumente
                            ÊØè‰∏™doc‰∏Ä‰∏™Á¥ØÂä†Âô® 
                            NEXT
                Zusammenfassung 
                    LMs vs. BIR
                        ËÅîÁ≥ª & - NEXT
                            term are independent
                        Âå∫Âà´ & - NEXT
                            each document forms a class vs - only two classes relevant or not 
                            the smoothing method is different 
                    LMs vs. Vector-Space-Modell
                        ËÅîÁ≥ª & - NEXT
                            considering term frequency 
                            //also normalized ,and has no relation to the length of the document 
                            the smoothing of it has something to do with idf 
                        Âå∫Âà´ & - NEXT
                            vector model:: based on geometric similarity vs .. based on probability model 
                            //normalized in different ways 
                    Language-Modeling-Ansatz besser als Vector-Space-Modell
                    zwei Alternative Vorgehensweisen
                    Jedes Dokument bildet eine eigene Klasse (LM) 
                    vs. Klassen (relevant und nicht relevant) 
                    Jedes Dokument bildet eine eigene Klasse (LM) VS. die durch Menschen definiert werden (BIR) 
                    Jelinek-Mercer Smoothing vs. Addition kleiner Werte :: Jelinek-Mercer Smoothing vs. Addition kleiner Werte
        ppt-13-Web Search Basics Á¨¨ 19 Á´† Web ÊêúÁ¥¢Âü∫Á°Ä --over --c
            ÂÖ≥‰∫éweb‰∏Ä‰∫õÂü∫Êú¨ÁöÑ‰∏úË•ø
            webÂåÖÊã¨ÂæàÂ§öÂæàÂ§öÊñπÈù¢ÔºåÊØè‰∏™ÊñπÈù¢ÈÉΩÊúâÂæàÂ§öÂèØ‰ª•ËØ¥ & -
            ‰∏∫‰ªÄ‰πàÁ∫∑ÁπÅÊùÇ‰π±„ÄÅÂèòÂåñËøÖÈÄüÁöÑ Web‰∏çÂêå
                Êó†Ê≥ïÈõÜ‰∏≠ÊéßÂà∂ÁöÑÊó†‰∏≠ÂøÉÁöÑÁΩëÈ°µÂÜÖÂÆπÂèëÂ∏ÉÊú∫Âà∂
                ::
                    there is no central control within the web 
            web IRÁâπÁÇπ & - exp

                    very large
                    lots of duplicates
                    lots of spam 


                    ÁΩëÈ°µÊ∞ë‰∏ªÂåñÔºöÈÄ†Êàê
                        ÁΩëÈ°µ‰∏≠Â≠òÂú®Â§ßÈáèËØ≠Ê≥ïÂíåÈ£éÊ†º‰∏äÁöÑÂ∑®Â§ßÂ∑ÆÂºÇ 
                    Web ‰∏≠ÂåÖÂê´ÁúüÁêÜ„ÄÅË∞éË®Ä„ÄÅÁüõÁõæÂíåÂ§ßÈáèÁåúÊµã„ÄÇ 
                    ::
                        1.different pages have different styles and maybe diff. gramma
                            write in diff. ways 
                        2.There are truths and lies on the Internet 
                Êàë‰ª¨Â∫îËØ•Áõ∏‰ø°Âì™‰∫õ Web ÁΩëÈ°µÔºü
                ÂØπÊüê‰∏™Áî®Êà∑ÂèØ‰ø°ÁöÑÁΩëÈ°µÂÜÖÂÆπ‰∏ç‰∏ÄÂÆöÂØπÂÖ∂‰ªñÁî®Êà∑ÂèØ‰ø°
            Á¥¢ÂºïËßÑÊ®°‰º∞ËÆ°
                Êüê‰∏™ÊêúÁ¥¢ÂºïÊìé‰∏≠Á¥¢ÂºïÁöÑÁΩëÈ°µÊï∞ÁõÆÊòØÂ§öÂ∞ëÔºü
                ÊâÄË∞ìÈùôÊÄÅÁΩëÈ°µÔºà static web pageÔºâÔºåÊåáÁöÑÊòØÈÇ£‰∫õÂÜÖÂÆπ‰∏ç‰ºöÂõ†ËØ∑Ê±Ç‰∏çÂêåËÄå‰∏çÂêåÁöÑÁΩëÈ°µ
                ‰∏Ä‰∏™Âä®ÊÄÅÁΩëÈ°µÁîüÊàêÁöÑ‰æãÂ≠ê„ÄÇËøôÁßçÈ°µÈù¢ÁöÑ‰∏Ä‰∏™Ê†áÂøóÊòØURL ‰∏≠ÈÄöÂ∏∏ÂåÖÂê´Â≠óÁ¨¶‚Äú ?‚Äù „ÄÇ
                Âä®ÊÄÅÁΩëÈ°µÊúâÊï∞ÊçÆËØ∑Ê±ÇÁöÑËøáÁ®ã
                ÊµèËßàÂô®ÂèëÈÄÅÊúâÂÖ≥ AA129 Ê¨°Ëà™Áè≠ÁöÑËØ∑Ê±ÇÁªô Web Â∫îÁî®ÊúçÂä°Âô®ÔºåÊúçÂä°Âô®‰ªéÂêéÁ´ØÊï∞ÊçÆÂ∫ì
                    ‰∏≠Ëé∑Âèñ‰ø°ÊÅØÂπ∂‰∏îÁîüÊàê‰∏Ä‰∏™Âä®ÊÄÅÁΩëÈ°µËøîÂõûÁªôÊµèËßàÂô®
                ÈùôÊÄÅÈ°µÈù¢‰∏éÂä®ÊÄÅ  & - exp  
                    Âä®ÊÄÅÈ°µÈù¢Ôºà dynamic pageÔºâ
                        ÈÄöÂ∏∏ÊòØÁî±Â∫îÁî®ÊúçÂä°Âô®Â∫îÁ≠îÊï∞ÊçÆÂ∫ìÁöÑÊü•ËØ¢ÈúÄÊ±ÇÊó∂‰∫ßÁîüÁöÑ 
                    ÈùôÊÄÅÁΩëÈ°µÔºàstaticÔºâ
                        Ê≤°Áî®post getÊñπÊ≥ïÁöÑÂ∞±ÊòØ
                    ::
                        need to query the database 
                        load the web content without post or get method 
            webÂõæ
                Áîª‰∏Ä‰∏™ÁÆÄÂçïÁöÑ & -
                    ÈùôÊÄÅ HTML ÁΩëÈ°µÈÄöËøáË∂ÖÈìæÊé•‰∫íÁõ∏ËøûÊé•ËÄåÊàêÁöÑÊúâÂêëÂõæ
                    ÊØè‰∏™È°∂ÁÇπ‰ª£Ë°®‰∏Ä‰∏™ÁΩëÈ°µÔºåÔº°ÁΩëÈ°µ‰∏äÊúâ‰∏Ä‰∏™Ë∂ÖÈìæÊé•ÊåáÂêëÔº¢ 
                ËØ•ÊúâÂêëÂõæÂèØËÉΩ‰∏çÊòØ‰∏Ä‰∏™Âº∫ËøûÈÄöÔºà strongly connectedÔºâÂõæÔºå‰πüÂ∞±ËØ¥Ôºå‰ªé‰∏Ä‰∏™ÁΩëÈ°µÂá∫ÂèëÔºåÊ≤øÁùÄË∂ÖÈìæÊé•ÂâçËøõÔºåÊúâÂèØËÉΩÊ∞∏Ëøú‰∏ç‰ºöÂà∞ËææÂè¶Â§ñÊüê‰∏™ÁΩëÈ°µ„ÄÇ &
                Âú®‰∏ÄÁ≥ªÂàóÁ†îÁ©∂‰∏≠ÂæóÂà∞ÁöÑÁΩëÈ°µÁöÑÂπ≥ÂùáÂÖ•Â∫¶Â§ßÊ¶Ç‰ªé 8 Âà∞ 15 Â∑¶Âè≥‰∏çÁ≠â
                ËØ•Âõæ‰∏çÊòØÂº∫ËøûÈÄöÂõæÔºåÂõ†‰∏∫ B ‰∏çÂèØËÉΩÂà∞ A
                ÊúâÂ§ßÈáèÁ†îÁ©∂Ë°®ÊòéËøô‰∏™ÂàÜÂ∏ÉÊª°Ë∂≥ÂπÇÂàÜÂ∏ÉÂÆöÂæãÔºà power lawÔºâÔºåÂÖ∑ÊúâÂÖ•Â∫¶‰∏∫iÁöÑÁΩëÈ°µÊÄªÊï∞ÁõÆÊ≠£ÊØî‰∫é 1/iŒ±ÔºåÁ†îÁ©∂‰∏≠‰∏Ä‰∏™Êúâ‰ª£Ë°®ÊÄßÁöÑŒ±ÂÄºÊòØ 2.1
                Êï¥‰∏™WebÊúâÂêëÂõæÁªìÊûÑÔºü & -
                    ÊòØ‰∏™Ëù¥Ëù∂Áªì(bowtie)ÂΩ¢ 
                    ÂàÜÂà´ÊòØIN„ÄÅ OUTÂíåSCCÂíåÁÆ°ÈÅì
                    ËøòÊúâ‰∏Ä‰∫õ‰∏çËÉΩ‰ªé IN Âà∞ËææÊàñËÄÖ‰∏çËÉΩÂà∞Ëææ OUTÁöÑÁΩëÈ°µÊûÑÊàêÁöÑÊâÄË∞ìÂç∑È°ªÔºà tendril , tubes , disconnect pagesÔºâ
            ‰ΩúÂºäÈóÆÈ¢òÔºö 
                Âá†ÁßçÊñπÊ≥ïÔºü & -
                    Â∞ÜÈáçÂ§çÁöÑËØçËÆæÁΩÆÊàêÂíåËÉåÊôØ‰∏ÄÊ†∑ÁöÑÈ¢úËâ≤
                    Ê†πÊçÆ‰∏çÂêåÁöÑËÆøÈóÆËØ∑Ê±ÇÔºàwebÈááÈõÜÂô®ÂíåÁî®Êà∑ÔºâÔºå‰ΩúÂºä Web ÊúçÂä°Âô®‰ºöËøîÂõû‰∏çÂêåÁöÑÁΩëÈ°µÁªìÊûú

                    ::
                        set the color of the words the same as background 
                        use different ways to answer IR systems and Users 
                Êìç‰ΩúÁΩëÈ°µÂÜÖÂÆπÊù•ËææÂà∞Âú®Êüê‰∫õÂÖ≥ÈîÆËØçÁöÑÊêúÁ¥¢ÁªìÊûú‰∏≠ÊéíÂêçËæÉÈ´òÁöÑÁõÆÁöÑ
                ‰∏Ä‰∫õËÄÅÁªÉÁöÑ‰ΩúÂºäËÄÖËøò‰ºöÈááÁî®‰∏Ä‰∫õÊâãÊÆµÂíåÊäÄÂ∑ßÔºåÊØîÂ¶ÇÔºü 
                    Â∞ÜÈáçÂ§çÁöÑËØçËÆæÁΩÆÊàêÂíåËÉåÊôØ‰∏ÄÊ†∑ÁöÑÈ¢úËâ≤
                ÂæàÂ§öÁΩëÈ°µÂÜÖÂÆπÁöÑÂª∫ËÆæËÄÖÈÉΩÊúâÂïÜ‰∏öÂä®Êú∫ÔºåÂõ†Ê≠§‰ªñ‰ª¨Â∏åÊúõÈÄöËøáÊìç‰ΩúÊêúÁ¥¢ÂºïÊìéÁöÑÁªìÊûúÊù•Ëé∑Áõä
                Âú®ÂæàÂ§ö Web ÊêúÁ¥¢ÂºïÊìé‰∏≠ÔºåÊúâÂèØËÉΩÂèØ‰ª•ÈÄöËøá‰ªòË¥πÊù•Â∞ÜËá™Â∑±ÁöÑÁΩëÈ°µÊîæÂÖ•Âà∞ÊêúÁ¥¢ÂºïÊìéÁöÑÁ¥¢Âºï‰∏≠Ôºå
                    Ëøô‰∏™Ê®°ÂûãÁß∞‰∏∫‰ªòË¥πÊî∂ÂΩïÔºà paid inclusionÔºâ„ÄÇ
                ÂØπ‰∫éÊòØÂê¶ÂÖÅËÆ∏‰ªòË¥πÊî∂ÂΩï„ÄÅ‰ªòË¥πÊòØÂê¶‰ºöÂΩ±ÂìçÊêúÁ¥¢ÂºïÊìéÁöÑÊéíÂêçÁªìÊûúÔºå‰∏çÂêåÁöÑÊêúÁ¥¢ÂºïÊìé‰ºöÊúâ‰∏çÂêåÁöÑÊîøÁ≠ñ
                Ê†πÊçÆ‰∏çÂêåÁöÑËÆøÈóÆËØ∑Ê±ÇÔºàwebÈááÈõÜÂô®ÂíåÁî®Êà∑ÔºâÔºå‰ΩúÂºä Web ÊúçÂä°Âô®‰ºöËøîÂõû‰∏çÂêåÁöÑÁΩëÈ°µÁªìÊûú 
                Êõ¥Â§çÊùÇÁöÑ‰ΩúÂºäÊäÄÊúØËøòÂåÖÊã¨ÊìçÁ∫µ‰∏éÁΩëÈ°µÁõ∏ÂÖ≥ÁöÑÂÖÉÊï∞ÊçÆÂèäÊåáÂêëÁΩëÈ°µÁöÑÈìæÊé•Á≠â
                ‰ªñ‰ª¨‰πãÈó¥ÁöÑÊñó‰∫âÂ∞ÜÊ∞∏‰∏çÂÅúÊ≠¢
                Á†îÁ©∂È¢ÜÂüüÈáå‰πüÂá∫Áé∞‰∫Ü‰∏Ä‰∏™Ë¢´Áß∞‰∏∫ÂØπÊäóÂºè‰ø°ÊÅØÊ£ÄÁ¥¢Ôºà adversarial information retrievalÔºâÁöÑÂ≠êÈ¢ÜÂüü **
                ÊúÄÊó©Â§ßËßÑÊ®°‰ΩøÁî®ÈìæÊé•ÂàÜÊûêÊñπÊ≥ïÁöÑÊêúÁ¥¢ÂºïÊìé‰ªçÁÑ∂ÊòØ GoogleÔºöÈò≤Ê≠¢‰ΩúÂºä
            ÂπøÂëäÁªèÊµéÊ®°Âûã
                Web ÁöÑ‰∫§‰∫íÊÄß‰ΩøÂæó CPC‰ªòË¥πÊ®°ÂûãÊàê‰∏∫ÂèØËÉΩÔºåÁî®Êà∑ÁöÑÁÇπÂáªÂèØ‰ª•Ë¢´ÁΩëÁ´ôËÆ∞ÂΩïÂíåÁõëÊéßÔºåÁÑ∂ÂêéÊ†πÊçÆËÆ∞ÂΩïÁöÑÊÉÖÂÜµÂ∞±ÂèØ‰ª•ÁªôÂπøÂëäÂïÜÂØÑÂéªË¥¶Âçï„ÄÇ
                ÂØπ‰∫éÁî®Êà∑ÁöÑÊü•ËØ¢ÔºåÊêúÁ¥¢ÂºïÊìé‰ºöÂ∞Ü‚Äú Á∫Ø‚Äù ÊêúÁ¥¢ÁªìÊûúÔºàÈÄöÂ∏∏‰πüË¢´Áß∞
                    ‰∏∫Âü∫‰∫éÁÆóÊ≥ïÁöÑÊêúÁ¥¢ÁªìÊûúÔºâ‰Ωú‰∏∫‰∏ªË¶ÅÁªìÊûúËøîÂõûÁªôÁî®Êà∑ÔºåÂêåÊó∂ËµûÂä©ÊêúÁ¥¢ÁªìÊûúÂú®ÁÆóÊ≥ïÁªìÊûúÁöÑÂè≥‰æßÁã¨Á´ã
                    Âπ∂ÊúâÂå∫Âà´ÊÄßÂú∞ÊòæÁ§∫Âá∫Êù•„ÄÇ
                È£ûÊú∫ÂπøÂëäÁöÑÁº∫‰πè‰πüÂèçÊò†‰∫ÜÂá†‰πéÊ≤°Êúâ‰∫∫‰ºöÂú® Web ‰∏äÂá∫ÂîÆ A320 È£ûÊú∫Ëøô‰∏™‰∫ãÂÆû
                ‰∏ÄÈó®Ë¢´Áß∞‰∏∫ SEMÔºà Search Engine MarketingÔºåÊêúÁ¥¢ÂºïÊìéËê•ÈîÄ
                ÊØîÂ¶ÇÔºå‰∏Ä‰∏™ÂøÉÊúØ‰∏çÊ≠£ÁöÑÂπøÂëäÂïÜÂèØËÉΩ‰ºöËØïÂõæÈÄöËøáÈáçÂ§çÁÇπÂáªÔºà‰ΩøÁî®Êú∫Âô®ÁÇπÂáªÁîüÊàêÂô®ÔºâÂÖ∂Á´û‰∫âËÄÖÁöÑËµûÂä©ÊêúÁ¥¢ÂπøÂëäÊù•ËÄóÂ∞ΩÂÖ∂ÂπøÂëäÈ¢ÑÁÆó
                ‰ΩÜÊòØÁ†¥ÂùèÂΩ¢ÂºèÔºü & -
                    ÂÖ∂‰∏≠‰∏ÄÁßçË¢´Áß∞‰∏∫ÂûÉÂúæÁÇπÂáªÔºà click spamÔºâ  

            Áî®Êà∑‰ΩìÈ™å
                ‰∏ÄÁ≥ªÂàóÁöÑÁ†îÁ©∂ÁªìÊûúÈÉΩË°®ÊòéÔºå Web ÊêúÁ¥¢‰∏≠ÁöÑÂπ≥ÂùáÊü•ËØ¢ÂÖ≥ÈîÆËØç‰∏™Êï∞Â§ßÊ¶ÇÊòØ 2 Âà∞ 3 ‰∏™
                googleÁöÑÁî®Êà∑‰ΩìÈ™åÁâπÁÇπÔºö & -
                    ÂÖ≥Ê≥®Áõ∏ÂÖ≥ÊÄßÔºåÁâπÂà´ÊòØÊéíÂêçÈù†ÂâçÁöÑ‰∏Ä‰∫õÁªìÊûúÁöÑÊ≠£Á°ÆÁéáËÄå‰∏çÊòØÂè¨ÂõûÁéá
                    Áî®Êà∑‰ΩìÈ™åË¶ÅËΩªÈáèÁ∫ßÔºå‰πüÂ∞±ÊòØËØ¥Êü•ËØ¢È°µÈù¢ÂíåËøîÂõûÁªìÊûúÈ°µÈù¢Â∫îËØ•ÁÆÄÊ¥ÅÊï¥ÈΩêÔºå
                    Âπ∂‰∏îËøô‰∫õÈ°µÈù¢‰∏äÂü∫Êú¨Ê≤°ÊúâÂõæÂÉèÊàêÂàÜÔºåËÄåÂ∫îËØ•Âá†‰πéÂÖ®ÊòØÊñáÊú¨ÂÜÖÂÆπ

                    lightweight 
                    no image on the result page , it is clean and pretty 
                    no too many ads on the page 
                ÊôÆÈÄöÁöÑ Web ÊêúÁ¥¢Êü•ËØ¢‰ºº‰πéÂèØ‰ª•ÂàÜÊàêÂì™‰∏âÂ§ßÁ±ªÔºü  & -
                    (i) ‰ø°ÊÅØÁ±ªÔºà informationalÔºâÔºõ 
                        Áî®Êà∑ËæìÂÖ•‰ø°ÊÅØÁ±ªÊü•ËØ¢ÁöÑÁõÆÁöÑÂæÄÂæÄÊòØÊÉ≥‰ªéÂ§ö‰∏™‰∏çÂêåÁöÑÁΩëÈ°µ‰∏≠ÊäΩÂèñ‰ø°ÊÅØ
                    (ii) ÂØºËà™Á±ªÔºà navigationalÔºâÔºõ 
                        ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÁî®Êà∑ÊúÄÊúüÊúõÁöÑÁªìÊûúÂ∞±ÊòØÊ±âËééËà™Á©∫ÂÖ¨Âè∏ÁöÑ‰∏ªÈ°µÂá∫Áé∞Âú®ÊêúÁ¥¢ÁªìÊûúÁöÑÁ¨¨‰∏Ä‰∏™‰ΩçÁΩÆ„ÄÇ
                    (iii) ‰∫ãÂä°Á±ªÔºà transactionalÔºâ„ÄÇ
                        ‰∫ßÂìÅË¥≠‰π∞„ÄÅÊñá‰ª∂‰∏ãËΩΩÊàñËøõË°åÈ¢ÑËÆ¢Á≠â„ÄÇËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÊêúÁ¥¢ÂºïÊìéÂ∫îËØ•Âú®ÁªìÊûú‰∏≠Âàó‰∏æÂá∫
                        ‰∏Ä‰∫õÊúçÂä°ÔºåÂÆÉ‰ª¨ËÉΩÂ§üÊèê‰æõ‰∏äËø∞‰∫ãÂä°Â§ÑÁêÜÁöÑ‰∫§‰∫íÊé•Âè£
                    ÂàÜÁ±ªÂπ∂‰∏çÂÆπÊòìÔºåËÄåÁ±ªÂà´‰ø°ÊÅØ‰∏ç‰ªÖÂèØ‰ª•Áî®‰∫éÊéßÂà∂Âü∫‰∫éÁÆóÊ≥ïÁöÑÊêúÁ¥¢ÁªìÊûúÔºå‰πüÂèØ‰ª•Áî®‰∫éÊü•ËØ¢ÂíåËµûÂä©ÊêúÁ¥¢ÁªìÊûúÁöÑÂåπÈÖçÂΩì‰∏≠
                    ËôöÁ∫ø‰∏ãÈù¢ÁöÑÈÉ®ÂàÜÂ±û‰∫éÊêúÁ¥¢ÂºïÊìéÁöÑÂÜÖÈÉ®ÁªìÊûÑ„ÄÇ
            WebËßÑÊ®°‰º∞ËÆ°
                ‰º∞ËÆ°Âá∫Êüê‰∏™ÊêúÁ¥¢ÂºïÊìéÁöÑÁ¥¢ÂºïËßÑÊ®°Âç†Êï¥‰∏™WebÁöÑÊØî‰æã‰ªçÁÑ∂ÊòØÈùûÂ∏∏Âõ∞ÈöæÁöÑÔºåËøôÊòØÂõ†‰∏∫Web‰∏≠Â≠òÂú®Êó†Êï∞ÁöÑÂä®ÊÄÅÁΩëÈ°µ
                ÊØèÊ¨°ÊêúÁ¥¢ËøáÁ®ãÂΩì‰∏≠‰∏ç‰∏ÄÂÆö‰ºöË∞ÉÁî®ÊâÄÊúâÁöÑÁ¥¢Âºï
                ‰∏Ä‰∏™ÁΩëÁ´ôÁöÑÊ∑±Â±ÇÈ°µÈù¢ÂèØËÉΩ‰ºöË¢´Á¥¢ÂºïÔºåËÄåÂú®‰∏ÄËà¨ÁöÑ
                    Web ÊêúÁ¥¢‰∏≠Âπ∂‰∏ç‰ºöË¢´ËøîÂõû„ÄÇ‰ΩÜÊòØÔºåÂ¶ÇÊûúÁî®Êà∑ÈôêÂÆöÂú®ËØ•ÁΩëÁ´ôÂÜÖÊêúÁ¥¢ÔºàÂ§ßÂ§öÊï∞ÊêúÁ¥¢ÂºïÊìéÈÉΩÊèê‰æõ‰∫ÜÈôê
                    ÂÆöÂú®Êüê‰∏™ÁΩëÁ´ôËøõË°åÊêúÁ¥¢ÁöÑÂäüËÉΩÔºâÊó∂ÔºåÂàôËØ•È°µÈù¢‰ºöË¢´ËøîÂõû
                Â§öÁ±ªÈ°µÈù¢ÔºåÂõ†Ê≠§Êó†Ê≥ïÈááÁî®Âçï‰∏ÄÊåáÊ†áÊù•Ë°°ÈáèÁ¥¢ÂºïËßÑÊ®°
                ‰∏ÄÁßçÊñπÊ≥ï & - NEXT exp
                    Âà©Áî®ÂÖ≥‰∫éE1 ÂíåE2 ÊòØWeb‰∏≠Áã¨Á´ãÂùáÂåÄÈöèÊú∫ÊäΩÊ†∑Â≠êÈõÜÁöÑÂÅáËÆæËøõË°åÂºïÊìéËßÑÊ®°‰º∞ËÆ° 
                    ÂùáÂåÄÈöèÊú∫ÈÄâÂá∫‰∏Ä‰∏™ÁΩëÈ°µÁî®Êù•ËøõË°åÊµãËØïÊòØÈùûÂ∏∏Âõ∞ÈöæÁöÑ
                    ::
                        count (More or less public) private pages
                        use the approch called ‚Äúmark and recapture‚Äù
                            Ôºàused in ecology to estimate an animal population's sizeÔºâ
                        let f be the number of first crawl , s second and b both of them 
                        ...
            webÊäìÂèñÊ¶ÇËø∞
                ‰ªéWeb‰∏≠Êî∂ÈõÜÁΩëÈ°µÔºåÈááÈõÜÁöÑÁõÆÊ†áÊòØÂ∞ΩÂèØËÉΩÈ´òÊïàÂú∞ÈááÈõÜÊõ¥Â§öÊï∞ÁõÆÁöÑÊúâÁî®È°µÈù¢
                ÂêåÊó∂Ëé∑ÂæóËøûÊé•Ëøô‰∫õÈ°µÈù¢ÁöÑÈìæÊé•ÁªìÊûÑ
                WebÂ§çÊùÇÊÄßÁöÑÊ†πÊ∫êÂú®‰∫éÂÖ∂ÂàõÂª∫ÁöÑÊó†ÂçèË∞ÉÊÄß
                WebÈááÈõÜÂô®Ôºà web crawlerÔºâ
                ÂäüËÉΩ
                    È≤ÅÊ£íÊÄßÔºåÈááÈõÜÂô®ÂøÖÈ°ªË¶ÅËÉΩ‰ªéËøôÁ±ªÈô∑Èò±‰∏≠Ë∑≥Âá∫Êù•„ÄÇÂΩìÁÑ∂ÔºåËøô‰∫õÈô∑Èò±ÂÄí‰∏ç‰∏ÄÂÆöÈÉΩÊòØÊÅ∂ÊÑèÁöÑÔºåÊúâÊó∂ÂèØËÉΩÊòØÁΩëÁ´ôËÆæËÆ°ÁñèÂøΩÊâÄÂØºËá¥ÁöÑÁªìÊûú
                    ÊéßÂà∂ËÆøÈóÆÈ¢ëÁéáÔºöËÆæËÆ°ÈááÈõÜÂô®Êó∂ÂøÖÈ°ªË¶ÅÈÅµÂÆàËøô‰∫õ‰ª£Ë°®Á§ºË≤åÊÄßÁöÑËÆøÈóÆÁ≠ñÁï•
                    ÂàÜÂ∏ÉÂºèÔºöÈááÈõÜÂô®Â∫îËØ•ÂèØ‰ª•Âú®Â§öÊú∫‰∏äÂàÜÂ∏ÉÂºèËøêË°å
                    ÂèØÊâ©Â±ïÊÄßÔºöÂú®Â¢ûÂä†È¢ùÂ§ñÁöÑÊú∫Âô®ÂíåÂ∏¶ÂÆΩÁöÑÊÉÖÂÜµ‰∏ãÔºåÈááÈõÜÂô®ÁöÑÊû∂ÊûÑÂ∫îËØ•ÂÖÅËÆ∏ÂÆûÁé∞ÈááÈõÜÁéáÁöÑÊèêÈ´ò
                    Êñ∞È≤úÂ∫¶ÔºöÂú®ÂæàÂ§öÂ∫îÁî®‰∏≠ÔºåÈááÈõÜÂô®ÈÉΩÂ§Ñ‰∫éËøûÁª≠Â∑•‰ΩúÁä∂ÊÄÅÔºåÊäìÂèñÊñ∞ÁöÑÂçèËÆÆÁ≠â
                ÂÆûÁé∞
                    Êï¥‰∏™ÈááÈõÜËøáÁ®ãÂèØ‰ª•ÁúãÊàêÊòØ Web ÂõæÁöÑÈÅçÂéÜËøáÁ®ã
                    Âú®ÊäìÂèñÈ´òË¥®ÈáèÁΩëÈ°µÁöÑÂêåÊó∂ÔºåÈááÈõÜÂô®Ë¶ÅÊª°Ë∂≥ÂàÜÂ∏ÉÂºè„ÄÅËßÑÊ®°ÂèØÊâ©Â±ï„ÄÅÈ´òÊïà„ÄÅÁ§ºË≤åÊÄß„ÄÅÈ≤ÅÊ£íÊÄßÂèäÂäüËÉΩÂèØÊâ©Â±ïÁ≠â‰∏ÄÁ≥ªÂàóË¶ÅÊ±Ç„ÄÇ
                    Mercator ÈááÈõÜÂô®
                        ÈúÄË¶ÅÂú®ÊØèÁßí‰πãÂÜÖÊäìÂèñÂá†Áôæ‰∏™ÁΩëÈ°µ
                        ‰∫î‰∏™Ê®°Âùó & 
                            DNS Ëß£ÊûêÊ®°Âùó--Áî®‰∫éÂàÜÊûêipÂú∞ÂùÄ
                            *fatchÊ®°Âùó--Âà©Áî® http ÂçèËÆÆËøîÂõûÊüê‰∏™ URL ÂØπÂ∫îÁöÑÁΩëÈ°µ
                            URL Ê±†--Â≠òÊîæÁ≠âÂæÖÈááÈõÜÁöÑURL
                                ÂΩìÊüê‰∏™ URL Âä†ÂÖ•Âà∞ URL Ê±†Êó∂ÔºåÂÆÉ‰ºöË¢´ÂàÜÈÖç‰∏Ä‰∏™‰ºòÂÖàÁ∫ßÔºåÂü∫‰∫éËøô‰∏™‰ºòÂÖàÁ∫ßÊù•ÂÜ≥ÂÆöÂÖ∂ÊúÄÁªàÂá∫ÂàóÁúüÊ≠£ËøõË°åÈááÈõÜÁöÑÊó∂Êú∫
                            ÂàÜÊûêÔºà parseÔºâÊ®°ÂùóÔºö‰ªéÈááÈõÜÂà∞ÁöÑÁΩëÈ°µ‰∏≠ÊäΩÂèñÊñáÊú¨ÂèäÈìæÊé•
                            URL ÂéªÈáçÊ®°ÂùóÔºöÁ°ÆÂÆöÊüê‰∏™ÊäΩÂèñÂá∫ÁöÑÈìæÊé•ÊòØÂê¶Â∑≤Âú® URL Ê±†‰∏≠ÊàñËÄÖÊúÄËøëÊòØÂê¶Â∑≤ÊäìËøá
                        Á∫øÁ®ãÊâßË°å‰∏äËø∞ÁöÑÊµÅÁ®ãÂõæÔºåÂÖ∂ÂèØ‰ª•ËøêË°åÂú®Âçï‰∏™ËøõÁ®ã‰∏≠ÔºåÊàñËÄÖÂàÜÂºÄÂà∞ÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁöÑ‰∏çÂêåËäÇÁÇπÁöÑÂ§ö‰∏™ËøõÁ®ãÂΩì‰∏≠
                        Áª¥ÊåÅ‰∏Ä‰∏™robots.txtÊñá‰ª∂ÁöÑÁºìÂ≠ò ÂéüÂõ†Ôºü &
                            Âú®ÁúüÊ≠£ÊäìÂèñÁΩëÈ°µ‰πãÂâçËøõË°åÈááÈõÜÈôêÂà∂Ê£ÄÊµã‰∏çÁÆ°ÊòØËøôÂá†Â§©ËøòÊòØ‰πãÂêéÂá†Â§©ÂØπÂÖ∂ËøõË°åËÆøÈóÆ
                    ÂàÜÂ∏ÉÂºèÈááÈõÜ
                        Â¶Ç‰ΩïÂàíÂàÜÔºü 
                        1.Âú∞ÁêÜ‰ΩçÁΩÆÂàíÂàÜ--ÊØîÂ¶ÇÔºåÂú∞ÁêÜ‰ΩçÁΩÆÂú®Ê¨ßÊ¥≤ÁöÑÈááÈõÜÂô®‰∏ªË¶ÅÂÖ≥Ê≥®Ê¨ßÊ¥≤ÂüüÂêç‰∏ãÁΩëÁ´ôÁöÑÈááÈõÜ„ÄÇÂΩìÁÑ∂ÔºåËøôÁßçÂÅöÊ≥ïÂπ∂‰∏çÂÆåÂÖ®ÂèØÈù†Ôºå
                            ÂéüÂõ†ÂåÖÊã¨Ôºö Internet ‰∏äÊï∞ÊçÆÂåÖÁöÑ‰º†ËæìË∑ØÁ∫øÂπ∂‰∏ç‰∏ÄÂÆöÂèçÊò†Âú∞ÁêÜ‰ΩçÁΩÆÁöÑÈÇªËøëÊÄß„ÄÇ &
                            Âπ∂‰∏îÔºåÂú®‰ªª‰ΩïÊÉÖÂÜµ‰∏ãÔºå‰∏ªÊú∫ÂüüÂêçÂπ∂‰∏çÊÄªÊòØÂèçÊò†ÂÖ∂ÂÆûÈôÖÁöÑÁâ©ÁêÜ‰ΩçÁΩÆ
                        2.‰ΩøÁî®‰∏ªÊú∫ÂàíÂàÜÂô®Ôºà host splitterÔºâÂ∞ÜÈÄöËøáËøáÊª§Ê£ÄÊµãÁöÑ URL ÂàÜÈÖçÂà∞‰∏çÂêåÁöÑÈááÈõÜËäÇÁÇπ‰∏äÂéª &
                            ÈáçÂ§çÊ£ÄÊµãÊ®°Âùó‰ºöÂæàÂ§çÊùÇÔºö
                            ÂøÖÈ°ªË¶ÅÂü∫‰∫éÊåáÁ∫πÊàñ shingle ÈõÜÂêàÁöÑÊüê‰∫õÊÄßË¥®ÂØπÂÆÉ‰ª¨ËøõË°åËäÇÁÇπÂàíÂàÜ
                            ÊñáÊ°£ÂèòÂåñ--Â∞ÜÊñáÊ°£ÁöÑÊåáÁ∫πÊàñ shingle ËøûÂêå URL Êú¨Ë∫´‰∏ÄËµ∑ÊîæÂÖ• URL Ê±†‰∏≠
            web‰∏≠ÁöÑÈáçÂ§çÈóÆÈ¢ò
                ÁôæÂàÜ‰πã40ÁöÑÁΩëÈ°µÊòØÈáçÂ§çÁöÑÔºöÊØîÂ¶Ç‰ø°ÊÅØÂ∫ìÁöÑÂ§ö‰ªΩÈïúÂÉè
                solution1 :ËÆæËÆ°ÊåáÁ∫πÊØîÂ¶Ç64‰Ωç
                    ÈóÆÈ¢òÔºöÂèØËÉΩÂè™ÊòØÊúÄÂêé‰∏§‰∏™Â≠óÁ¨¶‰∏çÂêå
                solution 2:k shinglingÊäÄÊúØ &
                    Êê≠Âè†Ê£ÄÊµãÔºåÂíångramÁ±ª‰ººÔºåËßÅp362
                    shingling ÁÆóÊ≥ïÔºöÂ¶ÇÊûúËÆ°ÁÆó‰∏§‰∏™ÁΩëÈ°µÁõ∏‰∫íÁ±ª‰ººÔºåÂ∞±ÂèØ‰ª•ÂéªÊéâÂÖ∂‰∏≠‰∏ÄÁØáÔºå‰∏çËøõË°åÁ¥¢Âºï
                    ÁÆóÊ≥ïÊúâ‰∏çÂêåÁâàÊú¨ÔºåËøΩÊ±ÇÊïàÁéáÁöÑÊúÄÂ§ßÂåñÔºåÂõ†‰∏∫ÊñáÁ´†Êï∞ÁõÆÂ§™Â§ö‰∫Ü
                    Â¶Ç‰ΩïÁº©ÂáèËÆ°ÁÆóÔºü
            ---------------------------------------------------------------------------------------------------------
            IR on the web vs. IR in general 
                ‰∏âÁÇπ‰∏çÂêå & 
            Big Picture
            Size of the WEB
                what should be counted &
                    Âõõ‰∏™‰∏úË•ø
                    Count them if they can be accessed by a large number of people
                How to find all Web pages? &
                    1.simply crawl the complete Web and count its number of pages
                        This doesn‚Äôt work due to the Web‚Äôs enormous size 
                    2.better approach : mark and recapture 
                        ÁîüÊÄÅ‰∏≠ÁßçÁæ§‰º∞ÁÆó
                        use two crawl,first and second 
                    In 2005, the Web has been estimated to contain at least 11.5 billion pages
                    now around 60 billion pages
            Web IR
                Differences from traditional IR &
                a bow tie 
                    wie oben 
                Êõ¥Â§öÁöÑ‰ø°ÊÅØÈúÄË¶Å 
                    wie oben 
                    Navigational and transactional
                    Difficult problem: How can the search engine tell what the user need or intent for a particular query is?
                    ÊåâÁÖßÁî®Êà∑ÊÑèÂõæ‰øÆÊîπÊêúÁ¥¢ÁªìÊûú
                        ÂèçÂØπÔºöAvoid the filter bubbleÔºö ÊñáÂåñÊÄùÊÉ≥Ê≥°Ê≥°
                ÁΩëÁªúÊêúÁ¥¢ÁöÑÂÖ∑‰ΩìÂΩ¢ÂºèÁâπÁÇπ & 
                    Use short queries (average < 3)
                        Don‚Äôt want to spend a lot of time on composing a query
                    Only look at the first couple of results
                ËØÑ‰ª∑
                    Classic IR relevance
                    Trust, duplicate elimination, readability, loads fast, no popups
                    On the web, precision is more important than recall
                ÁΩëÁªúÊñáÊ°£ÁâπÁÇπ
                    ‰∏ªË¶ÅÁöÑÂõõ‰∏™ÁâπÊÄß & -
                        1.ÂêÑÁßçÊ†ºÂºèÁöÑÊñáÊ°£ÈÉΩÊúâ
                            Most (truly) dynamic content : is ignored by web spiders -- 
                                it‚Äôs too much to index it all
                            Unstructured (text, html), 
                            semi-structured (html, xml),
                            structured/relational (databases)
                        2.Â§öËØ≠Ë®ÄÊÄß 
                            Documents in a large number of languages
                            Queries in a large number of languages
                            Don‚Äôt return English results for a Japanese query
                            query/document languages are Frequent mismatches

                            ÁâπÊÆäÁöÑÊü•ËØ¢ÔºöCross-language information retrieval (CLIRÔºâ Ë∑®ËØ≠Ë®ÄÊü•ËØ¢
                        3.ÈáçÂ§çÊÄß 
                            see before
                            Today‚Äôs search engines eliminate duplicates very effectively
                        4.ÂèØ‰ø°Â∫¶
                            Hoaxes abound È™óÂ±Ä
            Crawling
                ‰∏§‰∏™ÁâπÁÇπÔºü & 
                    should be distributed, scalable, efficient, polite, robust

                    H√∂flichkeit--
                        um eine √úberlastung des Web-- Servers zu vermeiden
                    Robustheit--
                        gegen Ung√ºltige/unvollst√§ndige HTMLDokumente
                        Netzwerkprobleme (z.B. hohe Latenzzeit / niedrige Bandbreite des Servers)
                ppt‰∏≠‰æãÂ≠ê & - ÁêÜËß£ÔºåÁªôÂõæÁîªÂá∫Êù•Ë∑ØÂæÑ
                    È°µÈù¢ÁöÑÂÆΩÂ∫¶ÊúâÈôêÊêúÁ¥¢ 
                ÊúâÂì™‰∫õÊåëÊàòÔºüÔºö & -
                    Âõõ‰∏™
                        SkalierbarkeitÂèØÊâ©Â±ïÊÄß
                            Mehr Rechner erh√∂hen Leistungsf√§higkeit
                        Aktualit√§tÂÆûÊó∂ÊÄß
                        Verteiltes System
                        Qualit√§tsbewusstsein
            Ads
                ÂêéÊù•ÔºöÊêúÁ¥¢‰∏éÂπøÂëä‰∏•Ê†ºÂàÜÂºÄÔºöStrict separation of search results and search ads
                How are ads ranked  & - exp
                    rank based on bid price AND relevance(two things)
                    Other ranking factors: 
                        location, time of day, loading speed of landing page
                    ÂèÇÊï∞Ôºö bid CTR ad rank rank paid p47
                Google‚Äôs second price auction 
                    Ê¨°È´ò‰ª∑ÊãçÂçñ & -Ëß£Èáä and calcu„ÄÇ
                        ËÆ°ÁÆó 
                        p47
                ÈóÆÈ¢òÔºö & -
                    ‰∏ªË¶ÅÁöÑ‰∏§‰∏™ÈóÆÈ¢ò 
                    Keyword arbitrage
                        we can buy a keyword on google 
                        Â∞ÜÊµÅÈáèÈáçÂÆöÂêëÂà∞ÊîØ‰ªòÊØîË∞∑Ê≠åÂ§öÂæóÂ§öÁöÑÁ¨¨‰∏âÊñπ
                    Violation of trademarks
                        The search term ‚Äúgeico‚Äù on Google was bought by competitors‰æµÊùÉ
            Duplication Detection
                //
                    
                            solution1 :ËÆæËÆ°ÊåáÁ∫πÊØîÂ¶Ç64‰Ωç
                            solution 2:k shinglingÊäÄÊúØ 
                25-40% of the web is duplicate (mirrors - e.g. Wikipedia, unix man pages, SPAM sites)
                we should use Near-duplicates
                ÂéüÂõ†Ôºö & -
                    Áî®Êà∑‰ΩìÈ™å 
                    Ëá™Ë∫´ÊÄßËÉΩConserve resources: 
                        reduced index size - less memory, faster computations, etc.
                True semantic similarity (similarity in content) is too difficult to compute
                Âá†ÁßçÂü∫Êú¨ÁöÑÂÅöÊ≥ï & -
                    Hashing
                    ÁºñËæëË∑ùÁ¶ª 
                    shinglingÁÆóÊ≥ïÔºåÈÄöËøáÂàÜÂâ≤ËΩ¨Êç¢ÊàêÈõÜÂêàÔºåÁÑ∂ÂêéËÆ°ÁÆóJaccard coefficient
                        Áî®‰∫éËÆ°ÁÆó‰∏§‰∏™ÊñáÊ°£ÁöÑÁõ∏‰ººÂ∫¶Ôºå‰æãÂ¶ÇÔºåÁî®‰∫éÁΩëÈ°µÂéªÈáç
                        -r 
                            Summarize each document in a short sketchÔºåand then Estimate the similarity based on the sketches
                            look at the percentage of min-hashes that agree **
                            with different hash functions(doc and hash table )
                            
                            use MinHash
                                ÂéüÂõ†Ôºü & - NEXT 
                                calcu &
                                    ‰ΩøÁî®‰∏âÂÖÉÁªÑÔºü ÔºüÔºü NEXT
                                    Store the minimum hash: {143}
                                    Sim(A,B)=Áõ∏Âêåhash‰∏™Êï∞/ÊÄªÁöÑ‰∏™Êï∞
                                    ÂÖ∂ÂÆû‰πüÊòØÂú®ËÆ°ÁÆóJaccard coefficient
                            a sketch for a document is a collection of minimum hashes
                            use 84 hash functions
                            Summarized each page in 672 bytes (84*8 byte values)
                        Super Shingles
                            -r 
                                Doing all pairwise comparisons still too expensive
                                package the Shingles into super Shingles : doc - s-Shingle table  


                                ÂÜçÊ¨°ÂìàÂ∏å **
                                    Group sketch into non overlapping super-shingles 
                                    Hash each super-shingle {1011, 6543, ..., 7327}
                                    ‰æãÂ≠ê & - p65
                                        Declare doc 2 and doc 3 to be 2-similar
                                In practice - store the above table sorted by different columns
                                    Only compare against neighboring rows
                                    Store the super shingle table sorted by columns
                        ÂØπ‰∫é‰∏ãÈù¢ÁöÑÂ¶Ç‰ΩïÂéªÈáç? & - exp
                                some Open Problems 
                                Flash, Ajax, and other not easily indexable content
                                Obtaining text from raw HTML is not as easy as it sounds
            SPAM detection
                ÂûÉÂúæÈìæÊé•ÂàÜÊûê
                You have a page that will generate lots of
                    revenue for you if people visit it
                It damages the search engine‚Äôs reputation
                Google no longer gives good rankings to
                    pages employing this technique
                Â∏∏ËßÅÁöÑSpamÊäÄÊúØ &x2
                    plus oben+ 
                        Misleading meta-tags, excessive repetition
                        Doorway page
                    Landing Page 
                    Serve fake content to search engine spider
                    Link spam 
                        Put these links on pages with high (or at least non-zero) PageRank
                ÂèçSpam
                It can also be a legitimate business ‚Äì which is called SEO
                Google bombs **
                The war against spam
                and there is one thing called ::Webmaster Guidelines & -
                lack of central access control! & -
                Major search engines have guidelines for webmasters
                Ignore these guidelines at your own risk
                Once a search engine identifies you as a spammer, 
                    all pages on your site may get low ranks (or
                    disappear from the index entirely) **
                There is often a fine line between spam and legitimate SEO **
        ppt-14-PageRank und HITS Á¨¨ 21 Á´† ÈìæÊé•ÂàÜÊûê --over --c 
            ÈìæÊé•ÁªìÊûÑ‰ø°ÊÅØÂú® Web ÊêúÁ¥¢ÁªìÊûúÊéíÂ∫è‰∏≠ÁöÑ‰ΩøÁî®--ÈìæÊé•ÂàÜÊûê
            ÈìæÊé•ÂàÜÊûêÁªìÊûúÂ∑≤ÁªèÊàê‰∏∫ Web ÊêúÁ¥¢ÂºïÊìéÂú®ËÆ°ÁÆóÊüê‰∏™ÁΩëÈ°µÁöÑÁªÑÂêàÂæóÂàÜ‰∏≠ÁöÑ‰∏Ä‰∏™Âõ†Â≠ê
            Web ÊêúÁ¥¢‰∏≠ÈìæÊé•ÂàÜÊûêÊÄùÊÉ≥ÁöÑÊúÄÊó©Ëµ∑Ê∫ê‰∫éÂºïÊñáÂàÜÊûêÈ¢ÜÂüüÔºåÂêéËÄÖÂú®ÂæàÂ§öÊñπÈù¢‰∏é‰∏Ä‰∏™Ë¢´Áß∞‰∏∫ÊñáÁåÆ
                ËÆ°ÈáèÂ≠¶Ôºà bibliometricsÔºâÁöÑÈ¢ÜÂüüÊúâ‰∫§Âèâ„ÄÇËøô‰∫õÂ≠¶ÁßëËØïÂõæÈÄöËøáÂàÜÊûêÊñáÁåÆ‰πãÈó¥ÁöÑÂºïÁî®Ê®°ÂºèÊù•ÈáèÂåñÂ≠¶ÊúØ
                ËÆ∫ÊñáÁöÑÂΩ±ÂìçÂäõ„ÄÇ
            ÈìæÊé•ÂàÜÊûêÁöÑÊÑè‰πâ & -
                Web ‰∏äÁöÑÈìæÊé•ÂàÜÊûêÊñπÊ≥ï‰πüÊääË∂ÖÈìæÊé•ÁúãÊàêÊòØ‰∏Ä‰∏™ÁΩëÈ°µÂØπÂè¶‰∏Ä‰∏™ÁΩëÈ°µÁöÑÊùÉÂ®ÅÂ∫¶ÁöÑËÆ§ÂèØ
                find pages with high authority
            ‰ªÖ‰ªÖÁÆÄÂçïÂú∞ÈÄöËøáÂÖ•ÈìæÊé•ÁöÑÊï∞ÁõÆÊù•Ë°°ÈáèÁΩëÈ°µÁöÑË¥®ÈáèÊòØ‰∏çÂ§üÈ≤ÅÊ£íÁöÑ„ÄÇ
                ÂûÉÂúæÈìæÊé•
            Â¶Ç‰ΩïÈÄâÊã©‰∏ã‰∏Ä‰∏™ÈááÈõÜÁΩëÈ°µÔºü
            ÈìæÊé•ÂàÜÊûêÁöÑÁ†îÁ©∂‰∏ªË¶ÅÂü∫‰∫é‰∏§‰∏™Âü∫Êú¨Áõ¥Ëßâ
                ÂéªÊéâËøô‰∫õ‚Äú ÂÜÖÈÉ®‚Äù ÁöÑÈìæÊé•
            Âõ†Ê≠§Ôºå Web ÁΩëÈ°µÊú¨Ë∫´Êê∫Â∏¶ÁöÑËØçÈ°πÂíåÁî®Êà∑Áî®‰∫éÊèèËø∞Âêå‰∏ÄÁΩëÈ°µÁöÑËØçÈ°π‰πãÈó¥ÂæÄÂæÄÂ≠òÂú®ÁùÄ‰∏ÄÂÆöÁöÑÂ∑ÆÂºÇ
            ‰∏æ‰æã & -
                ÂæàÂ§öÊåáÂêëwww.ibm.comÁöÑÈìæÊé•‰∏äÁöÑÈîöÊñáÊú¨ÈÉΩÂåÖÂê´ÂçïËØçcomputerÔºåËøô‰∏™‰∫ãÂÆûÂ∞±ÂèØ‰ª•‰∏∫WebÊêúÁ¥¢ÂºïÊìéÊâÄ‰ΩøÁî® 
            ÊØîÂ¶ÇÔºåÈîöÊñáÊú¨‰∏≠ÁöÑËØçÈ°πÂ∞±ÂèØ‰ª•‰Ωú‰∏∫Á¥¢ÂºïÁõÆÊ†áÁΩëÈ°µÁöÑËØçÈ°π„ÄÇÂõ†Ê≠§ÔºåËØçÈ°πcomputerÁöÑÂÄíÊéíËÆ∞ÂΩïË°®‰∏≠Â∞±‰ºöÂåÖÂê´ÊñáÊ°£www.ibm.com
            ÂêåÈ°µÂÜÖËØçÈ°π‰∏ÄÊ†∑ÔºåÈÄöÂ∏∏‰πü‰ºöÂü∫‰∫éËØçÈ¢ëÊù•ËÆ°ÁÆóÈîöÊñáÊú¨ËØçÈ°πÁöÑÊùÉÈáç **
            ÈÇ£‰∫õÂú®Â§ö‰∏™ÈîöÊñáÊú¨‰∏≠È´òÈ¢ëÂá∫Áé∞ÁöÑËØçÈ°πÔºàÂ¶ÇWebÈîöÊñáÊú¨‰∏≠ÊúÄÊôÆÈÅçÁöÑËØçÈ°πÊòØClickÂíåhereÔºâ‰ºöÂèóÂà∞ÊÉ©ÁΩö
            Êüê‰∏™ÁΩëÁ´ôÂèØ‰ª•ÈÄöËøáÊûÑÈÄ†ÂÖ∑ÊúâËØØÂØºÊÄßÁöÑÈîöÊñáÊú¨Êù•ÊåáÂêëËá™Â∑±Ôºå‰ªéËÄåÊèêÈ´òÂú®Êüê‰∫õÊü•ËØ¢ËØçÈ°π‰∏äÁöÑÊéíÂêç
            Âú®webÊ£ÄÁ¥¢Â∑•ÂÖ∑ÊûÑÂª∫ËøáÁ®ã‰∏≠ÔºåÈúÄË¶ÅÂæàÂ§ö‰ΩúÂºäÊ£ÄÊµãÂ∑•‰Ωú
            ÂæàÂ§öÂ≠¶ËÄÖËøòÂØπÊúâÊïàÁ™óÂè£ÁöÑÂ§ßÂ∞èËøõË°å‰∫ÜÁ†îÁ©∂--‰ΩøÁî®Â§öÂ∞ëÈîöÊñáÊú¨ÂêàÈÄÇÔºü 
            webÊêúÁ¥¢ÂºïÊìé‰∏çÂêå‰∫é‰º†ÁªüÊñáÊ°£ÈõÜÔºü 
            ÈùôÊÄÅPageRank
                formua & 
                    transport probability alpha .
                    the prob. of transporting from i to j  is:: alpha /n + ..  
                Âú®ÈöèÊú∫Ê∏∏Ëµ∞ËøáÁ®ã‰∏≠ËÆøÈóÆË∂äÈ¢ëÁπÅÁöÑÁΩëÈ°µ‰πüË∂äÈáçË¶Å
                ÂØπ WebÂõæ‰∏≠ÁöÑÊØè‰∏™ËäÇÁÇπËµã‰∏Ä‰∏™ 0 Âà∞ 1 ‰πãÈó¥ÁöÑÂàÜÂÄºÔºåËøô‰∏™ÂàÜÂÄºË¢´Áß∞‰∏∫ PageRank
                    ÁªôÂÆöÊü•ËØ¢ÔºåwebÂºïÊìé‰ºöÁªºÂêàÂêÑÁßçÊåáÊ†áÔºö
                ‰ΩôÂº¶Áõ∏‰ººÂ∫¶ÔºàÂèÇËÄÉ 6.3 ËäÇÔºâ„ÄÅËØçÈ°πÈÇªËøëÂ∫¶Ôºà 7.2.2 ËäÇÔºâÂèä PageRank Á≠â
                ÂÅáËÆæÂÜ≤Êµ™ËÄÖ‰ª•1/NÊ¶ÇÁéáË∑≥ÂêëÂÖ∂‰ªñ‰ªªÊÑè‰∏Ä‰∏™ÁΩëÈ°µ„ÄÇÂΩìÁÑ∂ÔºåÂÜ≤Êµ™ËÄÖ‰πü‰ª• 1/NÁöÑÊ¶ÇÁéáË∑≥Âà∞ÂÖ∂ÂΩìÂâç‰ΩçÁΩÆ
                ËøáÁ®ãÂÅáËÆæÔºö & -
                    1.Â¶ÇÊûúÊ≤°ÊúâÂá∫ÈìæÔºåÈöèÊú∫Ê∏∏Ëµ∞
                    2.Â¶ÇÊûúÊúâÂá∫Èìæalpha:ÈöèÊú∫Ë∑≥ËΩ¨Ôºõ1-alpha:ÁªßÁª≠ËøõË°åÈöèÊú∫Ê∏∏Ëµ∞
                ‰ªéËÄåÔºåÈÄöËøáÈ©¨Â∞îÁßëÂ§´ÈìæËßÑÂàôÔºåÂæóÁü•‰ªñÂ∞±‰ºö‰ª•‰∏Ä‰∏™Âõ∫ÂÆöÁöÑÊó∂Èó¥ÊØî‰æã œÄ(v)ËÆøÈóÆÊØè‰∏™ËäÇÁÇπ v &
                +È©¨Â∞îÁßëÂ§´ÈìæÔºö &
                    È©¨Â∞îÁßëÂ§´ÈìæÊòØ‰∏Ä‰∏™Á¶ªÊï£Êó∂Èó¥ÈöèÊú∫ËøáÁ®ã
                    PijË¢´Áß∞‰∏∫ËΩ¨ÁßªÊ¶ÇÁéáÔºåÂÆÉ‰ªÖ‰ªÖ‰æùËµñ‰∫éÂΩìÂâçÁöÑÁä∂ÊÄÅ iÔºåËøôÁßçÊÄßË¥®Ë¢´Áß∞‰∏∫È©¨Â∞îÁßëÂ§´ÊÄß
                    È©¨Â∞îÁßëÂ§´Èìæ‰∏≠Ôºå‰∏ã‰∏Ä‰∏™Áä∂ÊÄÅÁöÑÂàÜÂ∏É‰ªÖ‰ªÖ‰æùËµñ‰∫éÂΩìÂâçÁöÑÁä∂ÊÄÅÔºåËÄåÂíåÂ¶Ç‰ΩïÂà∞ËææÂΩìÂâçÁä∂ÊÄÅÊó†ÂÖ≥„ÄÇ
                    Ê†πÊçÆPÁîüÊàêÁöÑÈöèÊú∫Áü©ÈòµÔºåÊúÄÂ§ßÁâπÂæÅÂÄºÊòØ 1
                    Âõæ‰∏≠ÈìæÊé•‰∏äÁöÑÊï∞Â≠ó‰ª£Ë°®ÁöÑÊòØËΩ¨ÁßªÊ¶ÇÁéá
                    Êú¨Ë¥®ÊòØ‰∏ÄÁßçÂêëÂêéËø≠‰ª£ËßÑÂàô & -
                ÂõûÂà∞ÁΩëÈ°µÈóÆÈ¢òÔºöÂÖ∂‰∏≠È©¨Â∞îÁßëÂ§´Èìæ‰∏≠ÁöÑÊØè‰∏™Áä∂ÊÄÅÂØπÂ∫î‰∏Ä‰∏™ÁΩëÈ°µÔºå
                    ËÄåÊØè‰∏™ËΩ¨ÁßªÊ¶ÇÁéá‰ª£Ë°®‰ªé‰∏Ä‰∏™ÁΩëÈ°µË∑≥ËΩ¨Âà∞Âè¶Â§ñ‰∏Ä‰∏™ÁΩëÈ°µÁöÑÊ¶ÇÁéá
                Ê†πÊçÆ‰∏äÈù¢ÁöÑËßÑÂàôÔºåÁî±N√óNÁöÑ‰∏¥Ë°óÁü©ÈòµAÊé®ÂØºÂá∫È©¨Â∞îÁßëÂ§´ÈìæÁöÑËΩ¨ÁßªÊ¶ÇÁéáÁü©ÈòµP
                ËøôÊ†∑Êàë‰ª¨Âè™ÈúÄ Áä∂ÊÄÅÂàÜÂ∏ÉÂíåËΩ¨ÁßªÊ¶ÇÁéáÁü©Èòµ PÔºåÂ∞±ËÉΩËÆ°ÁÆóÂÜ≤Êµ™ËÄÖÂú®‰ªª‰∏ÄÊó∂ÂàªÊâÄÂ§ÑÁä∂ÊÄÅÁöÑÊ¶ÇÁéáÂàÜÂ∏É
                Êàë‰ª¨Â∞Ü PageRank ËÆæÁΩÆ‰∏∫ÊØè‰∏™ËäÇÁÇπ v Âú®Á®≥ÊÄÅ‰∏ãÁöÑËÆøÈóÆÈ¢ëÁéá
                ËÆ°ÁÆóPageRankÊùÉÈáç & - p322
                    ergodic Markov chain
                    it will converge to a steady state


                    Á¨¨‰∏Ä‰∏™Êù°‰ª∂‰øùËØÅ‰ªé‰ªªÊÑè‰∏§‰∏™Áä∂ÊÄÅ‰πãÈó¥ÈÉΩÂ≠òÂú®ÈùûÈõ∂Ê¶ÇÁéáËΩ¨ÁßªÂ∫èÂàóÔºåËÄåÁ¨¨‰∫å‰∏™Êù°‰ª∂‰øùËØÅ‰∏çÂ≠òÂú®ËøôÊ†∑ÁöÑÁä∂ÊÄÅÂàíÂàÜÔºö
                        ÊâÄÊúâÁöÑÁä∂ÊÄÅËΩ¨ÁßªÂè™ÂèëÁîüÂú®‰∏§‰∏™ÂàíÂàÜÂêéÁöÑÁä∂ÊÄÅÂ≠êÈõÜ‰πãÈó¥Âπ∂Âæ™ÁéØÂæÄÂ§ç„ÄÇ
                    Â∏¶ÈöèÊú∫Ë∑≥ËΩ¨Êìç‰ΩúÁöÑÊúÄÂêéÂèØ‰ª•ÂæóÂà∞ÂîØ‰∏ÄÁöÑÁ®≥ÊÄÅÊ¶ÇÁéáÂàÜÂ∏É„ÄÇÊüê‰∏™Áä∂ÊÄÅÁöÑÁ®≥ÊÄÅÊ¶ÇÁéáÂ∞±ÊòØÁõ∏Â∫îÁΩëÈ°µÁöÑ PageRank
                    ÂèçÂ§çËø≠‰ª£‰∏ÄÂÆöÊ¨°Êï∞‰πãÂêéÔºåÊàë‰ª¨‰ºöÁúãÂà∞ÂàÜÂ∏ÉÊî∂Êïõ‰∫é‰∏Ä‰∏™Á®≥ÂÆöÁä∂ÊÄÅ ** ‰æãÂ≠êp322 & -
                    Áä∂ÊÄÅÂÖ∑ÊúâÂØπÁß∞ÊÄßÁöÑÊó∂ÂÄôÂèØ‰ª•Áõ¥Êé•ËÆ°ÁÆóÁ®≥ÊÄÅÊ¶ÇÁéáÂàÜÂ∏É **
                    ÁΩëÈ°µÁöÑ PageRank ‰∏éÁî®Êà∑ËæìÂÖ•ÁöÑÊü•ËØ¢Êó†ÂÖ≥--ÈùôÊÄÅË¥®ÈáèË°°ÈáèÊåáÊ†á
                    ÊâÄ‰ª•Âè™ÊòØËøô‰∏™ÊåáÊ†á‰Ωú‰∏∫ÊéíÂêçÂõ†Â≠ê‰πã‰∏Ä
                ÁâπÁÇπ
                ÁΩëÈ°µÁöÑ PageRank ‰∏éÁî®Êà∑ËæìÂÖ•ÁöÑÊü•ËØ¢Êó†ÂÖ≥
            Èù¢Âêë‰∏ªÈ¢òÁöÑ PageRank
                -r 
                    use different PageRanks to calcu . when deal with different subjects .

                ÈùûÁ≠âÊ¶ÇÁéáË∑≥Âà∞‰∏Ä‰∏™ÈöèÊú∫ÁΩëÈ°µ
                ‰∏Ä‰∏™‰ΩìËÇ≤Ëø∑ÂèØËÉΩÂ∏åÊúõÊúâÂÖ≥‰ΩìËÇ≤‰∏ªÈ¢òÁöÑÁΩëÈ°µÁöÑÊéíÂêçË¶ÅÈ´ò‰∫éÈùû‰ΩìËÇ≤‰∏ªÈ¢òÁöÑÁΩëÈ°µ
                Â¶ÇÊûú‰∏çÊòØ‰ΩìËÇ≤Á±ªÁöÑÊñáÁ´†Ôºå‰ª§ÂÖ∂PageRank‰∏∫ 0 &
                    ÂÆûÈôÖ‰∏äÔºå‰ΩìËÇ≤Áõ∏ÂÖ≥ÁöÑÈõÜÂêàSÊú™ÂøÖÊúâÁ®≥ÊÄÅ
                    ÊâÄ‰ª•Êâæ‰∏Ä‰∏™ÊØîSÁ®çÂæÆÂ§ß‰∏ÄÁÇπÁöÑÈõÜÂêàYÔºåÊúâÁ®≥ÊÄÅ
                    È©¨Â∞îÁßëÂ§´ÈìæÁ®≥ÊÄÅÁöÑÊó∂ÂÄôÊØè‰∏™ÁΩëÈ°µÂÖ∑ÊúâÈùûÈõ∂ÂÄº
                    Y ‰∏≠ÁöÑÊØè‰∏™ÁΩëÈ°µÈÉΩÊúâÈùûÈõ∂ PageRank ÂÄº
                    Âú®Y‰πãÂ§ñÁöÑÊñáÁ´†ÂàÜÊï∞ÂΩíÈõ∂
                ÂêÑ‰∏™‰∏çÁî®ÁöÑ‰∏ªÈ¢òÔºöÊâìÂàÜÊó∂ÂÄôË∞ÉÁî®‰∏çÂêåÁöÑPageRankËøõË°åËÆ°ÁÆóÔºö &
                    ÊØîÂ¶ÇÁßëÂ≠¶ÂÆóÊïôÁ≠â
                    ÂΩìÊêúÁ¥¢Áî®Êà∑‰ªÖ‰ªÖÂØπÊüê‰∏™‰∏ªÈ¢òÊÑüÂÖ¥Ë∂£Êó∂ÔºåÈÇ£‰πàÂú®ÂØπÊ£ÄÁ¥¢ÁªìÊûúÊâìÂàÜÂíåÊéíÂêçËøáÁ®ãÂè™È°ªË∞ÉÁî®Áõ∏Â∫î‰∏ªÈ¢òÁöÑPageRankÂêëÈáèÂÄºËøõË°åËÆ°ÁÆó
                    Áî®Êà∑ÊúâÂèØËÉΩÊòæÂºèÂú∞Ê≥®ÂÜå‰∫ÜÂÖ∂ÂÖ¥Ë∂£ÔºåÊàñËÄÖÁ≥ªÁªüËÉΩÂ§ü‰ªéÊØè‰∏™Áî®Êà∑ÁöÑÂéÜÂè≤Ë°å‰∏∫‰∏≠Â≠¶Âà∞ÂÖ∂ÂÖ¥Ë∂£
                Â§ÑÁêÜÊ∑∑ÂêàÁöÑÊÉÖÂÜµÔºü &
                    -r 
                        each user can have an individual PageRank on different topics 
                        we mix them with linear functions 
                    ÂÆûÈôÖ‰∏ä‰ªª‰∏ÄÁî®Êà∑ÁöÑ‰∏™ÊÄßÂåñPageRank ÈÉΩÂèØ‰ª•Ë¢´Ë°®Á§∫ÊàêÂ§ö‰∏™Èù¢Âêë‰∏ªÈ¢òÁöÑ PageRank ÁöÑÁ∫øÊÄßÁªÑÂêà
                    0.6*pi1 + 0.4*pi2
                    ÂàÜÂà´ÊòØÈù¢Âêë‰ΩìËÇ≤Âíå ÁöÑ ÊîøÊ≤ª‰∏ªÈ¢ò PageRank ÂêëÈáè
                    Â¶ÇÊûúÈöèÊú∫Ë∑≥ËΩ¨Êìç‰ΩúÁöÑÊ¶ÇÁéáÊòØ 10%ÁöÑËØùÔºåÈÇ£‰πàÂÖ∂‰∏≠Êúâ 6%ÊòØÂà∞‰ΩìËÇ≤Á±ªÔºåËÄå 4%Âà∞ÊîøÊ≤ªÁ±ªÁΩëÈ°µ
            HITsÂü∫‰∫éÊü•ËØ¢ÁöÑhubÊåáÊï∞ÂíåÊùÉÂ®ÅÊåáÊï∞--ÈíàÂØπ‰ø°ÊÅØÁ±ªÊ£ÄÁ¥¢
                ‰∏§‰∏™ÊéíÂ∫èÁªìÊûúÂàóË°®ÔºåÂÖ∂‰∏≠‰∏Ä‰∏™Âü∫‰∫éhubÂÄºÔºåËÄåÂè¶‰∏Ä‰∏™Âü∫‰∫éauthorityÂÄº
                ‰πãÈó¥ÊûÑÊàêËâØÊÄßÂæ™ÁéØÔºåÂÖ±ÂêåÁªÑÊàê‰∏Ä‰∏™Web Â≠êÈõÜ
                    ÊûÑÈÄ†Ôºö
                        Â∞ÜÊ†πÈõÜÂèäÊåáÂêëÊ†πÈõÜ‰∏≠ÁöÑÁΩëÈ°µÂíåÊ†πÈõÜÊâÄÊåáÂêëÁöÑÁΩëÈ°µÂä†ÂÖ•Âà∞Âü∫Êú¨ÈõÜÔºà base setÔºâ‰∏≠
                        Âà©Áî®Âü∫Êú¨ÈõÜËøõË°åËÆ°ÁÆó
                        ÂéüÂõ†Êúâ‰∏â &
                        ÂèØ‰ª•Ëé∑ÂæóË∑®ËØ≠Ë®ÄÁöÑÊïàÊûúÔºöÊØîÂ¶ÇÂ¶ÇÊûúÊúâ‰∏Ä‰∏™Ëã±ÊñáÁöÑ hub ÁΩëÈ°µÊåáÂêë‰∏Ä‰∏™Êó•ÊñáÊèèËø∞ÁöÑÊó•Êú¨Â∞èÂ≠¶ÁöÑ‰∏ªÈ°µÔºåÊúÄÂêéËøîÂõûÁöÑÁªìÊûú‰ºöÊúâÁ±ª‰ºº‰π±Á†ÅÁöÑÊïàÂ∫î
                hub ÂÄºÊåáÁöÑÊòØËØ•ÁΩëÈ°µÁöÑÂØºËà™ËÉΩÂäõÔºå‰πüÂèØ‰ª•Áß∞‰∏∫ÂØºËà™Â∫¶„ÄÇËÄå authority ÂÄºÊåáÁöÑÊòØÁΩëÈ°µÁöÑÊùÉÂ®ÅÂ∫¶„ÄÇ
                Ëø≠‰ª£ÁÆóÊ≥ïÁöÑÊ†∏ÂøÉÁéØËäÇÂ∞±ÊòØ hub ÂÄºÂíå authority ÂÄºÁöÑÂèåÈáçÊõ¥Êñ∞ËøáÁ®ã
                ËÆ°ÁÆóÊØè‰∏™ÁΩëÈ°µÁöÑÊï∞ÂÄºÔºü &
                Âæ™ÁéØËø≠‰ª£ÂÖ¨Âºè & -
                ÈÇªÊé•Áü©ÈòµË°®Á§∫ÁöÑÂæ™ÁéØÂÖ¨Âºè &
                ‰∏äÈù¢ÈÇ£‰∏™ÂºèÂ≠êÂ∞±ÂèòÊàêÁü©Èòµ AATÁöÑÁâπÂæÅÊñπÁ®ãÔºåËÄå‰∏ãÈù¢ÈÇ£‰∏™ÂºèÂ≠êÂàô‰ºöÂèòÊàêÁü©Èòµ ATA ÁöÑÁâπÂæÅÊñπÁ®ã
                ÈÇ£‰πà h Âíå a ÊúÄÂêé‰ºöÊî∂Êïõ‰∫éÊüê‰∏™ÂîØ‰∏ÄÁöÑÁ®≥ÊÄÅÂêëÈáèÔºåÊúÄÂêéÁöÑÁ®≥ÂÆöÁä∂ÊÄÅÂèñÂÜ≥‰∫éÂõæÁªìÊûÑ & **
                ËÆ°ÁÆó‰ºòÂåñÔºü & - 
                    Ê†∏ÂøÉÂ∞±ÊòØÂèòÊàê‰∫ÜÊ±Ç‰∏ªÁâπÂæÅÂêëÈáè
                HITSÔºà Hyperlink-Induced Topic SearchÔºåË∂ÖÈìæÂØºÂêëÁöÑ‰∏ªÈ¢òÊêúÁ¥¢Ôºâ
            ÊÄªÁªìÔºö
                ÈìæÊé•ÂàÜÊûêÁöÑÁõÆÁöÑÔºöÂàÜÊûêÈìæÊé•ÊùÉÂ®ÅÊÄßÔºåÁªôÁΩëÈ°µÊâìÂàÜ
                ‰ªªÊÑèËÆ°ÁÆóÁâπÂæÅÂêëÈáèÁöÑÁÆóÊ≥ïÈÉΩÂèØ‰ª•Áî®‰∫éËÆ°ÁÆó hub ÂÄºÂíå authority ÂÄºÂêëÈáè
                Êàë‰ª¨Âπ∂‰∏çÈúÄË¶ÅËÆ°ÁÆóËøô‰∫õÂÄºÁöÑÁ≤æÁ°ÆÂÄºÔºåËÄåÂè™ÈúÄË¶ÅÁü•ÈÅìËøô‰∫õÂÄºÁöÑÁõ∏ÂØπÂ§ßÂ∞è‰ª•‰æøËÉΩÂ§üËøõË°åÊéíÂ∫èÂç≥ÂèØ
                ÂÆûÈôÖ‰∏äÂÖ¨ÂºèÔºà 21-8ÔºâÂè™ÈúÄË¶ÅÂ§ßÊ¶Ç 5 Ê¨°Ëø≠‰ª£Â∞±ÂèØ‰ª•‰∫ßÁîüÁõ∏ÂΩìÂ•ΩÁöÑÁªìÊûú
                Ëø≠‰ª£Êõ¥Êñ∞ËÄå‰∏çÊòØÁõ¥Êé•ËÆ°ÁÆóÔºåÂõ†‰∏∫Sparse
            -------------------------------------------------------------------------------------------
            Webgraph
            LinkankerÈìæÊé•ÊèèËø∞ÔºåmaoÊñáÊú¨
                Ankertexte
            Eigenschaften des WWW & -
                Nicht zusammenh√§ngend ÔÉ† unerreichbare Knoten
            ÈìæÊé•ÂàÜÊûêÊñπÊ≥ïÔºö PageRank Âíå HITS
            PageRank
                Statisches Qualit√§tsma√ü
                Why count links ? & -
                    Gute Inhalte werden h√§ufiger verlinkt *** 
                    good content offen means more links 
                1.Random Surfer
                    ‰æãÂ≠ê p14 & -
                    ÊúâÂì™‰∫õÂõ∞ÈöæÔºü: & - 
                        Sackgassen
                        Anderes Problem: Kreisl√§ufe
                2.Modellierung &
                    using stochastisch Matrix
                    calcu & -
                        ÊúÄÂêéÁªôÂá∫RankingÁªìÊûú
                    p23ÂÖ¨Âºè & -
                    bestimmten Zustand zu sein
                    Berechnung Eigenvektor
                pagerank ÁâπÁÇπ & -
                    Von Anfrage unabh√§ngig
                    Offline Berechnung
                    ÔºàÂÖ∂ÂÆûÊòØÊØèÂë®ËÆ°ÁÆó‰∏ÄÊ¨°Ôºâ
            HITS
                ÁêÜËß£ & -
                    ‰πüÊòØ‰∏ÄÁßçÈÄêÊ≠•ËøõÂÖ•Á®≥ÊÄÅÁöÑÁÆóÊ≥ïÔºå‰ΩÜÊòØÂè™ÊòØÂú®Âü∫Êú¨ÈõÜÂêà‰∏≠ËøõË°å
                    Â±ÄÈÉ®ÁÆóÊ≥ïÔºåÈíàÂØπIBMËøôÊ†∑ÂÖ®ÊòØÂõæÁâáÁöÑÁΩëÈ°µ
                idea & -
                    Authorities -Haben gute Inhalte
                    Hubs -Verweisen auf gute Inhalte
                Formalisierung & -
                    Darstellung √ºber Adjazenzmatrix
                    Multiplikation Matrix mit Vektor‰∏ÄÁßçÊï∞Â≠¶Êìç‰Ωú **
                    Iteratives Verfahren
                    Verfahren muss nicht konvergieren‰∏çÊòØ‰∏ÄÂÆöÊî∂Êïõ
                    Potenzmethode (vollst√§ndig) &&&&ËøôÁßçÊñπÊ≥ïÔºå‰πüÂ∞±ÊòØÈóÆÈ¢òËΩ¨Êç¢ÁöÑÁ¨¨‰∫å‰∏™ÂÖ≥ÈîÆÊ≠•È™§ÔºöÊ±Ç‰∏ªÁâπÂæÅÂêëÈáè
                    ËÆ°ÁÆó &
                ÂÖ∑‰ΩìÂ∫îÁî®ËøáÁ®ã & -
                    1.ÁªôÂÆöÊüê‰∏™Êü•ËØ¢ÔºàÊØîÂ¶Ç leukemiaÔºâÔºå
                        Âà©Áî®Êüê‰∏™ÊñáÊú¨Á¥¢ÂºïËé∑ÂæóÂåÖÂê´ leukemia ÁöÑÊâÄÊúâÁΩëÈ°µ„ÄÇËøô‰∫õÁΩëÈ°µË¢´Áß∞‰∏∫Ê†πÈõÜÔºà root setÔºâ
                    2.Êàë‰ª¨Âà©Áî®‰∏äËø∞ËøáÁ®ã‰∫ßÁîüÁöÑÂü∫Êú¨ÈõÜËøõË°å hub ÂÄºÂíå authority ÂÄºÁöÑËÆ°ÁÆó
                        ‰πãÊâÄ‰ª•Â¶ÇÊ≠§ÊûÑÈÄ†Âü∫Êú¨ÈõÜÁöÑÂéüÂõ†Âú®‰∫é

                    HITS nicht auf komplettem Web berechnen, sondern auf Suchergebnissen
                    ÔºöÂè™ÊòØÂú®ÊêúÁ¥¢ÁªìÊûú‰∏≠ËøõË°åÊêúÁ¥¢Ôºå‰∏çÊòØÂÖ®ÁΩë
                    Suchterme bestimmen eine Ergebnismenge von Dokumenten ÔÉ† root set (=Startmenge)
                    ÔºöÊÄéÊ†∑ËøêÁî®‰∫éÁΩëÁªúÊêúÁ¥¢ÊéíÂêç
                    √úberwindung von Sprachen
                hits ÁâπÁÇπ & -
                    -r 
                        goes only in the result of queries not the whole web 
                        different languages can be involved 

                    Âè™ÊòØÂú®ÊêúÁ¥¢ÁªìÊûú‰∏≠ËøõË°åÊêúÁ¥¢Ôºå‰∏çÊòØÂÖ®ÁΩë
                    √úberwindung von Sprachen
                ÈóÆÈ¢òÔºö  & -
                    -r 
                        not relevant website can be involved 

                    ÂèØËÉΩÂºïËµ∑‰∏ªÈ¢òÁ•®ÂèòÔºåÂõ†‰∏∫Aufnahme nicht relevanter Seiten 
        -----ÊñáÊú¨ÂàÜÁ±ª --‰ºº‰πé‰∏ç‰ªÖ‰ªÖÊòØÊèê‰æõÊêúÁ¥¢ÊúçÂä°ÔºåÊõ¥Êúâ‰∫∫Â∑•Êô∫ËÉΩÂú®ÈáåÈù¢Ôºà‰∏éÊó∂‰ø±ËøõÊ®°ÂùóÔºâ
            +
                ‰æãÂ¶ÇÊñáÁ´†Ëá™Âä®ÂàÜÁ±ª„ÄÅÈÇÆ‰ª∂Ëá™Âä®ÂàÜÁ±ª„ÄÅÂûÉÂúæÈÇÆ‰ª∂ËØÜÂà´„ÄÅÁî®Êà∑ÊÉÖÊÑüÂàÜÁ±ªÁ≠âÁ≠â
                Ê†πÊçÆ‰∏Ä‰∫õÂÖ¨Âè∏ÊèèËø∞È¢ÑÊµãÂÖ¨Âè∏ÊÄßË¥®ÔºåÁªôÂÖ∂ÂàÜÁ±ª
                Êï∞ÊçÆÈõÜÊòØ11Á±ªÂÖ¨Âè∏ÁöÑÊèèËø∞Êï∞ÊçÆÔºåÊàë‰ª¨Ë¶ÅÊ†πÊçÆ4774Êù°ËÆ≠ÁªÉÊï∞ÊçÆÂéªÈ¢ÑÊµã2381Êù°Êï∞ÊçÆÁöÑÁ±ªÂà´Ê†áÁ≠æ
                Á±ªÂà´‰∏çÂπ≥Ë°°ÈóÆÈ¢ò
                ‰ΩøÁî®CHIÈÄâÊã©ÁâπÂæÅÔºåTFIDFËÆ°ÁÆóÁâπÂæÅÊùÉÈáçÔºåÊú¥Á¥†Ë¥ùÂè∂ÊñØÂàÜÁ±ªÁöÑÊï¥‰ΩìÊµÅÁ®ã
                    https://blog.csdn.net/liuchonge/article/details/52204218
                Â§ßËá¥Ê≠•È™§Ôºö
                    ‰ΩøÁî®ÁªìÂ∑¥‰∏≠ÊñáÂàÜËØçÂ∑•ÂÖ∑ÂØπÊñáÊú¨ËøõË°åÂ§ÑÁêÜÔºå    
                        Âπ∂ÂéªÂÅúÁî®ËØçÂæóÂà∞ÊâÄÊúâÊñáÊú¨‰∏≠Âá∫Áé∞ÁöÑËØçËØ≠„ÄÇ
                    ‰ΩøÁî®CHI‰Ωú‰∏∫ÁâπÂæÅÈÄâÊã©ÁöÑ‰æùÊçÆÁªôÊØè‰∏ÄÁ±ªÊñ∞ÈóªÈÄâÂá∫150Áª¥ÁöÑÁâπÂæÅ
                        ÔºåÂπ∂ÂéªÈáç„ÄÇËøôÊ†∑Êàë‰ª¨Â∞±ÂèØ‰ª•Ëé∑ÂæóÂ§ßÊ¶Ç1000Áª¥ÁöÑÁâπÂæÅ„ÄÇ
                    ÁâπÂæÅ-->ÊØè‰∏™Êñ∞ÈóªÊûÑÈÄ†VSMÊ®°ÂûãÔºà‰∏Ä‰∏™ÁâπÂæÅÁü©ÈòµÔºåÊØè‰∏ÄË°åÈÉΩÊòØÁâπÂæÅÂêëÈáè ***Ôºâ
                        ÁâπÂæÅÊùÉÈáçÁöÑÂ§öÈáçÁÆóÊ≥ïÔºöÁúãÂõæÔºå‰∏âÁßç
                            1.Áî®ÊòØÂê¶Âá∫Áé∞Ë°®Ëææ
                                ÊúÄÁÆÄÂçï
                            2.Áî®tf
                            3.‰ΩøÁî®TFIDFÊñπÊ≥ïËÆ°ÁÆóÂêÑÁâπÂæÅÁöÑÊùÉÈáçÂæóÂà∞Ë°®Á§∫ËØ•ÊñáÊú¨ÁöÑÁâπÂæÅÂêëÈáè
                    Áé∞Âú®Â∞±ÂèØ‰ª•Êñπ‰æø‰ΩøÁî®KNN/SVMÁ≠âÊñπÊ≥ïÂàÜÁ±ªÁöÑÊï∞ÊçÆ
                ÊñáÊú¨ÁöÑÁõ∏‰ººÂ∫¶ËÆ°ÁÆó‰ºöÊúâËøõ‰∏ÄÊ≠•ÁöÑÂ∫îÁî®ÔºåÊØîÂ¶ÇÊñáÊú¨ÁöÑÂàÜÁ±ª„ÄÅËÅöÁ±ªÁ≠â ***
        ppt-15 ‰∏Ä‰∫õÂü∫Á°Ä  --over 
            Klassifikation als Aufgabe im IR
                Herleiten von ùõæÊù•Ê∫ê & 
                    ÔÇß Manuell
                    ÔÇß Erlernen aus Beispielen
                Beispiele Textklassifikation
                    Âú®ÂûÉÂúæÈÇÆ‰ª∂Â§ÑÁêÜ‰∏≠ÔºåÊúâÂì™‰∫õÂü∫Êú¨ÁöÑÂ§ÑÁêÜÊñπÊ≥ï & p7 
                        maybe we will let you deside if it is spam 
                        check the keyword or the metadata 
                    Ressortzuteilung von Nachrichten
                    Erkennung von Sprache / Zeichensatz
                    Sentimentanalyse
                    Who wrote which Federalist papers?
                    Male or female author?  
                        50 features to distinguish male-authored texts
                        from female-authored texts
                    Positive or negative movie review?
                    What is the subject of this article??
                the formal expression & 
                    Einfach Zuweisung
                    Mehrfache Zuweisung
                        Mehrfachklassifikation bzw. √úberschneidende Kategorien
                questions about Maschinelles Lernen &
                    1.how function this method ?use pic to explain
                    2.about overfitting problem (speak more than one min)
                Ergebnisse Eva.
                    richtig oder nicht ? & 
                        explain it about 0.5 min .
                        Korrekte Klassifikationen
                            ÔÇß d aus c1 als c1 klassifiziert
                            ÔÇß d aus c2 als c2 klassifiziert
                        Falsche Klassifikation
                            ÔÇß d aus c1 als c2 klassifiziert
                            ÔÇß d aus c2 als c1 klassifiziert
                    1.using Confusion Matrix & 
                        rem. pic.
                        Verwendung von Evaluationsma√üen wie Recall, Precision und F1 m√∂glich
                    2.Confusion Matrix erweitern p27 pic & 
                        Recall und Precision nicht als Gesamtma√ü anwendbar
                        calcu . with 
                            Accuracy 
                                korrekten Ergebnisse √ºber allen vorgenommenen Klassifikationen 
                                sum of diag numbers / sum of all numbers  **
                                calcu Beispiel p28 &
                            Error Rate 
                                calcu 
                            Kritik an Accuracy &
                                if one class has more articals ,  
                                and thus the total acc will be strongly infact by it 
                    3.cross validation 
                        exp & 
                            1.randomly part the dataset to k categraies 
                            2.choose one cat. for evaluation and the others for traning .
                            3.repeat it for n times with diff. choise .
                                n-times k-fold cross-validation ÔÉ† 
                                    typischer Wert: 10-times 10-fold cross-validation
            Êï∞ÊçÆË∞ÉÊï¥ÔºåÁâπÂæÅÊäΩÂèñÔºàÂàÜÁ±ªÁöÑÁ¨¨‰∏ÄÊ≠•Ôºâ    
                Klassenungleichgewicht
                    three methods & 
                        Undersampling
                            ÔÇß Ausgleich unterschiedlicher H√§ufigkeiten 
                                zwischen den Klassen durch Eliminierung von
                                Beobachtungen der gr√∂√üeren Klasse L
                        Oversampling
                            ÔÇß Vervielf√§ltigung von Beobachtungen der kleineren Klasse S
                        Hybrid-Verfahren
                            ÔÇß Mischform zwischen Under- und Oversampling

                    Undersampling
                        https://zhuanlan.zhihu.com/p/34782497
                        1.use Undersampling-Rate,ÈöèÊú∫Ê¨†ÈááÊ†∑
                            Imbalance-Rate & rario between big one and small one 
                            ÂáèÂ∞ëÂ§öÊï∞Á±ªÊ†∑Êú¨Êï∞ÈáèÊúÄÁÆÄÂçïÁöÑÊñπÊ≥ï‰æøÊòØÈöèÊú∫ÂâîÈô§Â§öÊï∞Á±ªÊ†∑Êú¨
                        2.NearMissÊñπÊ≥ï
                            ÊòØÂà©Áî®Ë∑ùÁ¶ªËøúËøëÂâîÈô§Â§öÊï∞Á±ªÊ†∑Êú¨ÁöÑ‰∏ÄÁ±ªÊñπÊ≥ïÔºåÂÆûÈôÖÊìç‰Ωú‰∏≠‰πüÊòØÂÄüÂä©kNN
                            ÂÖ∑‰Ωì &
                                NearMiss-1ÔºöÂú®Â§öÊï∞Á±ªÊ†∑Êú¨‰∏≠ÈÄâÊã©‰∏éÊúÄËøëÁöÑ3‰∏™Â∞ëÊï∞Á±ªÊ†∑Êú¨ÁöÑÂπ≥ÂùáË∑ùÁ¶ªÊúÄÂ∞èÁöÑÊ†∑Êú¨„ÄÇ
                                NearMiss-2ÔºöÂú®Â§öÊï∞Á±ªÊ†∑Êú¨‰∏≠ÈÄâÊã©‰∏éÊúÄËøúÁöÑ3‰∏™Â∞ëÊï∞Á±ªÊ†∑Êú¨ÁöÑÂπ≥ÂùáË∑ùÁ¶ªÊúÄÂ∞èÁöÑÊ†∑Êú¨„ÄÇ
                                NearMiss-3ÔºöÂØπ‰∫éÊØè‰∏™Â∞ëÊï∞Á±ªÊ†∑Êú¨ÔºåÈÄâÊã©Á¶ªÂÆÉÊúÄËøëÁöÑÁªôÂÆöÊï∞ÈáèÁöÑÂ§öÊï∞Á±ªÊ†∑Êú¨„ÄÇ
                            ÊØîËæÉ &
                                NearMiss-1ÂíåNearMiss-2ÊñπÊ≥ïÁöÑÊèèËø∞‰ªÖÊúâ‰∏ÄÂ≠ó‰πãÂ∑ÆÔºå‰ΩÜÂÖ∂Âê´‰πâÊòØÂÆåÂÖ®‰∏çÂêåÁöÑÔºö
                                    NearMiss-1ËÄÉËôëÁöÑÊòØ‰∏éÊúÄËøëÁöÑ3‰∏™Â∞ëÊï∞Á±ªÊ†∑Êú¨ÁöÑÂπ≥ÂùáË∑ùÁ¶ªÔºåÊòØÂ±ÄÈÉ®ÁöÑÔºõ
                                    NearMiss-2ËÄÉËôëÁöÑÊòØ‰∏éÊúÄËøúÁöÑ3‰∏™Â∞ëÊï∞Á±ªÊ†∑Êú¨ÁöÑÂπ≥ÂùáË∑ùÁ¶ªÔºåÊòØÂÖ®Â±ÄÁöÑ„ÄÇ
                                ËÆ∫Êñá‰∏≠ÊúâÂØπËøôÂá†ÁßçÊñπÊ≥ïÁöÑÊØîËæÉÔºåÂæóÂà∞ÁöÑÁªìËÆ∫ÊòØNearMiss-2ÁöÑÊïàÊûúÊúÄÂ•Ω
                                    Ôºå‰∏çËøáËøô‰πüÊòØÈúÄË¶ÅÁªºÂêàËÄÉËôëÊï∞ÊçÆÈõÜÂíåÈááÊ†∑ÊØî‰æãÁöÑ‰∏çÂêåÈÄ†ÊàêÁöÑÂΩ±Âìç
                        3.Condensed Nearest Neighbors (CNN)
                            ‰∏ÄÁßçÂü∫‰∫éËÅöÁ±ªÁöÑÊñπÊ≥ï *
                            NEXT
                            Durch Zufallskomponente kann die erzeugte Untermenge stark variieren
                        4.Underbagging
                            Ê¨†ÈááÊ†∑Ë¢´Êï¥ÂêàÂà∞AdaBoost.M1‰∏≠„ÄÇRUSBoostË¢´ËØÅÊòéÊòØÊØîAdaC2Ôºå
                                SMOTEBoostÔºåMSMOTEBoostÔºåUnderBaggingÔºå
                                EasyEnsembleÔºåBalanceCascadeÊõ¥Â•ΩÔºåÊõ¥Âø´Ôºå
                                Êõ¥ÁÆÄÂçïÁöÑÊõø‰ª£ÊñπÊ°àÔºåÂõ†Ê≠§Êàê‰∏∫‰ªéÂÅèÊñúËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÁöÑÂèØË°åÈÄâÊã©„ÄÇ 
                            NEXT
                            Mit steigender Anzahl der Modelle umso wahrscheinlicher, dass jede Beobachtung in zumindest
                            eines der Modelle einflie√üt und somit nicht komplett verworfen wird
                    Oversampling
                        Vergr√∂√üerter Trainingsdatensatz
                        1.use ratio to enlarge 
                        2.SMOTE-Verfahren (Synthetic Minority Oversampling Technique)
                            NEXT
                                ÊôÆÈÄöÁöÑËøáÈááÊ†∑‰ºö‰ΩøÂæóËÆ≠ÁªÉÈõÜ‰∏≠ÊúâÂæàÂ§öÈáçÂ§çÁöÑÊ†∑Êú¨„ÄÇ 
                                ‰∫ßÁîüoverfittingÈóÆÈ¢ò
                                SMOTEÁöÑÂÖ®Áß∞ÊòØSynthetic Minority Over-Sampling TechniqueÔºåËØë‰∏∫‚Äú‰∫∫Â∑•Â∞ëÊï∞Á±ªËøáÈááÊ†∑Ê≥ï‚Äù„ÄÇ
                                ÈÄöËøáfeature space
                                ‰ΩøÁî®Á∫øÊÄßÂèòÊç¢Ôºü
                            Distanzma√ü: euklidische Distanz bzw. Gower-Distanz f√ºr kategorische Attribute
                        ++can be more 
                Feature-Scaling
                    +
                        why two values should be likely :Âü∫Êú¨ÊÄùÊÉ≥ & -
                            when x1 :[0-2000]
                                x2: [1-5]
                            feature map ‰ºöÂèòÊàêÊ§≠ÂúÜÔºåÂØπÂêéÁª≠Â§ÑÁêÜ‰∏çÂ•ΩÔºöÔºöÊ±ÇÊï∞ÂÄºÁöÑÊó∂ÂÄô‰ºöÊù•ÂõûË∑≥Âä® 
                        ‰∏ÄËà¨Êàë‰ª¨ÈÄâÊã©Â∞ÜfeatureÁöÑvalueËêΩÂú® [‚àí1,1], 
                            ‰ΩÜÊòØÊàë‰ª¨‰∏çÈúÄË¶Å‰∏•ËãõÁöÑË¶ÅÊ±Ç‰∏ÄÂÆöÊª°Ë∂≥[‚àí1,1]ÔºåÂÖ∂ÂÆûÂ∑¶ËæπÁïåÂú®[‚àí3,‚àí13]ÔºåÂè≥ËæπÁïåÂú®[13,3] Â∞±ÂèØ‰ª•‰∫Ü
                    methods & 
                        1.Standard Scaler
                            Ê†áÂáÜÂåñ
                            Â§ÑÁêÜ‰ª•ÂêéÊòØN(0,1)
                        2.MinMax Scaler
                            Skaliert auf den Bereich 0 bis 1 bzw. -1 bis 1 falls negative Werte vorhanden sind
                            Sensitiv gegen√ºber Ausrei√üern-ÂØπÂºÇÂ∏∏Êï∞ÊçÆÊïèÊÑüÔºàÊØîÂ¶ÇÊúâ‰∏Ä‰∏™ÂæàÂ§ßÔºâ
                        3.Robust Scaler
                            wird der Interquartilsabstand ÂõõÂàÜ‰ΩçÊï∞(engl. interquartile range) verwendet
                            Dadurch robuster gegen√ºbern Ausrei√üern
                            calcu ?? NEXT
                Feature-Auswahl
                    -r
                        based on frequncy
                        MI formular & calcu. & p65
                        with the help of Chi-square test calcu. & p69
                    ‰∏ªË¶ÅÁõÆÁöÑ &
                        Text collections have a large number of features
                        it can Eliminates noise features and avoid overfitting
                        ÁâπÂæÅ‰∏™Êï∞Ë∂äÂ§öÔºåÂÆπÊòìÂºïËµ∑‚ÄúÁª¥Â∫¶ÁÅæÈöæ‚ÄùÔºåÊ®°Âûã‰πü‰ºöË∂äÂ§çÊùÇÔºåÂÖ∂Êé®ÂπøËÉΩÂäõ‰ºö‰∏ãÈôç„ÄÇ
                        ÁâπÂæÅÈÄâÊã©ËÉΩÂâîÈô§‰∏çÁõ∏ÂÖ≥ÔºàirrelevantÔºâÊàñ‰∫¢‰ΩôÔºàredundantÔºâÁöÑÁâπÂæÅÔºå
                            ‰ªéËÄåËææÂà∞ÂáèÂ∞ëÁâπÂæÅ‰∏™Êï∞ÔºåÊèêÈ´òÊ®°ÂûãÁ≤æÁ°ÆÂ∫¶ÔºåÂáèÂ∞ëËøêË°åÊó∂Èó¥ÁöÑÁõÆÁöÑ
                    Utility measures A(t,c), three different approaches &
                        -r
                            not all features are suitable for classification purpose
                            only this subset as features in text classification

                        ÔÇß Frequency: ùê¥ ùë°, ùëê = ùëÅ(ùë°, ùëê)
                            spec. in IR : Selecting terms that are most common in the class(document )
                            but: May have no specific information (such as, Monday, Tuesday ‚Ä¶)
                        ÔÇß Mutual information: ùê¥ ùë°, ùëê = ùêº(ùëàùë°; ùê∂ùëê)
                            ËøôÁßçÁî±‰∫éËÆ≠ÁªÉÈõÜÁöÑÂÅ∂ÁÑ∂ÊÄßÂØºÂá∫ÁöÑ‰∏çÊ≠£Á°ÆÁöÑÊ≥õÂåñÁªìÊûúÁß∞‰∏∫ËøáÂ≠¶‰π†ÔºàoverfittingÔºâ„ÄÇËß£ÂÜ≥Ëøô‰∏™
                            ËØçÈ°πÁöÑÂ≠òÂú®‰∏éÂê¶ÁªôÁ±ªÂà´cÁöÑÊ≠£Á°ÆÂà§Êñ≠ÊâÄÂ∏¶Êù•ÁöÑ‰ø°ÊÅØÈáè
                            ÈÄâÊã©ÊúÄÂ§ßÁöÑk‰∏™È°πÁõÆ
                            ÈÄâÂá∫ÁöÑËØçÊ±áÂØπÁ±ªÂà´ÁöÑÂà§ÂÆöËµ∑Âà∞ËâØÂ•Ω‰ΩúÁî®
                            formular &
                            calcu. & p65
                        ÔÇß The ùúí2 test: ùê¥ ùë°, ùëê = ùúí2(ùë°, ùëê)
                            Âú®ÁªüËÆ°Â≠¶‰∏≠Ôºåœá 2ÁªüËÆ°ÈáèÂ∏∏Â∏∏Áî®‰∫éÊ£ÄÊµã‰∏§‰∏™‰∫ã‰ª∂ÁöÑÁã¨Á´ãÊÄß
                            Â¶ÇÊûú‰ªñ‰ª¨ÊòØÁõ∏ÂÖ≥ÁöÑÂàôËØ¥Êòé‰ø°ÊÅØÊòØÊúâÁî®ÁöÑ
                            formular &
                            calcu. & p69
                        +ÂêéÈ™åÊ¶ÇÁéá
                                https://blog.csdn.net/u011508640/article/details/72815981
                                Â∞§ÂÖ∂ÊòØÂΩìÂêéÈ™åÂàÜÂ∏ÉÊ≤°Êúâ‰∏Ä‰∏™ÁÆÄÂçïÁöÑËß£ÊûêÂΩ¢ÂºèÁöÑÊó∂ÂÄôÊõ¥ÊòØËøôÊ†∑Ôºö
                                    Âú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÂêéÈ™åÂàÜÂ∏ÉÂèØ‰ª•‰ΩøÁî® Markov chain Monte Carlo ÊäÄÊúØÊù•Ê®°ÊãüÔºå
                                    ‰ΩÜÊòØÊâæÂà∞ÂÆÉÁöÑÊ®°ÁöÑ‰ºòÂåñÊòØÂæàÂõ∞ÈöæÊàñËÄÖÊòØ‰∏çÂèØËÉΩÁöÑ
                                ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºàMaximum likelihood estimation, ÁÆÄÁß∞MLEÔºâÂíå
                                    ÊúÄÂ§ßÂêéÈ™åÊ¶ÇÁéá‰º∞ËÆ°ÔºàMaximum a posteriori estimation, ÁÆÄÁß∞MAPÔºâ
                                    ÊòØÂæàÂ∏∏Áî®ÁöÑ‰∏§ÁßçÂèÇÊï∞‰º∞ËÆ°ÊñπÊ≥ï
                                ‰Ω†ÊúâÂ§öÂ§ßÊääÊè°ËÉΩÁõ∏‰ø°‰∏Ä‰ª∂ËØÅÊçÆÔºüÔºàhow much you can trust the evidenceÔºâ
                                Ë¥ùÂè∂ÊñØÂÖ¨ÂºèÔºöÂÅöÂà§Êñ≠ÁöÑÊó∂ÂÄôÔºåË¶ÅËÄÉËôëÊâÄÊúâÁöÑÂõ†Á¥†„ÄÇ 
                                    ËÄÅÊùøÈ™Ç‰Ω†Ôºå‰∏ç‰∏ÄÂÆöÊòØ‰Ω†Êää‰ªÄ‰πàÂ∑•‰ΩúÊêûÁ†∏‰∫ÜÔºåÂèØËÉΩÂè™ÊòØ‰ªñ‰ªäÂ§©Âá∫Èó®ÂâçÂíåÂ§™Â§™Âêµ‰∫Ü‰∏ÄÊû∂„ÄÇ
                                    ‰∏Ä‰∏™Êú¨Êù•Â∞±Èöæ‰ª•ÂèëÁîüÁöÑ‰∫ãÊÉÖÔºåÂ∞±ÁÆóÂá∫Áé∞Êüê‰∏™ËØÅÊçÆÂíå‰ªñÂº∫ÁÉàÁõ∏ÂÖ≥Ôºå
                                        ‰πüË¶ÅË∞®ÊÖé„ÄÇËØÅÊçÆÂæàÂèØËÉΩÊù•Ëá™Âà´ÁöÑËôΩÁÑ∂‰∏çÊòØÂæàÁõ∏ÂÖ≥Ôºå‰ΩÜÂèëÁîüÊ¶ÇÁéáËæÉÈ´òÁöÑ‰∫ãÊÉÖ
                                +MAP 
                                    ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÊòØÊ±ÇÂèÇÊï∞Œ∏, ‰Ωø‰ººÁÑ∂ÂáΩÊï∞P(x0|Œ∏)ÊúÄÂ§ß„ÄÇÊúÄÂ§ßÂêéÈ™åÊ¶ÇÁéá‰º∞ËÆ°ÂàôÊòØÊÉ≥Ê±ÇŒ∏‰ΩøP(x0|Œ∏)P(Œ∏)ÊúÄÂ§ß„ÄÇ
                                        Ê±ÇÂæóÁöÑŒ∏‰∏çÂçïÂçïËÆ©‰ººÁÑ∂ÂáΩÊï∞Â§ßÔºåŒ∏Ëá™Â∑±Âá∫Áé∞ÁöÑÂÖàÈ™åÊ¶ÇÁéá‰πüÂæóÂ§ß
                                    MAPÂÖ∂ÂÆûÊòØÂú®ÊúÄÂ§ßÂåñP(Œ∏|x0)
                                    ÂèØ‰ª•ÂáèÁºìÂÆûÈ™åÁöÑ‰∏çÁ®≥ÂÆöÊÄß
                                    Êõ¥Â•ΩÁöÑ‰º∞ËÆ°Ôºü 
                                MLEÊòØÊääÂÖàÈ™åÊ¶ÇÁéáP(Œ∏)ËÆ§‰∏∫Á≠â‰∫é1ÔºåÂç≥ËÆ§‰∏∫Œ∏ÊòØÂùáÂåÄÂàÜÂ∏É„ÄÇ
                                ÈÄöËøá‰∫í‰ø°ÊÅØÂéªÂô™ÂêéÔºåÁî±‰∫éÂè™‰øùÁïô‰∫ÜÊúâÊïàÁöÑÁâπÂæÅÔºåÂêéÈ™åÊ¶ÇÁéáÂä†Â§ßÔºüÔºüÔºü
                        +
                                https://blog.csdn.net/BigData_Mining/article/details/81279612
                                ‰ª•‰∏ä‰∏âÁßç‰∏çÂêåÁöÑËßíÂ∫¶ËØ¥Êòé: ‰ªé‰∏Ä‰∏™‰∫ã‰ª∂Ëé∑ÂæóÂè¶‰∏Ä
                                    ‰∏™‰∫ã‰ª∂ÁöÑÂπ≥Âùá‰∫í‰ø°ÊÅØÈúÄË¶ÅÊ∂àÈô§‰∏çÁ°ÆÂÆöÂ∫¶,‰∏ÄÊó¶Ê∂àÈô§‰∫Ü‰∏çÁ°ÆÂÆöÂ∫¶,Â∞±Ëé∑Âæó‰∫Ü‰ø°ÊÅØ„ÄÇ
        ppt-16--Rule-based Text Classification --over 
            +
                ÈÄöËøá‰∏ÄÁ≥ªÂàóËßÑÂàô‚ÄúÂ¶ÇÊûú„ÄÇ„ÄÇ„ÄÇÂ∞±„ÄÇ„ÄÇ„ÄÇ‚ÄùÔºåÊù•ËøõË°åÂàÜÁ±ª
                ÂÜ≥Á≠ñÊ†ëÂèØ‰ª•ËΩ¨Êç¢ÊàêÂü∫‰∫éËßÑÂàôÁöÑÂàÜÁ±ªÂô®
                ËßÑÂàôÊåâÁÖß‰ºòÂÖàÁ∫ß‰∏ÄÊ¨°ÊéíÂàó---->ÂÜ≥Á≠ñÂàóË°®
                ËßÑÂàôÊéíÂ∫èÊ®°Âºè
                    Ôºà1ÔºâÂü∫‰∫éËßÑÂàôÊéíÂ∫èÔºöËßÑÂàôÁöÑÂàÜÁ±ªËÉΩÂäõ
                    Ôºà2ÔºâÂü∫‰∫éÁ±ªÁöÑÊéíÂ∫èÔºöÁõ∏ÂêåÁ±ªÂà´ÁöÑËßÑÂàôÊéíÂú®‰∏ÄÂùó
                rule evaluation ËßÑÂàôËØÑ‰º∞
                    1.Â¶ÇÊûú‰ø°ÊÅØÂ¢ûÁõä‰∏çÁêÜÊÉ≥Ôºå‰∏¢ÂºÉËØ•ËßÑÂàô
                    2.ÂíåÂÜ≥Á≠ñÊ†ëÁöÑpost-pruningÂêé‰øÆÂâ™Á±ª‰ºº
                        reduced error pruningÔºö
                        Á°ÆÂÆö‰∏ÄÊù°ËßÑÂàô
                        Âú®‰øÆÂâ™‰πãÂâçÂíå‰øÆÂâ™ÂêéÔºåÂàÜÂà´ËÆ°ÁÆóÊØîËæÉÂú®È™åËØÅÈõÜ‰∏äÁöÑÈîôËØØÁéá
                        Â¶ÇÊûúÈîôËØØÁéáÂèòÈ´òÔºå‰∏¢ÂºÉËØ•ËßÑÂàô
                È°∫Â∫èË¶ÜÁõñ
                    Ê≠•È™§Ôºö
                    Ôºà1Ôºâstart from an empty rule
                Áî±ÂÜ≥Á≠ñÊ†ëÁîüÊàêÔºöC4.5rules
            
            ---
            Regelbasierte Systeme
                Form: &
                    Regeln haben Wenn-Dann Form 
                Formale Darstellung &
                Bezug zu Boolschem IR
                    Ati ÊèèËø∞ÔºüÔºü
            Manuell erstellte Regeln
                exp & p10
                    ::
                        simple quick and efficient 
                        can be used to filter emails ..
            Evaluation von Regeln
                Grundlegende Ma√üe
                    ‰∏§‰∏™ÊåáÊ†á ÂÖ¨Âºè &
                    accuracy
                    Coverage
                        Â§™Â∞èÔºöÔºöoverfitting,Regel zu speziell,rule too special 
                        match too many test data , also not good 

                    ËÆ°ÁÆó & p15
                    wir mochten hohe Accuracy und niedriger Coverage-Wert
                Drei Qualit√§tsma√üe -- ÁõÆÁöÑÂè™Êúâ‰∏Ä‰∏™ÔºåÊÄªÁªìÂâçÈù¢‰∏§‰∏™Êàê‰∏∫‰∏Ä‰∏™ÊåáÊ†á
                    formular &
                    p21 ?? NEXT
                    ÔÇß Laplace-Ma√ü 
                        ÂàÜÂ≠êÂàÜÊØçÁöÑ‰∏ÄÁßçÂ∏¶ÊúâsmoothingÁöÑÁªìÂêàÊñπÂºè
                    ÔÇß Likelihood Quotienten Test
                        ‰ΩøÁî®Ê¶ÇÁéáÊäÄÂ∑ßÔºåÂàÜÊàê‰∏§ÁßçÁ±ªÂà´
                    ÔÇß FOIL's Information Gain
                        ËÆ°ÁÆóÊîπËøõÔºåÁõ∏ÂØπ‰∫éÂéüÊúâÁöÑrule
                    eg. & p26
            Sequential Covering
                NEXTNEXTNEXT
                    Âü∫‰∫éFOIL_Gain
                    This sequential covering algorithm is one of the most widespread approaches to learning disjunctive sets of rules
                    Âú®ËÆ≠ÁªÉÈõÜ‰∏äÊØèÂ≠¶Âà∞‰∏ÄÊù°ËßÑÂàôÔºåÂ∞±Â∞ÜËØ•ËßÑÂàôË¶ÜÁõñÁöÑËÆ≠ÁªÉÊ†∑‰æãÂéªÈô§ÔºåÁÑ∂Âêé‰ª•Ââ©‰∏ãÁöÑËÆ≠ÁªÉÊ†∑‰æãÁªÑÊàêËÆ≠ÁªÉÈõÜÈáçÂ§ç‰∏äËø∞ËøáÁ®ã
                    http://www.cse.unsw.edu.au/~cs9417ml/Rule/sequential.html
                    ‰ªéD‰∏≠Âà†Èô§coverÊéâÁöÑtermÔºöÔºöÁ¨¨‰∏ÄÊ¨°ÁöÑÊó∂ÂÄôÂà†Èô§‰∫Üùëàùëöùë†ùëéùë°ùëß 
                idea &
                    use graddy search , depth first search 
                Abbruchkriterium f√ºr learnRule
                    1.Laplace-Mass begain to decrease 
                    2. keine Accuracy von 1 
                    3. all documents has been removed 
                formular &
                ÔºöÔºöeg p33 ***
                Nachbearbeitung
            Aber:
                Aufbau und Pflege regelbasierter Klassifikationssysteme ist aufw√§ndig und teuer
        ppt-17--Na√Øve Bayes Text Classification --over 
            ---
            Probabilistische Klassifikation
                +
                ÔÇß Idee
                ÔÇß Lernen / Sch√§tzen von Wahrscheinlichkeiten
                Zuordnung eines Dokuments zur wahrscheinlichsten Kategorie
                idea with formular & 
                why Na√Øve &
                    it assumps that there is no relationship between terms 
                why not Naive &
                    very robost against concept drift 
                        (Ver√§nderung der Definition einer Klasse √ºber die Zeit)
                    very quick runs 
            Multinomial Na√Øve Bayes
                +corpus 
                    direct proportion to 
                    inverse proportion 
                    They are often in direct proportion to their wealth.
                        - the value of .. is in direct proportion to the number of ..
                    

                
                ÂØπ‰∫éÂ§ö‰∏™docÁöÑformular & 
                √Ñhnlichkeit zu LM (Vorlesung 12) ,exp &
                Lernphase 
                    exp & 
                        ::
                            calcu. the probability of cj 
                            calcu. the p. of tk given cj 
                                if the term appears in the docu.    

                    Klassifikationsfunktion & formular 
                    eg & 
                
                -r 
                    similar to LM 
                    mianly beased on conditional probability 
                    ‰∏ªË¶Å‰ΩøÁî®ÂêéÈ™åÊ¶ÇÁéáËÆ°ÁÆó
                    Áî®ËØçÊ±áÈ¢ëÁéáËøõË°å‰º∞ËÆ°ÔºåÊù°‰ª∂Ê¶ÇÁéá

                +
                    NB-Klassifikatoren::steht fur Na√Øve Bayes 
                    Average Accuracy zwischen 0,81 und 0,84ÊØîÂü∫‰∫éËßÑÂàôÁöÑÂ•ΩÂæàÂ§ö
                    Êú¥Á¥†Ë¥ùÂè∂ÊñØÁöÑÁã¨Á´ãÊÄßÂÅáËÆæÂæàÂÇªÂæàÂ§©ÁúüÔºåtoo simpleÔºåsometimes naiveÔºåÊâÄ‰ª•È¢ÑÊµãÁ≤æÂ∫¶ÂæÄÂæÄ‰∏çÊòØÂæàÈ´ò
                    Â¶ÇÊûúÊ≤°ÊúâÁã¨Á´ãÂÅáËÆæÔºü‰πüÂèØ‰ª•ËÆ°ÁÆóÔºü
            Bernoulli Na√Øve Bayes
                +

                    ‰∏çÁî®È¢ëÁéáÔºåËÄåÊòØTermÊòØÂê¶Âá∫Áé∞ËØÑ‰ª∑ËÆ°ÁÆó:NB‚Äôs main strength is its efficiency
                    Âõ†Ê≠§ËøôÈáå‰ΩøÁî®ÁöÑ‰∏ªË¶ÅÊòØdfÔºåÊñáÊ°£È¢ëÁéá
                    ÊàëËøôÊúâÁöÑÁî®ÂÖ¨ÂºèËÆ°ÁÆóÔºåÂÖ∂‰ªñ‰∏çÂá∫Áé∞ÁöÑÂ∫îËØ•ÊòØ‰πò‰ª•ÂèçÂêëÊ¶ÇÁéá ** ËøôÂ∞±ÊòØË¢´Âä™ÂäõÔºàÊÉ≥ÊÉ≥ÂàÜÂ∏É‰πüÊòØËøôÊ†∑Ôºâ
                    ‰∏çÊòØ‰πò‰ª•Á≥ªÊï∞ÁöÑÂÖ≥Á≥ªÔºåÂ∫ïÂ±ÇÊ®°Âûã‰∏ç‰∏ÄÊ†∑
                    ‚ÄúTerm vorhanden‚Äù oder ‚ÄúTerm nicht vorhanden‚Äù als 
                        Ergebnis der Beobachtung eines Dokumentes
                        Bernoulli-Verteilung
                    KlassifikationsfunktionÔºü **
                    Anmerkungen
                    Aktuelle Anwendung
                        Ë∑®ËØ≠Ë®Ä‰ø°ÊÅØÊ£ÄÁ¥¢ 
                idea with formuar  &
                Klassifikationsfunktion &
                Beispiel &
                Anmerkungen &
                    -r 
                        NB‚Äôs main strength is its efficiency
                            uses binary term occurrence features 
                            Multinomial Naive Bayes is in a sense more complex model 
                        suitable for short doc.
                        It has the benefit of explicitly modelling the absence of terms
                        
                    F√ºr kurze Dokumente geeignet
                    Sinnvoll bei negativem Zusammenhang zwischen Termen und Kategorien
                    Etwa bei positiven oder negativen Ergebnisse von medizinischen 
                        Tests um einen Krankheitsbefund zu erstellen
            Aktuelle Anwendung &
                CLIR-System
        ppt-18--Vector Space Text Classification   --over
            +
                mit Cosinusma√ü
                Rocchio Klassifikator
                    -r 
                        Assign category to the nearest centroid
                        nearst centroid and far from others 

                        

                    Âü∫Êú¨ÊÄùË∑Ø
                        1.ÁÆÄÂçïÂæóÂè™ËÆ°ÁÆó‰∏≠ÂøÉ
                        2.ÊàñËÄÖËÄÉËôëÊ≠£Âèç‰æãÂ≠êÔºö‰∏≠ÂøÉÁöÑ‰ΩçÁΩÆ‰∏éÊ≠£‰æãÂ≠êËøëËÄå‰∏éÂèç‰æãËøú
                    Áº∫ÁÇπÔºöÔºö
                        ÂÆÉËÆ§‰∏∫‰∏Ä‰∏™Á±ªÂà´ÁöÑÊñáÊ°£‰ªÖ‰ªÖËÅöÈõÜÂú®‰∏Ä‰∏™Ë¥®ÂøÉÁöÑÂë®Âõ¥ÔºåÂÆûÈôÖÊÉÖÂÜµÂæÄÂæÄ‰∏çÊòØÂ¶ÇÊ≠§
                        ‰∏çÂèØ‰ª•ÂÆπÈîô--‰∏á‰∏ÄÊúâ‰∏Ä‰∏™ÈîôËØØÁöÑÂë¢ÔºÅÔºÅËá¥ÂëΩ

                        -r
                            there will be sth. wrong , when the classes are not convex  
                                but in practice , classes are rarely distributed in this way 
                                like spheres with similar radii
                            it is not robost with noise 

                    Â∏∏Â∏∏Ë¢´Áî®Êù•ÂÅöÁßëÁ†î‰∏≠ÊØîËæÉ‰∏çÂêåÁÆóÊ≥ï‰ºòÂä£ÁöÑÂü∫Á∫øÁ≥ªÁªüÔºàBase LineÔºâ
                    Áî®‰∫éÁõ∏ÂÖ≥ÂèçÈ¶àÔºö
                        ÂÅáÂÆöÊàë‰ª¨Ë¶ÅÊâæ‰∏Ä‰∏™ÊúÄ‰ºòÊü•ËØ¢ÂêëÈáèq Ôºå
                        ÂÆÉ‰∏éÁõ∏ÂÖ≥ÊñáÊ°£‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶ÊúÄÂ§ß‰∏îÂêåÊó∂ÂèàÂíå‰∏çÁõ∏ÂÖ≥ÊñáÊ°£‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶ÊúÄÂ∞è
                    0,86 und 0,87
                    Segmentierung des Vektorraumes = Voronoi-Diagramm
                kNN 
                    Â¶ÇÊûú‰∏Ä‰∏™Ê†∑Êú¨Âú®ÁâπÂæÅÁ©∫Èó¥‰∏≠ÁöÑk‰∏™ÊúÄÁõ∏‰ºº(Âç≥ÁâπÂæÅÁ©∫Èó¥‰∏≠ÊúÄÈÇªËøë)
                        ÁöÑÊ†∑Êú¨‰∏≠ÁöÑÂ§ßÂ§öÊï∞Â±û‰∫éÊüê‰∏Ä‰∏™Á±ªÂà´ÔºåÂàôËØ•Ê†∑Êú¨‰πüÂ±û‰∫éËøô‰∏™Á±ªÂà´Ôºå
                        ÂÖ∂‰∏≠KÈÄöÂ∏∏ÊòØ‰∏çÂ§ß‰∫é20ÁöÑÊï¥Êï∞
                    KNNÁÆóÊ≥ï‰∏≠ÔºåÊâÄÈÄâÊã©ÁöÑÈÇªÂ±ÖÈÉΩÊòØÂ∑≤ÁªèÊ≠£Á°ÆÂàÜÁ±ªÁöÑÂØπË±°„ÄÇ
                        ËØ•ÊñπÊ≥ïÂú®ÂÆöÁ±ªÂÜ≥Á≠ñ‰∏äÂè™‰æùÊçÆÊúÄÈÇªËøëÁöÑ‰∏Ä‰∏™
                        ÊàñËÄÖÂá†‰∏™Ê†∑Êú¨ÁöÑÁ±ªÂà´Êù•ÂÜ≥ÂÆöÂæÖÂàÜÊ†∑Êú¨ÊâÄÂ±ûÁöÑÁ±ªÂà´„ÄÇ
                    ‰æùËµñ‰∫ék
                    KNNÈÄöËøá‰æùÊçÆk‰∏™ÂØπË±°‰∏≠Âç†‰ºòÁöÑÁ±ªÂà´ËøõË°åÂÜ≥Á≠ñÔºå
                        ËÄå‰∏çÊòØÂçï‰∏ÄÁöÑÂØπË±°Á±ªÂà´ÂÜ≥Á≠ñ„ÄÇËøô‰∏§ÁÇπÂ∞±ÊòØKNNÁÆóÊ≥ïÁöÑ‰ºòÂäø
                    Varianten
            ---
            Klassifikation im Vektorraum
                Vektorraum:vector space
                ÁîªÂõæ & p6 
                exp &
                    Aside: 2D/3D graphs can be misleading 
                    ::
                        the vectors are high dimentional vectors 
                        compare them to measure similarity between them.
            Rocchio Klassifikator
                rocchio classification algorithm
                Rocchio relevance feedback is designed to
                    distinguish only two classes, relevant and nonrelevant.
                ÁΩóÂü∫Â••ÔºÅÔºÅ
                exp idea &
                    Kategorie des n√§chstgelegenen Zentroiden zuweisen
                    ËßÇÂØüÂæóÂà∞ÁöÑÁâπÁÇπÔºöliegen untereinander r√§umlich dicht
                        beisammen und gleichzeitig zu den anderen Kategorien weiter entfernt
                    Zentroiden liegen im Zentrum der Trainingsdokumente zu einer Kategorie
                    ::
                        it uses centroids to calcu. 
                            the center means :vector average
                            one centroid means one class 
                        boundaries?
                            points with equal distance from two centroids
                            are boundaries of classes .
                formular & p13 
                Beispiel & 
                Probleme bei Rocchio &
                    Fl√§che des Voronoi-Diagramms sind konvex
                    Probleme bei
                        ÔÇß Unterschiedlicher Gr√∂√üe
                        ÔÇß Unterschiedlicher Dichte
                        ÔÇß Verschlungene / ‚Äûverzahnte‚Äú Formen
                    Bereits Fehler auf Trainingsdaten
                    ÂÆÉËÆ§‰∏∫‰∏Ä‰∏™Á±ªÂà´ÁöÑÊñáÊ°£‰ªÖ‰ªÖËÅöÈõÜÂú®‰∏Ä‰∏™Ë¥®ÂøÉÁöÑÂë®Âõ¥ÔºåÂÆûÈôÖÊÉÖÂÜµÂæÄÂæÄ‰∏çÊòØÂ¶ÇÊ≠§
                Segmentierung des Vektorraumes = Voronoi-Diagramm
            KNN
                +ÂèÇËÄÉÊâãÂÜôÊï∞Â≠óËØÜÂà´Á®ãÂ∫è 
                ÊòØ‰∏äÈù¢ÊñπÊ≥ïÁöÑÂèòÁßçÊâ©Â±ï √ó√ó
                process & 
                    Zu Dokument k n√§chstgelegenen Dokumente ausw√§hlen und deren Kategorien betrachten
                    am h√§ufigsten vorkommt wird dem zu klassifizierenden Dokument zugewiesen

                    ::
                        the result class will be assigned to the class
                            most common among its k nearest neighbors
                        we find the k nearest neighbors, an then , find the most commen class among them 

                Beispiel &
                1NN: Voronoi-Diagramm mit mehreren Zellen
                Wahl von k? &
                    Â∞èÁöÑÊó∂ÂÄôÊòØ‰∏Ä‰∏™Á±ªÂà´ÔºåÂ¢ûÂä†‰ª•ÂêéÊòØÂè¶‰∏Ä‰∏™Ôºå‰∏çÁ®≥ÂÆö
                    ÂÖ∂‰∏≠KÈÄöÂ∏∏ÊòØ‰∏çÂ§ß‰∫é20ÁöÑÊï¥Êï∞
                    ÁîªÂõæ
                    Âú®Â∫îÁî®‰∏≠ÔºåÊàë‰ª¨‰∏ÄËà¨Âèñ‰∏Ä‰∏™ËæÉÂ∞èÁöÑkÂÄºÔºåÈÄöÂ∏∏ÈááÁî®‰∫§ÂèâÈ™åËØÅÊ≥ïÊù•ÈÄâÂèñÊúÄ‰ºòÁöÑkÂÄº„ÄÇ

                    ::
                        the result may not stable with k 
                            when k is small , the reuslt is one , when it larger , be an other .
                        Normally , k is no larger than 20 
                        use cross validation to choose the value of k 

                ÂèòÁßç & 
                    nennen Sie zwei
                    -r 
                        we can set weight to the neighbors 
                        use the raw vector , instead of normalize them.

                    1.Gewichtung der Nachbarn
                    2.Keine Operationen auf Vektoren
        ppt-19--SVMÂàÜÁ±ªÊï∞ÊçÆ  --over 
        +Âë®Êú∫Âô®Â≠¶‰π†
        +Âü∫‰∫éÁ•ûÁªèÁΩëÁªúÁöÑÂàÜÁ±ª
        +
            Skalarprodukts
            Mehrklassenprobleme
            Iteratives Verfahren
                Sonst bei jeder Iteration mindestens ein Fehler ÔÉ† Gewichtsvektor und Bias m√ºssten erneut
                    angepasst werden
                Lernrate: beeinflusst nur L√§nge von w
                Duale Darstellung
                Kein SVM
                Âè™ÊòØ‰∏ÄÁßçËø≠‰ª£ÊñπÊ≥ï‰∫ßÁîüË∂ÖÂπ≥Èù¢ÔºåÁî®‰∏Ä‰∏™Êù°‰ª∂ÔºåÁî®‰∏ÄÂÆöÁöÑÂ≠¶‰π†ÈÄüÁéáËøõË°åËøô‰∏™ËøáÁ®ã
                Wiederholung der Durchl√§ufe bis alle Beispiele richtig klassifiziert werden
                ËÄÉËôëÂØπÂÅ∂ÈóÆÈ¢ò
            SVM 
                die m√∂glichst viel Abstand zu den Datenpunkten aus den 
                    verschiedenen Kategorien l√§sst
                St√ºtzvektoren
                Duale Formulierung
                +‰∫åÊ¨°ËßÑÂàí
                    ‰ºòÂåñ(ÊúÄÂ∞èÂåñÊàñÊúÄÂ§ßÂåñ)Â§ö‰∏™ÂèòÈáèÁöÑ‰∫åÊ¨°ÂáΩÊï∞ÔºåÂπ∂Êúç‰ªé‰∫éËøô‰∫õÂèòÈáèÁöÑÁ∫øÊÄßÁ∫¶Êùü
                    ‰∏ÄÁßçÊï∞Â≠¶ÈóÆÈ¢ò
                    ‰∫åÊ¨°ËßÑÂàíÊòØ‰∏ÄÁßçÁâπÊÆäÁöÑÈùûÁ∫øÊÄßËßÑÂàí
                    Ê†πÊçÆ‰ºòÂåñÁêÜËÆ∫Ôºå‰∏Ä‰∏™ÁÇπxÊàê‰∏∫ÂÖ®Â±ÄÊúÄÂ∞èÂÄºÁöÑÂøÖË¶ÅÊù°‰ª∂ÊòØÊª°Ë∂≥Karush-Kuhn-TuckerÊù°‰ª∂ÔºàKKTÔºâ„ÄÇ
                    ÂΩìf(x)ÊòØÂá∏ÂáΩÊï∞Êó∂ÔºåKKTÊù°‰ª∂‰πüÊòØÂÖÖÂàÜÊù°‰ª∂
                    ÂΩì‰∫åÊ¨°ËßÑÂàíÈóÆÈ¢òÂè™ÊúâÁ≠âÂºèÁ∫¶ÊùüÊó∂Ôºå‰∫åÊ¨°ËßÑÂàíÂèØ‰ª•Áî®Á∫øÊÄßÊñπÁ®ãÊ±ÇËß£„ÄÇ
                    Âê¶ÂàôÁöÑËØùÔºåÂ∏∏Áî®ÁöÑ‰∫åÊ¨°ËßÑÂàíËß£Ê≥ïÊúâÔºö
                        ÂÜÖÁÇπÊ≥ï(interior point)„ÄÅ
                        active setÂíåÂÖ±ËΩ≠Ê¢ØÂ∫¶Ê≥ïÁ≠â„ÄÇ
                    Âá∏ÈõÜ‰∫åÊ¨°ËßÑÂàíÈóÆÈ¢òÊòØÂá∏‰ºòÂåñÈóÆÈ¢òÁöÑ‰∏Ä‰∏™Áâπ‰æã„ÄÇ
                    ÂØπ‰∫é‰∏ÄËà¨ÈóÆÈ¢òÔºåÂ∏∏Áî®ÁöÑÊñπÊ≥ïÊúâÂæàÂ§öÔºåÂåÖÊã¨
                        interior point,ÂÜÖÁÇπÊ≥ï
                        active set ÂÜÖÁÇπÊ≥ï
                        augmented Lagrangian
                        conjugate gradient,
                        gradient projection,
                        extensions of the simplex algorithm
                    ËÆ°ÁÆóÂ§çÊùÇÊÄß
                        ÂΩìQÊ≠£ÂÆöÊó∂ÔºåÁî®Ê§≠ÂúÜÊ≥ïÂèØÂú®Â§öÈ°πÂºèÊó∂Èó¥ÂÜÖËß£‰∫åÊ¨°ËßÑÂàíÈóÆÈ¢ò„ÄÇ
                        ÂΩìQÈùûÊ≠£ÂÆöÊó∂Ôºå‰∫åÊ¨°ËßÑÂàíÈóÆÈ¢òÊòØNPÂõ∞ÈöæÁöÑÔºàNP-HardÔºâ„ÄÇ
                        Âç≥‰ΩøQÂè™Â≠òÂú®‰∏Ä‰∏™Ë¥üÁâπÂæÅÂÄºÊó∂Ôºå‰∫åÊ¨°ËßÑÂàíÈóÆÈ¢ò‰πüÊòØNPÂõ∞ÈöæÁöÑ
                +ÂèòÁßç
                    ËΩØÈó¥ÈöîÂàÜÁ±ªÂô®
                    https://www.jianshu.com/p/8a499171baa9
                    ÈíàÂØπÂô™Â£∞ÈóÆÈ¢ò
                    ‰ΩøÁî®svmÂ∫ìÊó∂ÈúÄË¶ÅË∞ÉÊï¥‰∏çÂ∞ëÂèÇÊï∞Ôºå‰ª•È´òÊñØÊ†∏‰∏∫‰æãÔºåÊ≠§Êó∂Ëá≥Â∞ëÈúÄË¶ÅËÄÉËôëÁöÑÂèÇÊï∞ÊúâCÂíåœÉ
                        ÔºåÂÖ∂‰∏≠C‰∏∫ËÆ≠ÁªÉÊ†∑Êú¨ËØØÂ∑ÆÈ°πÁöÑÊÉ©ÁΩöÁ≥ªÊï∞ÔºåËØ•ÂÄºÂ§™Â§ßÊó∂Ôºå‰ºöÂØπËÆ≠ÁªÉÊ†∑Êú¨ÊãüÂêàÂæàÂ•ΩÔºàbiasÂ∞èÔºâ
                        Ôºå‰ΩÜÂØπÊµãËØïÊ†∑Êú¨ÊïàÊûúÂ∞±ËæÉÂ∑Æ(varianceÂ§ß)
                +Ëß£ÂÜ≥Êõ≤Á∫øÂàÜÁ±ªÈóÆÈ¢òÔºö
                    Â¶Ç‰ΩïÁêÜËß£Ôºü ÔºüÔºü pptÔºüÔºü
                    ÊîØÊåÅÂêëÈáèÊú∫È¶ñÂÖàÂú®‰ΩéÁª¥Á©∫Èó¥‰∏≠ÂÆåÊàêËÆ°ÁÆóÔºåÁÑ∂ÂêéÈÄöËøáÊ†∏ÂáΩÊï∞Â∞ÜËæìÂÖ•Á©∫Èó¥Êò†Â∞ÑÂà∞È´òÁª¥ÁâπÂæÅÁ©∫Èó¥Ôºå 
                        ÊúÄÁªàÂú®È´òÁª¥ÁâπÂæÅÁ©∫Èó¥‰∏≠ÊûÑÈÄ†Âá∫ÊúÄ‰ºòÂàÜÁ¶ªË∂ÖÂπ≥Èù¢Ôºå‰ªéËÄåÊääÂπ≥Èù¢‰∏äÊú¨Ë∫´‰∏çÂ•ΩÂàÜÁöÑÈùûÁ∫øÊÄßÊï∞ÊçÆÂàÜÂºÄ &
                    Kernel Trick & Ëß£Èáä Ê†∏ÊäÄÂ∑ßÔºü
                        Á∫¶ÊùüÈóÆÈ¢ò‰∏≠ÂÜÖÁßØœïi‚ãÖœïjÁöÑËøêÁÆó‰ºöÈùûÂ∏∏ÁöÑÂ§ß‰ª•Ëá≥‰∫éÊó†Ê≥ïÊâøÂèóÔºåÂõ†Ê≠§ÈÄöÂ∏∏Êàë‰ª¨‰ºöÊûÑÈÄ†‰∏Ä‰∏™Ê†∏ÂáΩÊï∞
                        ‰∏ÄËà¨ÂæàÈöæÊûÑÈÄ†Âá∫ÂÆåÂÖ®Á¨¶ÂêàËæìÂÖ•Á©∫Èó¥ÁöÑÊ†∏ÂáΩÊï∞ÔºåÂõ†Ê≠§Êàë‰ª¨Â∏∏Áî®Â¶Ç‰∏ãÂá†ÁßçÂ∏∏Áî®ÁöÑÊ†∏ÂáΩÊï∞Êù•‰ª£ÊõøËá™Â∑±ÊûÑÈÄ†Ê†∏ÂáΩÊï∞
                        Ê†∏ÂáΩÊï∞ÁöÑÈÄâÊã©Âú®SVMÁÆóÊ≥ï‰∏≠Â∞±ÊòæÂæóËá≥ÂÖ≥Èáç
                    Âá†ÁßçÂ∏∏Áî®ÁöÑÊ†∏ÂáΩÊï∞
                        Á∫øÊÄßÊ†∏ÂáΩÊï∞
                            Êàë‰ª¨ÈÄöÂ∏∏È¶ñÂÖàÂ∞ùËØïÁî®Á∫øÊÄßÊ†∏ÂáΩÊï∞Êù•ÂÅöÂàÜÁ±ªÔºåÁúãÁúãÊïàÊûúÂ¶Ç‰ΩïÔºåÂ¶ÇÊûú‰∏çË°åÂÜçÊç¢Âà´ÁöÑ &
                        Â§öÈ°πÂºèÊ†∏ÂáΩÊï∞
                            ÁöÑÂèÇÊï∞Â§öÔºåÂΩìÂ§öÈ°πÂºèÁöÑÈò∂Êï∞ÊØîËæÉÈ´òÁöÑÊó∂ÂÄôÔºåÊ†∏Áü©ÈòµÁöÑÂÖÉÁ¥†ÂÄºÂ∞ÜË∂ã‰∫éÊó†Á©∑Â§ßÊàñËÄÖÊó†Á©∑Â∞èÔºåËÆ°ÁÆóÂ§çÊùÇÂ∫¶‰ºöÂ§ßÂà∞Êó†Ê≥ïËÆ°ÁÆó
                            Áª¥Êï∞ÊòØÂèÇÊï∞parameter in sklearn learn
                        È´òÊñØ
                            ËÄå‰∏îÂÖ∂Áõ∏ÂØπ‰∫éÂ§öÈ°πÂºèÊ†∏ÂáΩÊï∞ÂèÇÊï∞Ë¶ÅÂ∞ëÔºåÂõ†Ê≠§Â§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÂú®‰∏çÁü•ÈÅìÁî®‰ªÄ‰πàÊ†∏ÂáΩÊï∞ÁöÑÊó∂ÂÄôÔºå‰ºòÂÖà‰ΩøÁî®È´òÊñØÊ†∏ÂáΩÊï∞
                            generate radial areas around training points
                        ÈááÁî®sigmoidÊ†∏ÂáΩÊï∞
                            ÊîØÊåÅÂêëÈáèÊú∫ÂÆûÁé∞ÁöÑÂ∞±ÊòØ‰∏ÄÁßçÂ§öÂ±ÇÁ•ûÁªèÁΩëÁªú ÔºüÔºü 
                    ÂèÇÊï∞
                        C:tradeoff between how smooth the decision boundary is and how well it classifies examples
                        Gamma: kernel coefficient for rbf, poly, and sigmoid
        ---
        Klassifikation √ºber Hyperebene
            Darstellung Hyperebene mit Ebenengleichung 
                -r 
                    If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes
                    hyperplane

                formular &
                clacu p7 &
            Vorgehen bei mehr als zwei Kategorien ?Multiclass SVMs?
                exp & p10
                    ::
                        1.Build a classifier for each class
                        2.Given the test document, apply each classifier separately.
                        3.Assign the document to the class with the maximum score
                        example ??
            
        Trainingsphase:Perzeptron--Iteratives Verfahren von Hyperebene
            verfahren exp &
                eg. Wiederholung der Durchl√§ufe bis alle Beispiele 
                    richtig klassifiziert werden
                Gewichtsvektor und Bias m√ºssten erneut angepasst werden bei Fehlern.
                :: 
                    0.start from w=0 and b=0
                    1.use the training set to update the value of w,b
                    2.repeat until all docs. are classified 

                    +the perceptron is an algorithm for supervised learning 
                        of binary classifiers
                    https://en.wikipedia.org/wiki/Perceptron#Multiclass_perceptron
            algo p15 & 
            calcu. p16 & 
            Perzeptron erlernt nur eine Hyperebene
            learn rate:
                control the relationship between weight vector and bias 
        SVM
            m√∂glichst viel Abstand
            ÔÇß Linearer Fall
                Einfach Verfahren &
                    formular p30 & -
                    Duale Formulierung & -
                        mit dem Gradientenverfahren l√∂sbar
                        L√∂sung dieses Problems = Lagrange-Multiplikatoren Œ± i
                        Trainingsdokument ist ein St√ºtzvektor ???? NEXT
                    Beispiel p33 &
                    Sensitivit√§t
                Erweitertes Optimierungsverfahren
                    Einf√ºhrung einer SchlupfvariableÔºöÔºöÂºïÂÖ•ÊùæÂºõÂèòÈáè
                    slack variable
                    formular &
                    Einfluss von C exp & -
                        darstellung mit ein Bild 
                        Je gr√∂√üer C je st√§rker werden falsch
                            klassifizierte Trainingspunkte bestraft
                        ::
                            The parameter C is a regularization term, 
                                which provides a way to control overfitting
                            in our course , it should be small .
                            small  c has a better effect , Robust against failure
                            
            ÔÇß Kernel Trick--Nichtlineare Entscheidungsgrenzen
                key idea &
                    Âú®È´òÁª¥ÁâπÂæÅÁ©∫Èó¥‰∏≠ÊûÑÈÄ†Âá∫ÊúÄ‰ºòÂàÜÁ¶ªË∂ÖÂπ≥Èù¢Ôºå‰ªéËÄåÊääÂπ≥Èù¢‰∏äÊú¨Ë∫´‰∏çÂ•ΩÂàÜÁöÑÈùûÁ∫øÊÄßÊï∞ÊçÆÂàÜÂºÄ
                    ::
                        saparate the calsses in high dimentional .
                        It will be easier to do classification .
                types of kernel  & 
                    mehr als 4 
                        RBF Kernel--Gau√ü‚Äòscher Kernel
                        Polynomial Kernel
                        Sigmoid Kernel
                        Cosinus Similarity Kernel
                        Chi-squared Kernel




