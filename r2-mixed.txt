
    -----
    full notes below
        ppt-1-Einfurung --over --c
            was ist IR? 
                Information Retrieval (IR) is 
                    -finding material in
                    -an unstructured text , 
                    -satisfies an information needs
                    -Searches can be based large collections (usually stored on computers)
                personal::
                    everything in our daily life , around us can not be searched without this knowledge/technique
            unstructured nature (usually text)? & - 三个level
                public 
                    articles -- 
                        scientific articals 
                        from press news  
                        Webseiten
                --
                Office
                    documents (eg. Spreadsheets)
                    e mails 
                --
                personal 
                    multimedia material (z.B. Fotos und Videos)
                    user information  (z.B. on Facebook)
            information need--Informationsbedürfnis des Benutzers
                Vage (z.B. Was kann man in Berlin unternehmen?)
                not Präzise Anfragen in (relational en) Datenbanken
            比较，Abgrenzung IR und Datenbank & - 
                p31
            Große Datenmengen mit rapidem Wachstum
            Typische Anwendung des Text-Retrievals sind heute Web-Suchmaschinen
            Was ist Information? 
                what is Daten, Wissen Information &

                    1.data : coded information that computer can understand   最原子的一个概念
                    2.knowledge : data with semantic 很大的一个范围
                    3.informaiton : part of knowledge , that can answer a specific question . useful knowledge  介于中间

                    1.Daten：Maschinell verarbeitbare, kodierte Informationen 具有结构的数据 1999-11-05 Zeichenkettenformat: ZZZZ-ZZ-ZZ
                    2.Wissen ist Daten mit Semantik：Gesamtheit aller Kenntnisse eines Sachgebietes 几句话，具有语义信息的数据 
                        "I need one Euro for the car park.“, "I will
                        be home about 6.30 if I can catch the 4.54.
                    3.Information ：Teilmenge des Wissens um spezielle Aufg
                        Information ist nutzbares Wissen   ， Wann war ich in Berlin?
            Aufbau eines IR-Systems  示意图 p15  & 
                    Dokumentverarbeitung包含什么？ 
                    Anfragen 包含啥 
                    Matching und Ergebnisliste
                    Feedback
                IR系统的概念模型是系统的基本方法
                Vektorraummodell向量空间模型
                Probabilistisches Retrieval概率模型
                    Wahrscheinlichkeit
            compare IR and databank &
                五个方面
                    matching , Model , query language - query fault (miss spelling)
                    result 

                    corpus:
                        exact match ... probability accuracy ...
                        natural langu. ... structured lan. .. relevant ranking ... 
                一个重点:对象不同
                    查询的是unstructured text and structured resources 
            Elementare Fragestellungen 
                基础问题 p23 &
                Informationsbedürfnis?
                Dokumente建模？
                effizient?
                Güte?
            Anwendungsbeispiele & x2
                Bildersuche
                Suche in Open-Source-Projekten
                Stack Overflow ,Programming QA site
                Soundcloud , Musikempfehlungen
        ppt-2-Bool --over --c
            Dokument ist entweder relevant oder nicht
            Suchbegriffe können durch boolsche Operatoren verknüpft werden 可以连接
            Boolean Retrieval
                the simplest model
                The search engine returns all documents that satisfy the Boolean expression
                Why is grep not the solution & p6 --
                    can not deal with negation expression 
                    can not support near operations 
                    slow on large collections

                     Slow (for large collections) 大量数据时候很慢
                     line-oriented, IR is document-oriented 面向行的检索
                     can not express negation [NOT CALPURNIA] is non-trivial
                     do not support near operations (e.g., find the word ROMANS near COUNTRYMAN ) not feasible 对于其他操作没有支持
                提前为文档建立索引
                Term-document incidence matrix 
                    是否出现，出现就是1 否则是0的一个矩阵
                    Entry is 1 if term occurs 
                    Incidence vectors 事件向量
                        概念 : 0/1 vector for each term
                    To answer the query with help of Term-document incident matrix ？ example & -
                        Do a (bitwise) AND on the three vectors
                    why not used? & - 
                        Bigger collections Can’t build the incidence matrix! **
                        needs huge storage 
                        是一个稀疏矩阵
                Inverted Index
                    idea：We only record the position of 1s
                    why inverted ? Inverted Index & 
                        Normally, we want to index form docs to terms . But now , we use terms as dictionary and docID as posting list
                        倒排？一般是文档索引到单词的，这里是从词项反向映射到文档的 For each term t *
                        ::For each term t, we store a list of all documents that contain t
                    postings？ 索引条目
                    what is posting list? & - p11     
                        For each term t, we store a list of all documents that contain t       
                    posting list 建立过程 3个步骤 & p14
                        画图  - 1,2,3,4   4的步骤有三个
                        Tokenize变成词条 词汇切分
                        Do linguistic preprocessing大小写转换之类的
                        实例中仅仅是大小写转换 ？ 
                        Sort posting ？ 按照字母顺序排序
                        Create postings lists, determine doc freq
                        横着三个项目column：term doc.freq. posting lists  
                    其他细节：
                        两个文件Split the result into dictionary and postings file
                            dic和posting如何存储 & -
                                ::
                                    we want to keep dictionary in memory 
                                    and store postings as files on disk
                        Size of postings much larger than size of dictionary  dictionary is commonly kept in memory, postings on disk
                        原理规律：其实文章中用词并不多，人类社会学现象
                        How much space do we need for dictionary and index? 
                        index compression: how can we efficiently store and process indexes for large collections?
                Queries 
                    和建立过程相对
                        Intersecting two posting lists
                            For each of the terms, get its postings list, then AND them together
                            complicity:This is linear in the length of the postings lists 
                            postings lists should be sorted
                        do intersection for two postings & -
                            画图之后再描述
                            pseudocode 
                            code 
                Query Optimization
                        What is the best order for processing this query? & 
                            Start with the shortest postings list, then keep cutting further
                            more general , Process in increasing order of or sizes
                        Optimized with sort 
                                pseudocode 
                                code 
                            Recall回忆  
                        using skip lists how ? &
                            we do better than this (sub-linear time)?
                                Skip pointers allow us to skip postings that will not figure in the search results
                                This makes intersecting postings lists more efficient
                            Intersection with Skip Pointers 
                                pseudocode & -
                                code 
                                For postings list of length p， use 根号p evenlyspaced skip pointers
                                harder in a dynamic environment because of updates
                                如果过了怎么办？
                                NEXT
                            why skip lists are not used now &
                                1.cpus are faster 
                                2.can be slow when the list is always in changing 
                Boolean Search
                    can answer Boolean expression
                    precise: Document matches condition or not精确的
                    Primary commercial retrieval method for three decades三十年来主要采用商业检索方法
                    Many professional searchers (e.g., lawyers) still like Boolean queries
                    You know exactly what you are getting
                    Many search systems you use are also Boolean: spotlight, email, intranet etc
                        email searching : when you searching an email content ,it use bool search at usual 
                    Westlaw
                        Commercially successful Boolean retrieval: Westlaw
                        Largest commercial legal search service
                        The service was started in 1975
                        特性 & -
                            Proximity operators
                            1.Space is disjunction, not conjunction! This was the default in search pre-Google
                            2.Long, precise queries
                                Precision, transparency, control
                            Boolean queries return set of matching documents 
                                order the returned results, scoring function
                            Search for compounds or phrases needs to increase the number of the index
                Phrase Queries 对于词组的支持
                    “The inventor Stanford Ovshinsky never went to university” should not be a match
                    About 10% of web queries are phrase queries
                    It is no longer suffices to store docIDs in postings lists
                    Two ways of extending the inverted index
                        Bi-word index 
                            explain & -
                                Index every consecutive pair of terms in the text as a phrase
                                Each of these bi-words is now a vocabulary term
                                add to dictionary (larger scale )
                                change the queries to AND form to do Phrase queries:: not only Two-word queries  
                            bi-words as vocabulary terms
                            A long phrase like “stanford university palo alto” 
                                --can be divided to some Bi-words with AND between them 
                            can be occasional false positives
                            but:we should do 
                                post-filtering 
                                of hits to identify subset actually 4-word phrase?
                        Extended Bi-words
                                key idea & -
                                    Bucket the terms into  Nouns (N) and Articles/prepositions (X) ***
                                    use it to match larger words , with arbitry length
                                    more efficient as Bi words 
                                Parse each document and perform partof-speech tagging
                                Classify words as nouns, verbs, etc.
                                    Now deem any string of terms of the form
                                    ��∗� to be an extended bi-word
                            DIS:
                                very large term vocabulary   
                        why Bi words not used ? &
                            1.false positives
                            2.large term vocabulary 
                        what is Positional index &
                            use the positional informaiton to determine a phrase query 
                            commonly used 
                            a more efficient alternative to 
                            Each posting is a docID and a list of positions
                            Example in page 38: & -
                                pay attention to positon: 
                                    in the 4th doc, TO has a position of 16
                                    and BE has a positoin of 17
                                    this means that they are close to each other in doc 4
                                    and it can be used as resault 
                            Proximity search :: k word proximity search 临近检索
                                E.g. Find all documents that contain EMPLOYMENT and PLACE within 4 words of each other
                                +stop words: a the in or ....
                                “Proximity” intersection algo 
                                    pseudocode & -
                        Combination scheme
                            how & -
                                Include frequent bi-words as vocabulary terms in the index
                                Do all other phrases by positional intersection
                            extremely frequent :: Bi-words
                            Many bi-words are extremely frequent: “Michael Jackson”, “Barack Obama” etc
                            For these bi-words, increased speed compared to positional postings intersection is substantial
                            What are “good” bi-words & -
                                Phrase where the 
                                    individual words are common 
                                    but::the desired phrase is comparatively rare
                                        explain as follow :
                                        Adding “Barack Obama” as a phrase index entry may
                                            only give a speedup factor to that query of
                                        about 3 (most documents that mention either
                                            word are valid results) whereas…
                                        - …adding “The Who” as a phrase index entry may speed up that query by a factor of 1,000
        ppt-3-Getting Term --over --c
            How to get terms out of documents
                challenges
                    General and Non-english
                        What format is it in (pdf, word, excel, html etc.)?
                        Reading direction: from left to right, right to left, column-wise
                        What character set is in use? - UTF-8, ASCII, ISO-8859-1,
                        determine lan? & -
                            use frequent words (German: der, die, das, und, ein, einer,
                            Englisch: the, a, and, or, one)
                        What is the document unit
                            Answering the question “what is a document?” is not trivial 
                        Definitions  
                            Term,Morphem,Inflection & - NEXT
                            Derivation:Forming a new word from an existing word
                            Kompositum: consists of more than one stem 词干
                            Noun Phrase (NP):noun as its head word
                    Tokenization problems
                        string into words or tokens
                        Needed to apply further processing, e.g. stemming and lemmatization
                        promblems for reading word? & - NEXT
                        promblems for reading words in IR? & *5
                            //
                                numbers 
                                no white space between words.
                                diff. meaning of words . eg Bush 
                            Short Forms
                            Orthographic word: string of characters with ‘whitespace’ at each end;
                            Word form: have, has, had, having are word forms of the lemma have
                            One word or two or several
                                 Hewlett-Packard
                                 State-of-the-art
                                 Co-education
                            Numbers
                                 3/20/91
                                 20/3/91
                                 Mar 20, 1991
                            Apostrophes can be 
                                part of a word, 
                                a part of a possesive 
                                just a mistake
                            Capitalized words: different meanings
                            chinese:No whitespaces between words
                            chinese:Ambiguous segmentation
                            japanese:End user can express query entirely in hiragana!
                    Normalization
                        in indexed text as well as query terms into the same form
                            eg:We want to match U.S.A. and USA
                        we can 
                            do asymmetric expansion,but less efficient
                            Normalization and language detection interact
                        some Terminology & -
                            Grammatical markings
                                bakes is a (grammatically) inflected form
                            Stemming:找词干
                                algorithms work by cutting off the end or the beginning of the word
                            Lemmatization：词形还原，单复数 
                                更加完整，不会仅仅去掉前缀
                            Case Folding
                                Reduce all letters to lower case
                            Stop words = extremely common words
                        More Classing tech？
                            phonetic equivalents：Beijing and Peking
                            Thesauri：Semantic equivalence, car = automobile
                            calcu Soundex & -  
                                根据语法规则写一串数字
                                而后是个step的缩减工作（消除重复，，）
                    stemming algorithms
                         Table lookup approach 
                            &explain -
                            root forms and inflected forms wrote in a table 
                            Building lookup tables is very labour-intensive
                            High probability that these tables may miss out some exceptional cases
                         Successor Variety 
                            idea 
                                a method to do stemming .
                                find the boundaries of the words with the help of the Varieties of the suffix .
                            &calcu -
                            distribution of phonemes 
                            Successor variety of substrings of a term will decrease as more characters are added ----> boundry is reached 
                            based on Corpus to find characters following 
                            see Pic P28
                            是词干的标志：sharply increase **
                            Example
                                 Test Word: READABLE
                                 Corpus: ABLE, APE, BEATABLE, FIXABLE, READ,
                                READABLE, READING, READS, RED, ROPE, RIPE
                         n-gram stemmers
                            based on shared unique n-grams 
                                to calculate Association measures 
                            Dice’s coefficient Dice’s coefficient (similarity)
                            &calcu -
                            实现stemming: **
                                Once such a similarity matrix is available
                                terms are clustered using a single link clustering method
                         Affix Removal Stemmers
                        Porter's Stemmer
                            idea 
                                use a rule table to remove suffix in words
                            explain & -
                            原理很简单，其实就是一个 rule table **
                            Most used in IR, probably because of its
                                balance between simplicity and accuracy
                                (1980)
                            the defacto standard algorithm used for English stemming
                            Porter extended his work by developing Snowball, a
                                framework for writing stemming algorithms 
                                http://snowball.tartarus.org
                            Rules are composed of a pattern (left) and a string (right side)
                            
                            alternate vowel-consonant sequences
                            5 phases of reduction rules applied sequentially **
                                Step 1 deals with plurals and past participles
                                Steps 2 to 5 deal with English-specific suffixes
                                简单计算
                                    需要知道CVCV公式
                                    需要知道m数值进行判断
                                    见例题 & -
                    Lemmatization
                        to base form
                            am, are, is → be
                        Inflectional morphology vs. derivational morphology 曲折形态学vs派生形态学
                        为啥为检索带来的好处非常有限 & - NEXT
                        vs Stemmer:: 
                            Stm just cutting affix.
                            Lemmatization adds some alphabets.
                    Stemming?or not &
                        一般来说，词干化会提高某些查询的效率，而降低其他查询的效率
                        Benefits of stemming depend on the language 屈折语更有好处
                        Danger’s of stemming：：[information retrieval] vs. [information on Golden Retrievers] 错误得处理了一些情况
                            increses the probability of false positiv
                    Part-of-Speech Tagging &
                        Labeling each word 
                        Different approaches
                             Rule-based Tagger
                             Stochastic POS Tagger
                            - Simplest stochastic Tagger
                                Each word is assigned its most frequent tag 
                                Learning from examples : we can training it 
                                WSJ (Wall Street Journal) in the Penn Treebank around 1.2 Million tokens
                                Wall Street Journal corpus:p42 as a example 
                                    a conditional probability = 0.019
                                    给一个条件概率表，进行POS & -
                            - HMM Tagger
                    编辑距离
                        https://nlp.stanford.edu/IR-book/html/htmledition/img152.png
                        写出伪代码就都懂了 & -
                            动态规划保证最后是最优的
                            横向变动是del操作，纵向变动是insert操作
                            要会根据表格写具体操作**
                                一共四种操作
                            ppt4-p21
        ppt-4-Tolerant Retrieval  --over --c
            +BG 
                前面的工作无法支持etst 查询
                容错式检索也需要支持任意多字符匹配* &
                k-gram 索引结构如何支持通配符的检索
                词典的数据结构 &
                    可以采取前缀数 
                    为了加快对字典的检索速度
                    https://blog.csdn.net/lsjseu/article/details/12234769
                    示意图
                用户有时候对单词的不确定性，可能会输入一些通配符操作
                    采用B树
                        字符串前缀搜索一直是B树的优点，而字符串后缀搜索，
                        我们可以采用反向B树，那么对于这种ab*cd，那么我就可以通过B树和反向B树的结合，最后取一个交集
                    轮排索引解决方案
                        对一个单词建立轮排索引。假如有hello，我们建立下列轮排单词表，考虑各种情况
                        轮排索引的缺点很显然：词典集合太大。
                    kgram
                        举个例子，对于一个单词castle，它的3-gram包括：$ca、cas、ast、stl和tl$。（在开始和结尾处添加$）
                        在k-gram中，其词典由词汇表中所有单词的k-gram形式组成。最后与操作一下，注意下图引入了一个后过滤的过程（剔除一些不满足条件的单词吧）。
                        k-gram索引可能会返回很多伪正例，需要做过滤处理。
                    感觉最好采用b树
                用户可能存在错误输入，这时，我们需要对用户的输入给出一些校正的提示方案
                    edit distance ::
                        idea of it :
                            measure the steps converting from one word to an other.
                            -based on inserting or deleting methods 
                    https://blog.csdn.net/lsjseu/article/details/12234769
                    基于编辑距离的校正
                        运用DP &
                        在键盘上把“a”敲成“s”的可能性大于，把“a”敲成“u”，因为a和s靠得很近，
                            这样我们就可以在求字符串编辑距离的时候，给定不同的权重。
                        因为我们要同字典的每个单词进行求编辑距离。这是我们
                            就可以通过给定一些额外的条件来减少匹配单词的个数，比如我们假设首字母肯定要对。
                    k-gram的校正方法
                    上下文敏感的拼写技术
                    基于发音技术的校正
                        基于发音的校正技术（Soundex算法）

            ---------------------------------------------------------------------------------------
            Dictionaries
                数据结构？ & -
                    Some IR systems use hashes, some use trees 
                    两者如何选用？
                    will it keep growing。。。
                hash:
                    Each vocabulary term is hashed into an integer
                    Lookup in a hash is faster than lookup in a tree
                    BUT:
                        No prefix, infix or suffix search 
                        no tolerant retrieval
                        Need to rehash everything periodically if vocabulary keeps growing
                tree:
                    B-trees mitigate the rebalancing problem
                    Search is slightly slower than in hashes: O(logM), only on balanced trees 
            通配符查询Wildcard queries
                1.using B-tree immedately ::intersect
                    三种情况 & -
                        Trailing wildcard query-----B-tree
                        Leading wildcard query-----inverse B-tree
                        middle ------intersect the two term sets 
                    +btree 
                        多路搜索树
                        海量数据搜索
                        磁盘，数据库
                    
                2.permuted index 轮排索引
                    原理 & -
                        Store each of these rotations in the dictionary, in a B-tree 
                        hello--增加4个term
                        add terms to dictionary 
                    where $ is a special symbol
                    Problem : quadruples the size of the dictionary
                        compared to a regular B-tree (empirical number)
                    Permuterm index doesn’t require post-filtering
                    [hel*o] look up & -   ****
                        [o$hel*]
                        具体查询要会用x表示 & -
                3.kgram 
                    More space-efficient than permuterm index
                    例子 & -
                        Query [mon*] can now be run as [$m AND mo AND on]  ****
                        do postings-filter
                        but also many “false positives” like “Moon” 
                        change the queries 
                    execute a large number of Boolean queries
                比较 & -
                    k-gram index should do postings filter
                    permuted index tasks huge anount of spaces to store dic.
            Spelling Correction
                26%: Web queries (Wang et al. 2003)
                Correcting documents and queries
                The general philosophy in IR is: do not change the documents
                we use the the smallest distance to the misspelled word
                using :A standard dictionary (Webster’s, OED etc.)
                basic operations that convert s1 to s2
                based on Distance 
                    Edit distance 
                        The edit distance between string s1 and
                            string s2 is the minimum number of basic
                            operations that convert s1 to s2
                        +Damerau-Levenshtein 
                            includes transposition as a fourth possible operation
                        Computation & -
                        Algorithm & -
                    Weighted edit distance
                        Typewriter distance：基于键盘
                        Confusion Matrix for Spelling Errors p31 
                        Key Idea: we seek the string(s) in S of least edit distance from q
                    Problem:
                        Computing the edit distance from q to each string in S is inordinately expensive
                Invoke the k-gram index to assist --with low edit distance to the query
                    基于kgram的拼写检查 & -
                    1.将query拆分kgram 
                    2.根据kgram阈值做交叉 ： only vocabulary terms that differ by at most 3 k-grams p33
                    3.then, Jaccard coefficient 
                        Declare a match if 𝐽𝑎𝑐𝑐𝑎𝑟𝑑 𝑞, 𝑡 > 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑
                Context-sensitive spelling correction
                    hit-based spelling correction--基于以往查询
                        The correct query “flew from munich” has the most hits
                        BUT： we have to test 7 × 20 × 3 different variants -- too many 
                        SOLVE: we can log queries
                Issues when using Spelling correction 
                    & - 
                    Spelling correction is potentially expensive
                    Avoid running on every query?
                        Maybe just on queries that match few documents
                how to write 
                    & - NEXT
                    Spelling corrector in 21 lines Python code
                    Peter Norvig: How to write a spelling corrector (http://norvig.com/spell-correct.html)
            Soundex
                key idea of Sondex & 
                    for each word , we can calcu. a Sondex code ,
                        and we can use this code to check the similarity of their pronunciation
                Especially applicable to searches on the names of people **
                Soundex algorithm 
                    转换成四位代码
                    计算 & -
                    words with same pronunciation will generate the same code 
                    原理 
                        Vowels are viewed as interchangeable **
                        Consonants with similar sounds (e.g., D and T) are put in equivalence classes
                        ：发音类似的辅音放在一个等价类中
                        Better alternatives for phonetic matching in IR
        ----!!constructing!!
        ppt-5-Scoring, Term Weighting --over --c
            +btree:是一种自平衡的树，能够保持数据有序
                B树，概括来说是一个一般化的二叉查找树（binary search tree）一个节点可以拥有最少2个子节点
                B树适用于读写相对大的数据块的存储系统，例如磁盘。B树减少定位记录时所经历的中间过程，从而加快存取速度。
                B树这种数据结构可以用来描述外部存储。这种数据结构常被应用在数据库和文件系统的实现上
            
            compare: & -
                    Boolean--
                        too many results::Most users don’t want to wade through 1000s of results
                            Feast or famine
                            Not good for the majority of users
                            it’s too much work
                        skills：：a lot of skill to come up with a query 
                    Why rank &ranked--
                        With ranking, large result sets are not an issue
                            Doesn’t overwhelm the user
                        doesnt need too many skills 
                            more relevant results are ranked higher than less relevant results
            the most basic approach to scoring -- Jaccard coefficient to Scoring
                交集处以并集合（个数）
                calcu  & -
                    one query : [ides of March]
                    and two docs
                problems: & -
                    Normalization with 根号
                    it Doesn’t consider term freq
                    it Doesn’t consider how rare items
            verbesserung : tf-idf weighting how & as vectors
                    what is tf idf &
                    basic calculations  
                        BOW:Bag of Words : do not consider order 
                        tf: number that t in doc d 
                            count vector
                            formular & -
                                p19
                            sum over terms to Score for a document-query pair
                            The score is 0 if none of the query terms found in d
                            challenge & -
                                Relevance does not increase proportionally with term frequency
                            ->should be promoted with log function 
                            we can score it only using the tf .
                        idf : number of docs that t occurs in 
                            where comes from the idea ? 
                                explain & - p21
                            ->should be used with log func 
                            to “dampen” the effect of idf
                            calcu p27 & -
                            idf affects the ranking of documents for queries with at least two terms
                            idf has no effect on ranking for one-term queries
                            why not collection frequency ? & -
                                Which word is a better search term (and should get a higher weight)?
                                that means , this term should appear in some docs with more freq and less freq in others 
                                so we can combine the collection freq at the same time to get a better score for queries  
                        we use the log10 transformation for both term frequency and document frequency
                        tf-idf:Best known weighting schema in information retrieval
                            Combines local (term frequency) and global (inverse document frequency) weights with product 
                                of its tf weight and its idf weight
                            character 
                                 Increases with the number of occurrences within a document (term frequency)
                                 Increases with the rarity of the term in the collection (inverse document frequency)
                                 Is 0 if tf = 0 or df = N
                        calculate scores 
                            Assign a tf-idf weight for each term t in each document d
                            vectors are generated
                            calcu & - P28
                        tf-idf compare &
                            idf is a global weight whereas tf is a local weight
                    The vector space model
                        at first : 0/1 
                        then: 次数 count 
                        now : tf-idf weights 
                            document vectors with weights

                        it was sparse too ,most entries are zero
                        we should rank relevant documents higher than non-relevant documents
                        Vector space similarity
                            why Euclidean distance is a bad idea & -
                                because Euclidean distance is large for vectors of different lengths
                                Use angle instead of distance
                                距离近但是不相似，看角度更合理
                            should be calculated with angle cos 
                            normalization
                            Cosine similarity : equivalently, the cosine of the angle between
                            after calculations , we can get Cosine Metric
                            calcu & - 从p41开始计算
                    Practical implemention
                        algo p46 & - NEXT
                    Variant tf-idf functions
                        tf 
                            augmented
                        df 
                            prob idf
                        Normalization
                            pivoted unique
                            byte size
                    tf-idf example: Inc.Itn 
                        use different weightings for queries and documents & -
                        计算题中使用ltn.bnn计算 
                        NEXT
                            分别是：： term document normalization
                            term::log boolean(b)
                            doc:: idf(t) none(n)
                            normal:: cosine(c) none(n) 
        ppt-6-index Compression --over --c
            +corpus 
                unary code -
                    key steps : calculate offset and length and combine them as one code 
                    unary code can be applied to any distribution 
                ϒ codes always has odd length
                Gamma code is universal


            ---
            effizient Dokumente findet?
            compress the postings component
            Hardware    
                Disk I/O is block-based
                Block sizes: 8 KB to 256 KB
            WHY ? & -
                Use less disk space  saves money
                Keep more stuff in memory - increases speed
                    Decompression algorithms should be fast
            Lossy vs. lossless compression & -
                Discard some information in Losy compression
                All information is preserved in lossless
                    we mostly do in index compression
                    index--- dictionary + posting lists
            Term statistics
                Heaps’ law
                    Estimates vocabulary size M as a function of collection size number:
                    M is the size of the vocabulary, T is the number of tokens in the collection
                    Typical values for the parameters k and b are: 30 ≤ k ≤ 100 and b ≈ 0.5
                    Heaps’ law is linear in log-log space (line with a slop of about 0.5)
                    it was a Empirical law
                    formular & -
                    Tokens can repeat 
                    For Reuters : das functioniert
                    fit is good in general
                    including numbers and spelling errors affect the result 
                    it means & -
                        no maximum vocabulary size reached    
                        the size of the dictionary is quite large for large collections
                Zipf’s law
                    formular & -
                    it means & -
                    In natural language, there are a few very frequent terms and many very rare terms
                    formular 
            Dictionary compression
                at first :for Reuters: (20+4+4) × 400,000 = 11.2 MB
                resolve Fixed-width bad 
                    explain & -
                        Most of the bytes in the term column are wasted
                        We can’t handle larger strings
                        Average length of a term in English: 8 characters 
                    2.Dictionary as a string & -
                        画图 **
                        need to know how to caklcu 3 here 
                        400,000 × (4 + 4 + 3 + 8) = 7.6 MB 
                    3.Dictionary as a string with blocking & -
                        We eliminate k − 1 term pointers
                        We save 4 × 3 − (3 + 4 × 1) = 5 bytes per block
                        Total savings: 400,000/4 · 5 = 0.5 MB
                        from 7.6 MB to 7.1 MB
                        block不可以太大的原因 & -
                            Slightly slower
                            we should Tradeoff between compression and the speed of term lookup
                    4.Front coding & -
                        A sequence of terms with identical prefix (e.g. “automat”) 
                        Consecutive entries: common prefixes
                        the first byte of each entry encodes the number of characters
                        with blocking and front coding: 5.9MB
                        写出示意串串 & -
                            注意两个符号不同！
                            被忘了数字！
            Postings compression
                The postings file is much larger than the dictionary
                For Reuters (800,000 documents), we would use 32 bits per docID when using 4-byte integers
                Alternatively, we can use log2 800,000 ≈ 19.6 < 20 bits per docID
                goal : less than 20 bits per docID 
                基本思路：using gaps 
                    Postings for frequent terms are close together
                    Gaps between postings are short
                    Postings list using gaps: COMPUTER - 283154, 5, 43, …
                Variable length encoding 
                    we need a variable encoding method that uses fewer bits for short gaps
                    Two solutions
                         Bytewise compression - Variable length encoding 
                            Dedicate 1 bit (high bit) to be a continuation bit c
                            At the end c=1 others c=0
                            Postings are stored as a byte concatenation see P34
                            still : For a small gap (e.g. 5) VB use a whole byte this can be promoted later 
                            the algorithm & - NEXT
                            exmaple p34 & -
                            do not sensitiv to computer memory alignment matches 
                         Bitwise compression - Elias gamma encoding 
                            Gamma Codes for gap encoding
                            Unary code ：Represent n as n 1s with a final 0  & -
                                3= 1110
                            Gamma Codes 
                                calcu & -
                                    13? p39
                                    a pair of length and offset
                                    13 → 1101 → 101 = offset
                                    Encode length in unary code: 1110
                                    length = 1110 and offset = 101 --》 1110101
                                    Length of the entire code is 2 x ⌊log2 G⌋ + 1 bits
                                    ϒ codes are always of odd length
                                    steps :
                                        1.write as Binary code 
                                        2.calcu. offset 
                                        3.calcu. len of offset as unary
                                        4.combine them together 
                                some advantages & - explain -
                                    3.prefix-free
                                    2.optimal within a factor of 2
                                    1.universal
                                        independent of the distribution of gaps
                                        it is parameter-free
                                    
                                Gamma seldom used in practice & - explain
                                    1.it is not aligned and thus potentially less efficient
                                postings, gamma encoded 101MB
            p42 回顾
            less reality ? NEXT
        ppt-7-Index Construction --over --c
            +corpus 
                    -distributed index construct system
                how can we construct index efficiently ?
                Performance characteristics typical of systems in 2007
                to motivate IR system
                Access to data in memory is much faster than access to data on disk
                    -we can access the data faster when we keep it in mem. 
                takes a few clock cycles    
                    -when we want ot access data on disk , a few clock cycles can be taken  
                    -it takes a few clock cycles to access it 
                We call the technique of keeping frequently used disk data in main memory caching
                    -the cashing technique will be used in this expriment. 
                if it is stored as one chunk
                    -when the data are localted as the same chunk , ...
                Operating systems generally read and write entire blocks. Thus, reading 
                    a single byte from disk can take as much time as reading the entire block.
                    -it takes the same time to ...
                We call the part of main memory where a block being read or written is stored a buffer
                    -buffer is part of 


            建立倒排索引einen invertierten Index dafür erstellen
            索引构建算法的设计受硬件的配置所制约
            练习题目和这个不是很大关系，所以更要看ppt！！！！！
            Static Indexing
                操作系统往往以数据块为单位进行读写。因此，从磁盘读取一个字节和读取一个数据块
                    所耗费的时间可能一样多。数据块的大小通常为 8 KB、 16 KB、 32 KB 或 64 KB。我们
                    将内存中保存读写块的那块区域称为缓冲区（ buffer）
                Naïve Indexierung
                    Termvorkommen im Hauptspeicher halten und dort sortieren
                    Wie groß darf eine Dokumentensammlung sein,mit 8 GB Hauptspeicher
                        calcu & - NEXT
                        134.216 Dokumente 13 wan< 80 wan
                        unter Zuhilfenahme von Sekundärspeicher
                        Sortieren mit Sekundärspeicher 
                            Zahl der Ebene ? & 
                    External Memory Sort &
                基于块的排序索引方法Blocked Sort-Based Indexing (BSBI)
                    idea & - NEXT
                        画图
                        1.分块读入
                        2.sortierte Teilfolgen von Termvorkommen,als
                            Ebene 0 von External Memory Sort in zweitem Durchlauf
                        3.最后一步：
                            内存中维护了为 10 个块准备的读缓冲区和一个为最终合并索引准备的写缓冲区
                            每次迭代中，利用优先级队列（即堆结构）或者类似的数据结构选择最小的未处理词项 ID (term id)进行处理
                            合并结果写回磁盘中
                            需要时，再次从文件中读入数据到每个读缓冲区。
                        algo & - NEXT
                    实际中的很多语料库远比这个语料库要大，就需要这种技术建立索引
                    问题：
                        但是需要一种将词项映射成其 ID 的数据结构。
                        中间文件很大 intermediate files
                内存式单遍扫描索引构建方法Single-Pass In-Memory Indexing (SPIMI)
                    一种更具扩展性的算法
                    posting list 动态增长的一种策略，在内存满了以后，排序并写入磁盘。后续合并与based on block are the same 
                    pseudocode & - NEXT
                    优势：
                        节省了id存储的开销
                        节省内存
                    为使倒排记录表按照词典顺序排序来加快最后的合并过程，要对词项进行排序操作（程序第 11 行）
                    由于事先并不知道每个词项的倒排记录表大小，算法一开始会分配一个较小的倒排记录表空间，每次当该空间放满的时候，就会申请加倍的空间
                    压缩对于这种是有效的
                    算法复杂度是线性的关于T：pairs that can be held into main memory
                    assign a small space at first . Then, dopple it when necessary.
                    Fill main memory as a block
                Caching
                    reduziert dadurch die Antwortzeiten eines IR-Systems
                    & -
                     Least Recently Used (LRU) schafft Platz für neues Element, indem es das am längsten unbenutzte
                        Element entfernt
                     Least Frequently Used (LFU) schafft Platz für neues Element, indem es das am seltensten benutzte
                        Element entfernt
                    Cache-Hit-Ratio： die aus dem Cache beantwortet werden können
                Verteilte IR-Systeme
                    Fargen:
                        auf mehreren Rechnern abzulegen
                        invertierten Index schneller aufzubauen
                    zwei Arten von Rechner‐Knoten :  & -
                        Master und Slaves 
                        Other structures :Grid
                    Mathods of Distributing ***
                        Term-Partitionierter:
                            jeder Rechner-Knoten speichert -- eine Teilmenge der Terme
                        Dokument-Partitionierter:
                            jeder Rechner-Knoten speichert -- Teilmenge der Dokumente
                        compare and see the advantages or dis- & - NEXT
                        Vorteil und Nachteil nennen 
                            Term:
                                NEXT
                            Document:
                                如果Slave宕机，几乎不受影响
                                和每个Slave都要交流
                    parallelisierbar? & - NEXT Grund dafur  explain
                        Ohne Probleme parallel auf verschiedenen Rechnern stattfinden？ 
                        Grund nennen -- P39
                    BUT: & - explain
                        Character of a distributed computer: 
                            cluster Can unpredictably slow down or fail
                    Master machine assigns each task to an idle machine from a pool
                    MapReduce programming model
                        users modify map and a reduce function
                        MapReduce Architecture & - NEXT
                        NEXT 
                        MapReduce是可以串联的
            动态索引构建方法
                How to keep the index up-to-date as the collection changes
                Drei Methode 
                    1.Naïver Ansatz
                        jedes neue anzupassen 
                        Problem:
                            Lange Zugriffszeit
                    2.Re-Indexierung
                        Regelmäßige vollständiger Neuaufbau 
                        Problem :
                            Es geht nur Für kleine oder wenig dynamische Dokumentensammlungen
                            Ineffizienz
                            hoche Speicherbedarf
                    3.Delta-Index
                        -r
                            zusätzlichen invertierten Index
                            in an additional inverted index store 
                            use this additional index to deal with queries 
                            put the delta index to disk when necessary 
                            or do it regularly

                        beschreibung:
                            in einem zusätzlichen invertierten Index im Hauptspeicher
                            Gelöschte Dokumente werden in einer Liste
                            Während der Anfragebearbeitung : Haupt- und Delta‐Index verschmolzen
                            put the delta index to Disk:
                                gesamt n Terme in einem Document und T ist die Lange des Indexes 
                            bis zu T/n mal miteinander verschmolzen?
                                Complxity &
                        Logarithmisches Verschmel
                            put the delta index to Disk: with a flexible length 2^m*n
                            NEXT
        ppt-8-QueryProcessing --over --c
            如何加速比较出Top-k的算法
            实际上的比较过程,利用累加器
            当时在图书馆看懂的
            Wie kann man effizient :
                wert ermitteln：可以利用累加器，优先队列加速这个过程
                die Top-k Dokumente bestimmen
            一行一行Term读取，但是这里未必要读到最后
            1.Term-at-a-Time (TAAT): die in Vorlesung 5 gegeben 
                一行一行Term读取，但是这里未必要读到最后
                不用排序
                必须全读完
                p8回顾计算
                calcu & -

            2.DAAT
                一次一个Document
                p10回顾计算
                calcu & -
            NRA ： No Random Accesses
                explain & -
                    NRA ist ein allgemeines Verfahren
                    frühzeitig beenden  and korrekten Top-k Dokumenten finden
                    monotonen Aggregationsfunktionen f单调聚合函数
                posting list需要排序降序
                最差：当前数值
                best(d)：当前数值+非当前doc的列的两个/n个数值之和 
                unseen = 当前列数值之和
                条件：
                    unseen ≤ mink: 未知的都不行了，现在已经出来topk了
                    best ≤ mink：当前doc不行了，不可能入选了
                p27计算 & -
                p29公式 & -
                我的网上照片例子 &计算 - NEXT
        ppt-9-Evaluation --over --c
            Unranked evaluation
                给定测试集：
                 User Happiness
                    However, the key measure for a search engine is user happiness
                    how to Measure the happiness ? & - p5  explain
                        individual users:
                            relevant quality
                            speed of response time ,
                            size of index, 
                            uncluttered UI

                        for Web search engine:
                        for Ecommerce:
                        ...
                 Precision and Recall
                    explain & - NEXT explain
                        正确率：结果中相关文档所占的比例，是不是有不相关的？
                        召回率：是所有相关都返回了吗？返回的相关文档占所有相关文档的比例
                    Web 检索用户希望第一页的所有的结果都是相关的，
                        也就是说他们非常关注高正确率，
                        而对是否返回所有的相关文档并没有太大的兴趣
                    相反地，一些专业的搜索人士（如律师助手、情报分析师等）
                        却往往重视高召回率，有
                        时甚至宁愿忍受极低的正确率也要获得高的召回率
                    对本机硬盘进行搜索的个人用户也常常关注召回率
                    公式 p9 & -
                    计算p10 & -
                 F-Measure
                    Precision/recall tradeoff
                    就是一个调和平均值，公式 & -
                    increase recall by returning more documents
                    A system that returns all documents has 100% recall!
                    It’s easy to get high precision for very low recall
                    use harmonic mean
                    一个融合了正确率和召回率的指标是 F 值（ F measure），它是正确率和召回率的调和平均值
                        默认情况下，正确率和召回率的权重相等
                        β < 1 表示强调正确率，而 β > 1 表示强调召回率
                        Values of β < 1 emphasize precision, while values
                            of β > 1 emphasize recall
                        为什么使用调和平均而不是其他简单的平均方法（如算术平均）来计算 F 值呢 & - NEXT
                            没有最大最小数值限制
                            调和平均值往往小于算术平均和几何平均值，并且常常与两个数的较小值更接近
                            a kind of smooth minimum
                        Example & - p13
                 Cost-based Measure
                    As an alternative to precision and recall
                    垃圾邮件的计算，加权的和
                    example & -
                 Accuracy
                    精确率：根据上述两个，以及表格：精确率指标在很多机器学习问题中的使用非常普遍
                            但是：不均衡性，比如通常情况下，超过 99.9%的文档都是不相关文档。
                            TP TN 占用的比例 公式 & -
                            Why is accuracy not a useful measure
                                Normally over 99.99% of the documents are in the non-relevant category
                                explain &
                            问题bug：How to build a 99.9999% accurate search 
                                engine on a low budget
                                 The Snoogle search engine 
                                below always returns 0 results (“0 matching results found”),
                                regardless of the query
                Macro average (precision) and Micro average (precision) 
                    p19 calcu & 公式有问题  
            Ranked evaluation
                 Precision-Recall-Curve
                    ROC Curve
                    ROC 曲线通常起于左下角而逐渐向右上角延伸。一个好的系统，曲线图的左部会比较陡峭
                    在很多领域，一个普遍使用的指标是计算 ROC 曲线下的面积
                    近年来，往往应用在基于机器学习的排序方法中（参考 15.4 节）的指标——
                    CG（ cumulative gain，累积增益）
                    first k elements in the result list
                    For each such sets (with size k), 
                        precision and recall values can be plotted
                    呈锯齿形的原因? & -
                        k+1篇文档不相关，召回率不变，正确率下降
                        如果返回的第(k+1)篇文档相关，那么正确率和召回率都会增大

                        in top-k hits ::　
                            assume that k+1 is not relevant .
                            from k to k+1 , recall will not be changed , but precision has decreased
                 11-point interpolated average precision
                    定义一个 11 点插值平均正确率，
                        用于浓缩信息 ***
                    transform this information down to a few numbers
                    Each point corresponds to a result for the top k ranked hits
                    遍历recall获得precision 而后取平均值 ***
                from multiple queries ？有的information need容易有的难
                    TREC中最常规的指标是MAP（ mean average precision，平均正确率均值）
                    MAP被证明具有非常好的区别性（ discrimination）和稳定性（ stability）
                    MAP可以粗略地认为是某个查询集合对应的多条正确率—召回率曲线下面积的平均值。
                    MAP: Average of several average precision values
                        Gold standard？？
                            可能就是估计有50%是相关的
                            也就是说这些判定构成所谓的“ 标准答案” 集合** 不一定多少，是题目条件
                        p28计算 & -
            Evaluation benchmarks
                to mwasure effectiveness
                 Standard relevance benchmarks
                    what is that actually & -
                        documents
                        information needs
                        Human relevance assessments
                    eg
                        TREC:1.89 million documents, mainly newswire articles, 
                            450 information needs
                        Five largest classes in the Reuters-21578
                评价过程
                    Pooling策略--评价所有文档工作量是很大的，可以只评价检索的
                    给定信息需求集及文档集，需要给出它们之间的相关性判定情况，这是一项需要人工参与
                    的费时费力的工作
                 relevance assessments需要一致性--Kappa statistics,计算judge是不是变了？
                    measure consistency .
                    Kappa 只是一个统计量，存在抽样误差。
                    统计学，一致性检验
                    http://sofasofa.io/forum_main_post.php?postid=1000321&
                    xiangqin 例子 
                    ppt上计算 & p37 - 给定两个判断，求Kappa统计
                    网页中那个计算 & - NEXT
                    Relevance assessments are only usable if they are consistent
                    As a rule of thumb： 0.8 0.67？
                    Marginal relevance？ 
                        最大边界相关法（Maximal Marginal Relevance） & - NEXT
                ASK Consumer
                Consumer (Data) Science
                    Ongoing studies of user behavior in the lab
                    利用用户信息进行黑盒测试
                     A/B Testing
                        two versions of the same product
                        其实就是两个版本的产品
                        例如奥巴马的竞选募捐网站。这个网站最核心的目标是：让网站的访客完成注册并募捐竞选资金
                        这个团队当时做了一个非常成功的实验：通过对6个不同风格的主页进行AB测试，
                            最优的版本将网站注册转化率提升了40.6%，
                            而这40.6%的新增用户直接带来了额外的5700万美金募捐资金
                        A/B测试带来的收益会远高于A/B测试的实施成本
                        “假设把注册流程中的图片校验码方式，改成短信校验码的方式，我们的注册转化率可能提升10%”。
                        基于这个假设，我们会设计对应的A/B测试
                        widget variations
                     Interjudge agreement
                     Query Logs
                        Logs can be used to tune / evaluate search engines
                        Aggregate clicks to reduce noise 
                        Click deviation &避免随机点击？
            Result summaries
                Presenting Results
                    as a list with discreption
                    what to descreption? & -
                        document title, url, some metadata
                    Two basic kinds result Summaries
                        static
                            NLP（ natural language processing，自然语言处理）领域中存在大量更好的文本摘要方法。
                            在更复杂的 NLP 方法中，系统可以通过自动全文生成
                                方式或者对原文档中句子进行编辑或组合的方法来自动生成摘要句子
                            basic method: a subset of the document
                            better: Emphasizing sentences with key terms
                            or: complex NLP to synthesize/generate a summary
                        dynamic 
                            通常这些窗口会包含一个或者多个查询词项
                            动态中有静态内容部分
                            keyword-in-context (KWIC) snippets
                            解决计算复杂问题：：预先进行磁盘缓存
                            生成动态摘要的目标是选出满足如下条件的片段 & - NEXT p49
                                (i) 在文档中最大限度地包括这些词项的信息；should contain several of the query terms as many as possible
                                (ii) 内容足够完整，方便用户阅读理解；as a phrase,should be complete sentences
                                (iii) 足够短，满足摘要在空间上的严格限制。short enough , beacause the space is limited 
                            Summary should answer the query, so we don’t 
                                have to look at the document
                            Detail?
                                cannot construct a dynamic summary from the positional inverted index 
                                超过固定前缀长度的文档在产生动态摘要时只基于文档前缀来实现
                                +NEXT
                            We need to cache documents
        -----!!improving!!信息检索中最重要的版块----文档评分，评分变换，针对不同的系统评分
        ppt-10-Relevance Feedback and Query Expansion 第 9 章 相关反馈及查询扩展 --over --c
            a quick review:
                用户对相关性给出反馈意见
                针对一义多词现象 **
            相关反馈--“local”(relevance feedback)
                通过查询的初始匹配文档对原始查询进行修改
                相关反馈
                    用户会对初次检索结果的相关性给出反馈意见
                    提交反馈后的检索结果，其正确率得到显著提高
                    Rocchio 算法是相关反馈实现中的一个经典算法
                    基于概率的相关反馈方法
                    Ad hoc retrieval自组织检索
                    four different examples with picture & -
                    后来排名向前了 **
                    document : term vector 
                     Rocchio Algorithm
                        based on the vector space model
                        key idea & -
                            find a new query vector : The optimal query vector  :
                            make the  similarity with relevant documents  bigger ! 
                            change the q vector !!?
                        origionally , the query vector does not separate 
                            relevant / non-relevant ***
                        formular & - p17 NEXT
                            Dr and Dnr: 
                                sets of known relevant and 
                                non-relevant documents respectively
                            Why parameter & -
                                If we have a lot of judged documents, 
                                we want a higher β/γ
                        有图来解释这个公式 & -：两个图 p29
                            假设: we find document besed on query 
                            vector and the docs returned 在 q 
                            为中心的圆中： 计算相似度的时候是欧氏距离？ 
                        it separates relevant and non-relevant documents p28
                     Positive versus negative feedback & -
                        Positive feedback is more valuable than negative feedback ***
                        Many systems (e.g. image search at the beginning) 
                        allow only positive feedback  is γ = 0
                        Most IR systems set γ < β
                     Assumptions / Evaluation / Problems
                        Assumptions 应该做哪些假设？ & - NEXT
                            sufficient knowledge to be able to make an initial query
                                can be various reasons why initial query may fail 
                                是什么原因在一开始的搜索中失败？ & -
                                    Misspellings
                                    ..
                                    ..p30
                            Term distribution in all non-relevant documents will be different
                            At least five judged documents can give stable results 
                        Evaluation 
                            one round of relevance feedback is Ok & -
                            we can Pick one of the evaluation measures from last lecture 
                            根据以往继续评价 & ! -
                            例如：
                                Compute p@10 for original query q0
                                Compute p@10 for modified relevance feedback query q1
                                然后再处理这两个数据，得到一个评价数据
                            判断题： & -
                                Fair evaluation must be on “residual” documents (not yet judged by user) ***
                                in most cases : q1 is spectacularly better than q0
                        problems，不起作用的原因  & - NEXT
                            “Excite web search engine”
                                :Initially provided full relevance feedback,However
                            no incentive to give feedback
                            users do not understand .
                                Web search users are only rarely concerned with increasing recall
                            this method creates long modified queries
                伪相关反馈
                    it assumes that topk documents are relevant .
                    can improve quaity without interaction with users  
                    it automates the “manual” part of true relevance feedback
                    用户不需要进行额外的交互就可以获得检索性能的提升。
                    假设排名靠前的 k 篇文档是相关的，最后在此假设上像以往一样进行相关反馈。
                    它不可能完全避免自动化操作所带来的风险
                        it can leads to  query drift & -
                    pseudo-relevance feedback is effective on average & -
                        use the notation : relevance feedback (PsRF)
                间接相关反馈 Indirect relevance feedback
                    -r     
                        implicit feedback from user data 
                            eg. location of users , time of the query ...

                    Less reliable than explicit feedback
                    users are often reluctant to provide explicit feedback 
                    understand it  & -
                        Web 搜索引擎一样的具有高访问量的系统中，收集用户的大量隐式反馈信息是十分容易的
                        如果用户浏览的次数越多，那么它的排名也越高。
                        这实际是点击流挖掘（ clickstream mining）这个通用领域的一种形式。
                        一个非常相关的方法用于与 Web 查询相匹配的广告排序
                    维护信息过滤器（如新闻过滤器）
            查询扩展(Query expansion)
                Web 搜索引擎会给出相关的推荐查询，然后用户可以选择其中的某个推荐查询进行搜索
                    "also try :"
                但是总的来说查询扩展不如相关反馈技术成功。当然，它的优点是更容易为用户所理解
                    企图解决"翻译转换” 的问题，即用户如何知道文档使用哪些词项
                types of it : & -
                    1.based on query log mining
                        example give 
                            example 1:
                                After issuing the query [herbs], 
                                users frequently search for [herbal remedies]
                            example 2:
                                Users searching for [flower pix] 
                                    frequently click on the URL photobucket.com/flower
                                Users searching for [flower clipart] 
                                    frequently click on the same URL
                    2.手工编辑
                        built up sets of synonymous 
                        Example for manual thesaurus: PubMed
                    3.基于统计学的结果，相关性矩阵
                        Automatically derived thesaurus (e.g., 
                            based on co-occurrence statistics)
                        idea & -
                        using term-document matrix A , 
                            we can generate co-occurrence statistics 
                        semantically related with t
                        Widely used in specialized search engines 
                            for science and engineering
                        we use global resource, i.e. a 
                            resource that is not query-dependent
                        构建同义词词典：use thesaurus
                        geneerate from 
                            Co-occurrence is more robust, grammatical relations are more accurate &
                            process：   
                                A is a term-document matrix 
                                C = AA^T
                                cij is the number of times two terms ti and tj co-occur
                                For each ti, pick terms with high values in C
                                with a larger number being better
                                calcu & - NEXT p48 
                                we can calculate the 
                                    Nearest neighbors ** & -
                                    with this method 
                Query expansion:it may be as good as pseudo-relevance feedback
        ppt-11-Probabilistic Information Retrieval 第 11 章 概率检索模型 --over --c
            file:///G:/i.Note-%E8%BF%87%E4%BA%94%E5%85%B3%E6%96%A9%E5%85%AD%E5%B0%86/2%E8%AF%BE%E4%B8%9A%E5%85%B3/TUD%E8%AF%BE%E7%A8%8B/WS1819%E7%AC%94%E8%AE%B0/%E8%87%AA%E5%AD%A6%E8%AF%BE%E4%BB%B6/pic/IR/ppt-11/Viewer.html
            一种监督的，基于概率的，返回排名的检索策略
                训练分类器
            probability ranking principle，概率排序原理
            文档和查询都表示为词项出现与否的布尔向量
            许多不同的文档可能都有相同的向量表示。
            独立性假设和实际情况很不相符，但在实际中常常却能给出令人满意的结果
            Schätzung der Wahrscheinlichkeit, dass ein Dokument d 
                für eine Anfrage q relevant ist
            Beispiel
                1.收集表格 周围是统计信息,根据用户给出的反馈，只要有反馈就可以
                2.Berechnung des Termgewichts
                    Gewichtungsfunktion：formular  & -
                3.计算 状态检索值 Retrievalstatuswert
                    log相加
                    计算 & -
            基础概率知识
                    事件的优势率（ odds）   **
                        它提供了一种反映概率如何变化的“ 放大器” （ multiplier）
                    无关性判断，计算条件概率 
                        ppt上的计算
                    ct 是查询词项的优势率比率（ odds ratio）的对数值。
                        相关和不相关，两个优势率的比值，最后对这个值取对数
                    如果词项在相关和不相关文档中的优势率相等， ct值为 0。
                    如果词项更可能出现在相关文档中，那么该值为正。 
                    ct 实际上给出的是模型中词项的权重，粒度比较小，查询文档的得分是RSVd，是ct的求和
                    得分高则相关性大，根据评分可以进行排序出k个项目
                    Beispiel--Basic Model 
                    Einfache Wahrscheinlichkeit P(A) 
                    Theorem von Bayes 
                        Umkehren von Schlussfolgerungen相反
                        priori-Wahrscheinlichkeit先验概率
                        Wahrscheinlichkeit für ein Ereignis B unter der Bedingung后验概率
                    Chance / Quote (Odds) statt Wahrscheinlichkeiten
                        Wenn P(A1) > P(A2), dann ist auch O(A1) > O(A2)
                    Unabhängige Ereignisse独立活动
                        zwei Würfel geworfen
                        Test auf Unabhängigkeit &calcu - p
            Binary Independence Retrieval Model (BIR) **
                二值独立模型BIR &
                Binary Independence Retrieval Model (BIR)
                a model to measure rank documents .give each document a weight ,value .
                 Theorie und Definitionen
                    RSVd：Retrievalstatuswert eines Dokuments
                    sim(dm,qk) = RSV相似度评分
                    Anfrage-Vektor und Dokument-Vektor
                    Wahrscheinlichkeit der Relevanz, wenn eine Anfrage q und ein Dokument d gegeben sind?
                 Retrievalstatuswert eines Dokuments (RSV)
                    推导！！！ 要考？？ & - 大概其就可以 NEXT 以后细化
                        互补概率的转换Komplementäre Umformung der Wahrscheinlichkeit
                        NEXT
                        Anwendung eines Logarithmus
                            Grund? 
                 Termgewichtungsfunktion
                    r : Wahrscheinlichkeit, dass der Term ti 
                        in einem für die Anfrage q relevanten Dokument d vorkommt
                    n : Wahrscheinlichkeit, dass der Term ti 
                        in einem für die Anfrage q nicht relevanten Dokument d vorkommt
                    其实就是加和而已，后面才有参数修正
                    +
                        应用：  
                            Interaktives Relevance Feedback交互相关反馈
                            自动处理
                        以R 非R的分布为基础auf der Basis der Verteilung 
                            in relevanten und nichtrelevanten Dokumenten
                 Probabilistisches Relevance Feedback
                    可以通过（伪）相关反馈技术，不断迭代估计过程来获得 pt的更精确的估计结果
                    这个过程叫参数估计：Parameterschätzung durch Relevance Feedback (cont’d)
                    wenn es positiv ist ,in relevanten Dokumenten größer ist als in nicht relevanten Dokumenten
                    实际中往往要对上述估计进行平滑，此时可以对包含和不包含词项的文档数目都加上0.5 --Parameterkorrektur
                    也可以基于伪相关反馈的方法来实现上述算法 Rekursive Parameterschätzung
                        用户对某个文档子集 V 的相关性判断
                        V 可以划分成两个子集： VR and VNR 是否相关
                        词项 t 出现在相关文档的比例
                        NEXT 迭代过程实例算法 & - NEXT p47 书中p158
                            Anfangsschätzung ？ 
                        Erzeugung neuer Termgewichte
                        Korrekturwerte
                    这里对log是数值求和，而不是求积，tf-idf是求机
            Okapi 
                +
                    用来对匹配文档进行排序的函数
                    BM 是 Best Matching (最佳匹配) 的缩写
                    在概率搜索的框架下被提出的。Okapi 是第一个使用这种方法的信息获取系统的名称
                    文档评分纵向比较： & -
                        1.tf-idf 
                        2.基于BIR二值概率模型
                            没有考虑文档长度
                        3.基于Okapi
                            基于词项频率、文档长度等因子来建立概率模型的一种方法
                            Termgewichtungsfunktionen不同，引入更多参数
                    Okapi是tf idf以及一些参数的组合而已
                    如果搜索词中包含比较独特的词，则会提升分数；搜索词在一个文档中出现的次数越高，分数也会更高；但文档越长，分数会越低。
                    BM25较新升级版
                        BM25F，把文档结构和锚文本也考虑进来。另一个叫 
                        BM25+，只是在上面公式中的方括号里加了一个 δ，用来弥补原来公式对超长文档的不公。
                        Erweiterung der BM25 Gewichtung für den Fall dass sehr lange Abfragen auftreten
                    另外的模型：
                        ElasticSearch/Lucene 的分数计算
                            ElasticSearch 底层采用了 Lucene，
                                而 Lucene 的分数计算综合了布尔模型
                                (Boolean model), TF-IDF, 以及矢量空间模型。
                    BM25是一种BOW（bag-of-words）模型
                    BM25算法首先由Okapi系统实现（Okapi是伦敦城市大学实现的信息检索系统），所以又称为Okapi BM25
                    在计算IDF时，如果被查询的词语不在语料库中 & -
                        就会导致分母为零，所以通常会加上一个较小的数以保证分布不为零
                    是对tf-idf的升级：Okapi BM25 是到目前为止被认为最先进的排名算法之一
                    饱和度：
                        既然两个两个文档都是大篇幅讨论棒球的，那么“棒球”这个词出现 40 
                        次还是 80 次都是一样的。事实上，30 次就该封顶啦！
                BM11, BM15, BM25都属于BM25算法家族
                Erweiterung der BM25 Gewichtung für den Fall dass sehr lange Abfragen auftreten
                公式*2 & - NEXT
                总结比较 
                    在哪里使用？ & - NEXT explain 
                        Suche ähnlicher Dokumente
                        ohne besondere Verfahren multilingual
                    Vgl. der klassischen IR-Modelle & - NEXT explain
                    Besides the big error in estimating the
                        probabilities the classification is still
                        correct
                    概率IR优缺点 & - NEXT explain
                        vor 
                            it can be very quick 
                            good theoretical background 
                        nach 
                            Need to guess the initial ranking
                                user have to mark some of docs as relevant or not
                            the term frequency will be ignored
                            //Independence assumption
        ppt-12-Language Models for IR 第 12 章 基于语言建模的信息检索模型  --over --c
            +lan corpus
                a double circle indicates a (possible) finishing state.
                    here can we find a double circle . it means the state is finished  
                idea: 
                    for each doc. we generate a LM
                    then ,we can judge which doc is likely to generate the query 
                    it is a method to measure the relevance of a document d and a query ...
                A language model is constructed -for each document -in the collection
                some of the strings in the language it generates    
                    these strings can be gen. by the LM here 
                After generating each word, we decide whether to stop or to loop around
                a mathematical model of computation
                The FSM can change from one state to another in response to some external inputs
                    with some input data , state can be changed from to an other
                    the number of stats has a limit
                    external input can cause transition
                initial state, and the conditions for each transition
                A combination lock is a type of locking device
                This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows)
                An automaton is a finite representation of a formal language that may be an infinite set
                Automata play a major role in theory of computation, compiler construction, artificial intelligence, parsing and formal verification. 
                to calcu. , we Multiply the probabilities together. 
                MATH
                    dividing is the opposite of multiplying
                    Numerator and denominator!!!
                    decimals, percentages and fractions. 
                    +https://www.cs.cmu.edu/~venkatg/teaching/codingtheory/




            +    
                idea & -
                按照模型生成查询的概率来进行排名
                对每个文章都生成一个语言模型 & - exp -
                    ::
                        we gener. a Lan. Model for each doc.
                        the LM can be used to calcu. the relevant between doc and query 
                        then , we can rank the docs 
                有穷自动机
                    语言：所有可能的字符串的全集 & - exp
                        ::the lan. means all possible strings that LM can accept 
                    双圈节点对应的是（可能的）终止状态
                语言模型： 
                        每个节点都有一个生成不同词项的概率分布
                        还需要一个在终止状态停止的概率
                        一元语言模型的单状态有穷自动机 书中 see p164
                        一个具体的字符串或文档的概率往往非常小
                        停止概率：不会影响文档的排序
                        一元语言模型 
                            计算条件概率时不考虑前一个词项的出现情况
                            计算例子 & - p165 例12-2
                            例题：
                                但是通常在概率应用中
                                    实际上往往采用对数求和的计算方法
                                    因为计算机精度问题
                                两个一元语言模型的部分概率赋值
                            大部分工作都只使用了一元模型
                            在一元语言模型中，词出现的先后次序无关紧要
                        二元语言模型 
                            计算条件概率时只考虑前一个词项的出现情况
                        基于文法的语言模型 &
                            对于诸如语音识别、拼写校对、机器翻译等需要
                                根据上下文来求词项条件概率的应用非常重要
                            NEXT
                        +二项式分布，多项式分布
                            二项式：每个事件取值是2，组成一个序列的概率
                            多项式：每个事件取值是1/N，组成一个序列的概率
                            二项式概率和二巷展开式系数关系 &
                Einsatz im IR--查询似然模型
                    考虑查询？
                        我们对文档集中的每篇文档 d 构建其对应的语言模型 Md
                        我们的目标是将文档按照其与查询相关的似然 P(d|q) 排序。
                        使用贝叶斯公式， 只需要对P(q|d)排序 ，它是在文档 d 对应的语言模型下生成 q 的概率
                        具体怎么查，概率乘积 & - exp    
                            ::
                                we multiply the probabilities together to estimate the result
                                it is usually approximated by considering each term from the retrieved document 
                                the model is unknown 
                            每篇文档在估计中都是一门独立的“ 语言”
                            一个具体的字符串或文档的概率往往非常小 
                    +最大似然估计   
                        正态分布，取得一些点，反过来估计参数，估计函数的状况 
                        不一定存在，也不一定唯一。
                    在 IR 环境下，还有其他使用 LM 的思路
                    零概率问题
                        由于很多词在训练数据上存在稀疏性，所以在预测下一个词时可能会遇到零概率问题
                        使用平滑
                            简单平滑，归一化
                            引入参照概率分布函数
            -------
                Einführung
                    Statistische Sprachmodelle
                        本质上是啥 & - exp
                            ::essentially a method to rank docu.
                        用于总结这文章是什么个规律生成的
                        ：beschreibt die Erzeugung von Texten

                        应用
                             Erkennung gesprochener Sprache
                             Part-of-Speech-Tagger
                             Digitalisierung handschriftlicher Texte
                    Funktionsweise
                        描述有限自动机 EA & - explain
                            Doppelkreis kennzeichnet ？
                                den (möglichen) Endzustand desEA (=Ende der Zeichenkette)
                            箭头代表啥...
                            ::
                                stands for transformation(transition) 
                                double circle means finishing state
                        计算 P(Katzen fangen Mäuse) & - 
                        Mehrere Modelle M1, M2, ..., M 判断计算 & -
                            大O几率？
                            ppt上有点问题
                        unigram language model
                            P(Hunde, fangen, Katzen) = P(Katzen, Hund, fangen) 
                        二元语言模型为啥不用 & - exp
                            Zu speziell, zu aufwändig
                            使用bedingte Wahrscheinlichkeiten
                            ::
                                it uses Conditional probability , complex/difficult to compute 

                    Modelle / Markow-Ketten
                        使用马尔科夫链
                            一个马尔科夫过程是状态间的转移仅依赖于前n个状态的过程。这个过程被称之为n阶马尔科夫模型
                            kann man LM durch Markow-Ketten darstellen & -
                    Horizont（向前看的视野）
                        durch Analyse n- vorangegangener Wörter
                        视野 n
                            - n = 2 : Trigramm LM
                            - n = 1 : Bigramm LM
                            - n = 0 : Unigramm LM
                Einsatz im IR
                    Ranking mit absteigender Wahrscheinlichkeit 
                    Aber：Anfrage q zu kurz für LM
                    Vereinfachungen, Annahmen
                        fest für Anfrage q (kann somit weggelassen werden) 
                        :对每个q查询，是固定的，因此可以去掉
                        Für jedes Dokument d ein Sprachmodelle Md ableiten
                        ：每个形成一个语言模型 & -
                        P（d|q）Wird zum Zeitpunkt der Anfrage berechnet
                        BUT:Zu wenig Daten für komplexes Language Models
                            losung:使用一元模型
                        Multinomialverteilung 
                        ：多项分布（用于计算一元模型概率）
                    Schätzen von Wahrscheinlichkeiten
                        Schätzer über relative Häufigkeit 
                            p20公式 & -
                            其实就是用tf/d代替自动机中的概率
                    Smoothing
                        um Nullwerte zu verhindern
                        1.使用Gesamtkorpus 
                            idea & -
                        2.Jelinek-Mercer Smoothing 
                            idea & -
                                Linearkombination 
                                ：局部的有时候是0 所以引入全局的做平滑
                            p23公式 & -
                                参数大小的影响
                                Hoher Wert: kaum / kein Smoothing
                                Niedriger Wert: fast nur Korpuswerte
                                这里的零阶指的是全局概率 
                                一个document本来是相关的，但是因为查询长，查询中
                                    很多在文章中有可能并没有出现。
                                    所以说平滑，对于长的查询是有利的 **
                                    Viele Anfrageterme: stärkeres Smoothing
                                    Wenige Anfrageterme: schwaches Smoothing
                            输入为Tasse Kanne的那个计算 & - NEXT
                                原来的表格，横向加起来是1，但是有很多0
                            Alternative Vorgehensweisen？还有啥替代的不
                                & NEXT
                        +各种平滑方法
                            加法平滑
                                最朴素的思想就是对所有事件的出现次数加1
                            古德-图灵（Good-Turing）估计
                            Katz平滑
                            Jelinek-Mercer平滑
                                拿bi-gram举例，如果一个bi-gram没有出现过，
                                一个朴素的思想是退一步，看一下uni-gram的概率，
                                然后大概推测一下bi-gram的概率
                            贝叶斯平滑
                    实际应用
                        Multiplikation kleiner Werte Führt zu Rechenungenauigkeiten & -
                            Summe statt Produkt
                            Logarithmieren des Wertes rechnen
                            :using log function to calcu. **
                        Term-at-a-time
                            一行一行索引表进行处理
                            Akkumulatoren für die Dokumente
                            每个doc一个累加器 
                            NEXT
                Zusammenfassung 
                    LMs vs. BIR
                        联系 & - NEXT
                            term are independent
                        区别 & - NEXT
                            each document forms a class vs - only two classes relevant or not 
                            the smoothing method is different 
                    LMs vs. Vector-Space-Modell
                        联系 & - NEXT
                            considering term frequency 
                            //also normalized ,and has no relation to the length of the document 
                            the smoothing of it has something to do with idf 
                        区别 & - NEXT
                            vector model:: based on geometric similarity vs .. based on probability model 
                            //normalized in different ways 
                    Language-Modeling-Ansatz besser als Vector-Space-Modell
                    zwei Alternative Vorgehensweisen
                    Jedes Dokument bildet eine eigene Klasse (LM) 
                    vs. Klassen (relevant und nicht relevant) 
                    Jedes Dokument bildet eine eigene Klasse (LM) VS. die durch Menschen definiert werden (BIR) 
                    Jelinek-Mercer Smoothing vs. Addition kleiner Werte :: Jelinek-Mercer Smoothing vs. Addition kleiner Werte
        ppt-13-Web Search Basics 第 19 章 Web 搜索基础 --over --c
            关于web一些基本的东西
            web包括很多很多方面，每个方面都有很多可以说 & -
            为什么纷繁杂乱、变化迅速的 Web不同
                无法集中控制的无中心的网页内容发布机制
                ::
                    there is no central control within the web 
            web IR特点 & - exp

                    very large
                    lots of duplicates
                    lots of spam 


                    网页民主化：造成
                        网页中存在大量语法和风格上的巨大差异 
                    Web 中包含真理、谎言、矛盾和大量猜测。 
                    ::
                        1.different pages have different styles and maybe diff. gramma
                            write in diff. ways 
                        2.There are truths and lies on the Internet 
                我们应该相信哪些 Web 网页？
                对某个用户可信的网页内容不一定对其他用户可信
            索引规模估计
                某个搜索引擎中索引的网页数目是多少？
                所谓静态网页（ static web page），指的是那些内容不会因请求不同而不同的网页
                一个动态网页生成的例子。这种页面的一个标志是URL 中通常包含字符“ ?” 。
                动态网页有数据请求的过程
                浏览器发送有关 AA129 次航班的请求给 Web 应用服务器，服务器从后端数据库
                    中获取信息并且生成一个动态网页返回给浏览器
                静态页面与动态  & - exp  
                    动态页面（ dynamic page）
                        通常是由应用服务器应答数据库的查询需求时产生的 
                    静态网页（static）
                        没用post get方法的就是
                    ::
                        need to query the database 
                        load the web content without post or get method 
            web图
                画一个简单的 & -
                    静态 HTML 网页通过超链接互相连接而成的有向图
                    每个顶点代表一个网页，Ａ网页上有一个超链接指向Ｂ 
                该有向图可能不是一个强连通（ strongly connected）图，也就说，从一个网页出发，沿着超链接前进，有可能永远不会到达另外某个网页。 &
                在一系列研究中得到的网页的平均入度大概从 8 到 15 左右不等
                该图不是强连通图，因为 B 不可能到 A
                有大量研究表明这个分布满足幂分布定律（ power law），具有入度为i的网页总数目正比于 1/iα，研究中一个有代表性的α值是 2.1
                整个Web有向图结构？ & -
                    是个蝴蝶结(bowtie)形 
                    分别是IN、 OUT和SCC和管道
                    还有一些不能从 IN 到达或者不能到达 OUT的网页构成的所谓卷须（ tendril , tubes , disconnect pages）
            作弊问题： 
                几种方法？ & -
                    将重复的词设置成和背景一样的颜色
                    根据不同的访问请求（web采集器和用户），作弊 Web 服务器会返回不同的网页结果

                    ::
                        set the color of the words the same as background 
                        use different ways to answer IR systems and Users 
                操作网页内容来达到在某些关键词的搜索结果中排名较高的目的
                一些老练的作弊者还会采用一些手段和技巧，比如？ 
                    将重复的词设置成和背景一样的颜色
                很多网页内容的建设者都有商业动机，因此他们希望通过操作搜索引擎的结果来获益
                在很多 Web 搜索引擎中，有可能可以通过付费来将自己的网页放入到搜索引擎的索引中，
                    这个模型称为付费收录（ paid inclusion）。
                对于是否允许付费收录、付费是否会影响搜索引擎的排名结果，不同的搜索引擎会有不同的政策
                根据不同的访问请求（web采集器和用户），作弊 Web 服务器会返回不同的网页结果 
                更复杂的作弊技术还包括操纵与网页相关的元数据及指向网页的链接等
                他们之间的斗争将永不停止
                研究领域里也出现了一个被称为对抗式信息检索（ adversarial information retrieval）的子领域 **
                最早大规模使用链接分析方法的搜索引擎仍然是 Google：防止作弊
            广告经济模型
                Web 的交互性使得 CPC付费模型成为可能，用户的点击可以被网站记录和监控，然后根据记录的情况就可以给广告商寄去账单。
                对于用户的查询，搜索引擎会将“ 纯” 搜索结果（通常也被称
                    为基于算法的搜索结果）作为主要结果返回给用户，同时赞助搜索结果在算法结果的右侧独立
                    并有区别性地显示出来。
                飞机广告的缺乏也反映了几乎没有人会在 Web 上出售 A320 飞机这个事实
                一门被称为 SEM（ Search Engine Marketing，搜索引擎营销
                比如，一个心术不正的广告商可能会试图通过重复点击（使用机器点击生成器）其竞争者的赞助搜索广告来耗尽其广告预算
                但是破坏形式？ & -
                    其中一种被称为垃圾点击（ click spam）  

            用户体验
                一系列的研究结果都表明， Web 搜索中的平均查询关键词个数大概是 2 到 3 个
                google的用户体验特点： & -
                    关注相关性，特别是排名靠前的一些结果的正确率而不是召回率
                    用户体验要轻量级，也就是说查询页面和返回结果页面应该简洁整齐，
                    并且这些页面上基本没有图像成分，而应该几乎全是文本内容

                    lightweight 
                    no image on the result page , it is clean and pretty 
                    no too many ads on the page 
                普通的 Web 搜索查询似乎可以分成哪三大类？  & -
                    (i) 信息类（ informational）； 
                        用户输入信息类查询的目的往往是想从多个不同的网页中抽取信息
                    (ii) 导航类（ navigational）； 
                        这种情况下，用户最期望的结果就是汉莎航空公司的主页出现在搜索结果的第一个位置。
                    (iii) 事务类（ transactional）。
                        产品购买、文件下载或进行预订等。这种情况下，搜索引擎应该在结果中列举出
                        一些服务，它们能够提供上述事务处理的交互接口
                    分类并不容易，而类别信息不仅可以用于控制基于算法的搜索结果，也可以用于查询和赞助搜索结果的匹配当中
                    虚线下面的部分属于搜索引擎的内部结构。
            Web规模估计
                估计出某个搜索引擎的索引规模占整个Web的比例仍然是非常困难的，这是因为Web中存在无数的动态网页
                每次搜索过程当中不一定会调用所有的索引
                一个网站的深层页面可能会被索引，而在一般的
                    Web 搜索中并不会被返回。但是，如果用户限定在该网站内搜索（大多数搜索引擎都提供了限
                    定在某个网站进行搜索的功能）时，则该页面会被返回
                多类页面，因此无法采用单一指标来衡量索引规模
                一种方法 & - NEXT exp
                    利用关于E1 和E2 是Web中独立均匀随机抽样子集的假设进行引擎规模估计 
                    均匀随机选出一个网页用来进行测试是非常困难的
                    ::
                        count (More or less public) private pages
                        use the approch called “mark and recapture”
                            （used in ecology to estimate an animal population's size）
                        let f be the number of first crawl , s second and b both of them 
                        ...
            web抓取概述
                从Web中收集网页，采集的目标是尽可能高效地采集更多数目的有用页面
                同时获得连接这些页面的链接结构
                Web复杂性的根源在于其创建的无协调性
                Web采集器（ web crawler）
                功能
                    鲁棒性，采集器必须要能从这类陷阱中跳出来。当然，这些陷阱倒不一定都是恶意的，有时可能是网站设计疏忽所导致的结果
                    控制访问频率：设计采集器时必须要遵守这些代表礼貌性的访问策略
                    分布式：采集器应该可以在多机上分布式运行
                    可扩展性：在增加额外的机器和带宽的情况下，采集器的架构应该允许实现采集率的提高
                    新鲜度：在很多应用中，采集器都处于连续工作状态，抓取新的协议等
                实现
                    整个采集过程可以看成是 Web 图的遍历过程
                    在抓取高质量网页的同时，采集器要满足分布式、规模可扩展、高效、礼貌性、鲁棒性及功能可扩展等一系列要求。
                    Mercator 采集器
                        需要在每秒之内抓取几百个网页
                        五个模块 & 
                            DNS 解析模块--用于分析ip地址
                            *fatch模块--利用 http 协议返回某个 URL 对应的网页
                            URL 池--存放等待采集的URL
                                当某个 URL 加入到 URL 池时，它会被分配一个优先级，基于这个优先级来决定其最终出列真正进行采集的时机
                            分析（ parse）模块：从采集到的网页中抽取文本及链接
                            URL 去重模块：确定某个抽取出的链接是否已在 URL 池中或者最近是否已抓过
                        线程执行上述的流程图，其可以运行在单个进程中，或者分开到分布式系统的不同节点的多个进程当中
                        维持一个robots.txt文件的缓存 原因？ &
                            在真正抓取网页之前进行采集限制检测不管是这几天还是之后几天对其进行访问
                    分布式采集
                        如何划分？ 
                        1.地理位置划分--比如，地理位置在欧洲的采集器主要关注欧洲域名下网站的采集。当然，这种做法并不完全可靠，
                            原因包括： Internet 上数据包的传输路线并不一定反映地理位置的邻近性。 &
                            并且，在任何情况下，主机域名并不总是反映其实际的物理位置
                        2.使用主机划分器（ host splitter）将通过过滤检测的 URL 分配到不同的采集节点上去 &
                            重复检测模块会很复杂：
                            必须要基于指纹或 shingle 集合的某些性质对它们进行节点划分
                            文档变化--将文档的指纹或 shingle 连同 URL 本身一起放入 URL 池中
            web中的重复问题
                百分之40的网页是重复的：比如信息库的多份镜像
                solution1 :设计指纹比如64位
                    问题：可能只是最后两个字符不同
                solution 2:k shingling技术 &
                    搭叠检测，和ngram类似，见p362
                    shingling 算法：如果计算两个网页相互类似，就可以去掉其中一篇，不进行索引
                    算法有不同版本，追求效率的最大化，因为文章数目太多了
                    如何缩减计算？
            ---------------------------------------------------------------------------------------------------------
            IR on the web vs. IR in general 
                三点不同 & 
            Big Picture
            Size of the WEB
                what should be counted &
                    四个东西
                    Count them if they can be accessed by a large number of people
                How to find all Web pages? &
                    1.simply crawl the complete Web and count its number of pages
                        This doesn’t work due to the Web’s enormous size 
                    2.better approach : mark and recapture 
                        生态中种群估算
                        use two crawl,first and second 
                    In 2005, the Web has been estimated to contain at least 11.5 billion pages
                    now around 60 billion pages
            Web IR
                Differences from traditional IR &
                a bow tie 
                    wie oben 
                更多的信息需要 
                    wie oben 
                    Navigational and transactional
                    Difficult problem: How can the search engine tell what the user need or intent for a particular query is?
                    按照用户意图修改搜索结果
                        反对：Avoid the filter bubble： 文化思想泡泡
                网络搜索的具体形式特点 & 
                    Use short queries (average < 3)
                        Don’t want to spend a lot of time on composing a query
                    Only look at the first couple of results
                评价
                    Classic IR relevance
                    Trust, duplicate elimination, readability, loads fast, no popups
                    On the web, precision is more important than recall
                网络文档特点
                    主要的四个特性 & -
                        1.各种格式的文档都有
                            Most (truly) dynamic content : is ignored by web spiders -- 
                                it’s too much to index it all
                            Unstructured (text, html), 
                            semi-structured (html, xml),
                            structured/relational (databases)
                        2.多语言性 
                            Documents in a large number of languages
                            Queries in a large number of languages
                            Don’t return English results for a Japanese query
                            query/document languages are Frequent mismatches

                            特殊的查询：Cross-language information retrieval (CLIR） 跨语言查询
                        3.重复性 
                            see before
                            Today’s search engines eliminate duplicates very effectively
                        4.可信度
                            Hoaxes abound 骗局
            Crawling
                两个特点？ & 
                    should be distributed, scalable, efficient, polite, robust

                    Höflichkeit--
                        um eine Überlastung des Web-- Servers zu vermeiden
                    Robustheit--
                        gegen Ungültige/unvollständige HTMLDokumente
                        Netzwerkprobleme (z.B. hohe Latenzzeit / niedrige Bandbreite des Servers)
                ppt中例子 & - 理解，给图画出来路径
                    页面的宽度有限搜索 
                有哪些挑战？： & -
                    四个
                        Skalierbarkeit可扩展性
                            Mehr Rechner erhöhen Leistungsfähigkeit
                        Aktualität实时性
                        Verteiltes System
                        Qualitätsbewusstsein
            Ads
                后来：搜索与广告严格分开：Strict separation of search results and search ads
                How are ads ranked  & - exp
                    rank based on bid price AND relevance(two things)
                    Other ranking factors: 
                        location, time of day, loading speed of landing page
                    参数： bid CTR ad rank rank paid p47
                Google’s second price auction 
                    次高价拍卖 & -解释 and calcu。
                        计算 
                        p47
                问题： & -
                    主要的两个问题 
                    Keyword arbitrage
                        we can buy a keyword on google 
                        将流量重定向到支付比谷歌多得多的第三方
                    Violation of trademarks
                        The search term “geico” on Google was bought by competitors侵权
            Duplication Detection
                //
                    
                            solution1 :设计指纹比如64位
                            solution 2:k shingling技术 
                25-40% of the web is duplicate (mirrors - e.g. Wikipedia, unix man pages, SPAM sites)
                we should use Near-duplicates
                原因： & -
                    用户体验 
                    自身性能Conserve resources: 
                        reduced index size - less memory, faster computations, etc.
                True semantic similarity (similarity in content) is too difficult to compute
                几种基本的做法 & -
                    Hashing
                    编辑距离 
                    shingling算法，通过分割转换成集合，然后计算Jaccard coefficient
                        用于计算两个文档的相似度，例如，用于网页去重
                        -r 
                            Summarize each document in a short sketch，and then Estimate the similarity based on the sketches
                            look at the percentage of min-hashes that agree **
                            with different hash functions(doc and hash table )
                            
                            use MinHash
                                原因？ & - NEXT 
                                calcu &
                                    使用三元组？ ？？ NEXT
                                    Store the minimum hash: {143}
                                    Sim(A,B)=相同hash个数/总的个数
                                    其实也是在计算Jaccard coefficient
                            a sketch for a document is a collection of minimum hashes
                            use 84 hash functions
                            Summarized each page in 672 bytes (84*8 byte values)
                        Super Shingles
                            -r 
                                Doing all pairwise comparisons still too expensive
                                package the Shingles into super Shingles : doc - s-Shingle table  


                                再次哈希 **
                                    Group sketch into non overlapping super-shingles 
                                    Hash each super-shingle {1011, 6543, ..., 7327}
                                    例子 & - p65
                                        Declare doc 2 and doc 3 to be 2-similar
                                In practice - store the above table sorted by different columns
                                    Only compare against neighboring rows
                                    Store the super shingle table sorted by columns
                        对于下面的如何去重? & - exp
                                some Open Problems 
                                Flash, Ajax, and other not easily indexable content
                                Obtaining text from raw HTML is not as easy as it sounds
            SPAM detection
                垃圾链接分析
                You have a page that will generate lots of
                    revenue for you if people visit it
                It damages the search engine’s reputation
                Google no longer gives good rankings to
                    pages employing this technique
                常见的Spam技术 &x2
                    plus oben+ 
                        Misleading meta-tags, excessive repetition
                        Doorway page
                    Landing Page 
                    Serve fake content to search engine spider
                    Link spam 
                        Put these links on pages with high (or at least non-zero) PageRank
                反Spam
                It can also be a legitimate business – which is called SEO
                Google bombs **
                The war against spam
                and there is one thing called ::Webmaster Guidelines & -
                lack of central access control! & -
                Major search engines have guidelines for webmasters
                Ignore these guidelines at your own risk
                Once a search engine identifies you as a spammer, 
                    all pages on your site may get low ranks (or
                    disappear from the index entirely) **
                There is often a fine line between spam and legitimate SEO **
        ppt-14-PageRank und HITS 第 21 章 链接分析 --over --c 
            链接结构信息在 Web 搜索结果排序中的使用--链接分析
            链接分析结果已经成为 Web 搜索引擎在计算某个网页的组合得分中的一个因子
            Web 搜索中链接分析思想的最早起源于引文分析领域，后者在很多方面与一个被称为文献
                计量学（ bibliometrics）的领域有交叉。这些学科试图通过分析文献之间的引用模式来量化学术
                论文的影响力。
            链接分析的意义 & -
                Web 上的链接分析方法也把超链接看成是一个网页对另一个网页的权威度的认可
                find pages with high authority
            仅仅简单地通过入链接的数目来衡量网页的质量是不够鲁棒的。
                垃圾链接
            如何选择下一个采集网页？
            链接分析的研究主要基于两个基本直觉
                去掉这些“ 内部” 的链接
            因此， Web 网页本身携带的词项和用户用于描述同一网页的词项之间往往存在着一定的差异
            举例 & -
                很多指向www.ibm.com的链接上的锚文本都包含单词computer，这个事实就可以为Web搜索引擎所使用 
            比如，锚文本中的词项就可以作为索引目标网页的词项。因此，词项computer的倒排记录表中就会包含文档www.ibm.com
            同页内词项一样，通常也会基于词频来计算锚文本词项的权重 **
            那些在多个锚文本中高频出现的词项（如Web锚文本中最普遍的词项是Click和here）会受到惩罚
            某个网站可以通过构造具有误导性的锚文本来指向自己，从而提高在某些查询词项上的排名
            在web检索工具构建过程中，需要很多作弊检测工作
            很多学者还对有效窗口的大小进行了研究--使用多少锚文本合适？ 
            web搜索引擎不同于传统文档集？ 
            静态PageRank
                formua & 
                    transport probability alpha .
                    the prob. of transporting from i to j  is:: alpha /n + ..  
                在随机游走过程中访问越频繁的网页也越重要
                对 Web图中的每个节点赋一个 0 到 1 之间的分值，这个分值被称为 PageRank
                    给定查询，web引擎会综合各种指标：
                余弦相似度（参考 6.3 节）、词项邻近度（ 7.2.2 节）及 PageRank 等
                假设冲浪者以1/N概率跳向其他任意一个网页。当然，冲浪者也以 1/N的概率跳到其当前位置
                过程假设： & -
                    1.如果没有出链，随机游走
                    2.如果有出链alpha:随机跳转；1-alpha:继续进行随机游走
                从而，通过马尔科夫链规则，得知他就会以一个固定的时间比例 π(v)访问每个节点 v &
                +马尔科夫链： &
                    马尔科夫链是一个离散时间随机过程
                    Pij被称为转移概率，它仅仅依赖于当前的状态 i，这种性质被称为马尔科夫性
                    马尔科夫链中，下一个状态的分布仅仅依赖于当前的状态，而和如何到达当前状态无关。
                    根据P生成的随机矩阵，最大特征值是 1
                    图中链接上的数字代表的是转移概率
                    本质是一种向后迭代规则 & -
                回到网页问题：其中马尔科夫链中的每个状态对应一个网页，
                    而每个转移概率代表从一个网页跳转到另外一个网页的概率
                根据上面的规则，由N×N的临街矩阵A推导出马尔科夫链的转移概率矩阵P
                这样我们只需 状态分布和转移概率矩阵 P，就能计算冲浪者在任一时刻所处状态的概率分布
                我们将 PageRank 设置为每个节点 v 在稳态下的访问频率
                计算PageRank权重 & - p322
                    ergodic Markov chain
                    it will converge to a steady state


                    第一个条件保证从任意两个状态之间都存在非零概率转移序列，而第二个条件保证不存在这样的状态划分：
                        所有的状态转移只发生在两个划分后的状态子集之间并循环往复。
                    带随机跳转操作的最后可以得到唯一的稳态概率分布。某个状态的稳态概率就是相应网页的 PageRank
                    反复迭代一定次数之后，我们会看到分布收敛于一个稳定状态 ** 例子p322 & -
                    状态具有对称性的时候可以直接计算稳态概率分布 **
                    网页的 PageRank 与用户输入的查询无关--静态质量衡量指标
                    所以只是这个指标作为排名因子之一
                特点
                网页的 PageRank 与用户输入的查询无关
            面向主题的 PageRank
                -r 
                    use different PageRanks to calcu . when deal with different subjects .

                非等概率跳到一个随机网页
                一个体育迷可能希望有关体育主题的网页的排名要高于非体育主题的网页
                如果不是体育类的文章，令其PageRank为 0 &
                    实际上，体育相关的集合S未必有稳态
                    所以找一个比S稍微大一点的集合Y，有稳态
                    马尔科夫链稳态的时候每个网页具有非零值
                    Y 中的每个网页都有非零 PageRank 值
                    在Y之外的文章分数归零
                各个不用的主题：打分时候调用不同的PageRank进行计算： &
                    比如科学宗教等
                    当搜索用户仅仅对某个主题感兴趣时，那么在对检索结果打分和排名过程只须调用相应主题的PageRank向量值进行计算
                    用户有可能显式地注册了其兴趣，或者系统能够从每个用户的历史行为中学到其兴趣
                处理混合的情况？ &
                    -r 
                        each user can have an individual PageRank on different topics 
                        we mix them with linear functions 
                    实际上任一用户的个性化PageRank 都可以被表示成多个面向主题的 PageRank 的线性组合
                    0.6*pi1 + 0.4*pi2
                    分别是面向体育和 的 政治主题 PageRank 向量
                    如果随机跳转操作的概率是 10%的话，那么其中有 6%是到体育类，而 4%到政治类网页
            HITs基于查询的hub指数和权威指数--针对信息类检索
                两个排序结果列表，其中一个基于hub值，而另一个基于authority值
                之间构成良性循环，共同组成一个Web 子集
                    构造：
                        将根集及指向根集中的网页和根集所指向的网页加入到基本集（ base set）中
                        利用基本集进行计算
                        原因有三 &
                        可以获得跨语言的效果：比如如果有一个英文的 hub 网页指向一个日文描述的日本小学的主页，最后返回的结果会有类似乱码的效应
                hub 值指的是该网页的导航能力，也可以称为导航度。而 authority 值指的是网页的权威度。
                迭代算法的核心环节就是 hub 值和 authority 值的双重更新过程
                计算每个网页的数值？ &
                循环迭代公式 & -
                邻接矩阵表示的循环公式 &
                上面那个式子就变成矩阵 AAT的特征方程，而下面那个式子则会变成矩阵 ATA 的特征方程
                那么 h 和 a 最后会收敛于某个唯一的稳态向量，最后的稳定状态取决于图结构 & **
                计算优化？ & - 
                    核心就是变成了求主特征向量
                HITS（ Hyperlink-Induced Topic Search，超链导向的主题搜索）
            总结：
                链接分析的目的：分析链接权威性，给网页打分
                任意计算特征向量的算法都可以用于计算 hub 值和 authority 值向量
                我们并不需要计算这些值的精确值，而只需要知道这些值的相对大小以便能够进行排序即可
                实际上公式（ 21-8）只需要大概 5 次迭代就可以产生相当好的结果
                迭代更新而不是直接计算，因为Sparse
            -------------------------------------------------------------------------------------------
            Webgraph
            Linkanker链接描述，mao文本
                Ankertexte
            Eigenschaften des WWW & -
                Nicht zusammenhängend  unerreichbare Knoten
            链接分析方法： PageRank 和 HITS
            PageRank
                Statisches Qualitätsmaß
                Why count links ? & -
                    Gute Inhalte werden häufiger verlinkt *** 
                    good content offen means more links 
                1.Random Surfer
                    例子 p14 & -
                    有哪些困难？: & - 
                        Sackgassen
                        Anderes Problem: Kreisläufe
                2.Modellierung &
                    using stochastisch Matrix
                    calcu & -
                        最后给出Ranking结果
                    p23公式 & -
                    bestimmten Zustand zu sein
                    Berechnung Eigenvektor
                pagerank 特点 & -
                    Von Anfrage unabhängig
                    Offline Berechnung
                    （其实是每周计算一次）
            HITS
                理解 & -
                    也是一种逐步进入稳态的算法，但是只是在基本集合中进行
                    局部算法，针对IBM这样全是图片的网页
                idea & -
                    Authorities -Haben gute Inhalte
                    Hubs -Verweisen auf gute Inhalte
                Formalisierung & -
                    Darstellung über Adjazenzmatrix
                    Multiplikation Matrix mit Vektor一种数学操作 **
                    Iteratives Verfahren
                    Verfahren muss nicht konvergieren不是一定收敛
                    Potenzmethode (vollständig) &&&&这种方法，也就是问题转换的第二个关键步骤：求主特征向量
                    计算 &
                具体应用过程 & -
                    1.给定某个查询（比如 leukemia），
                        利用某个文本索引获得包含 leukemia 的所有网页。这些网页被称为根集（ root set）
                    2.我们利用上述过程产生的基本集进行 hub 值和 authority 值的计算
                        之所以如此构造基本集的原因在于

                    HITS nicht auf komplettem Web berechnen, sondern auf Suchergebnissen
                    ：只是在搜索结果中进行搜索，不是全网
                    Suchterme bestimmen eine Ergebnismenge von Dokumenten  root set (=Startmenge)
                    ：怎样运用于网络搜索排名
                    Überwindung von Sprachen
                hits 特点 & -
                    -r 
                        goes only in the result of queries not the whole web 
                        different languages can be involved 

                    只是在搜索结果中进行搜索，不是全网
                    Überwindung von Sprachen
                问题：  & -
                    -r 
                        not relevant website can be involved 

                    可能引起主题票变，因为Aufnahme nicht relevanter Seiten 
        -----文本分类 --似乎不仅仅是提供搜索服务，更有人工智能在里面（与时俱进模块）
            +
                例如文章自动分类、邮件自动分类、垃圾邮件识别、用户情感分类等等
                根据一些公司描述预测公司性质，给其分类
                数据集是11类公司的描述数据，我们要根据4774条训练数据去预测2381条数据的类别标签
                类别不平衡问题
                使用CHI选择特征，TFIDF计算特征权重，朴素贝叶斯分类的整体流程
                    https://blog.csdn.net/liuchonge/article/details/52204218
                大致步骤：
                    使用结巴中文分词工具对文本进行处理，    
                        并去停用词得到所有文本中出现的词语。
                    使用CHI作为特征选择的依据给每一类新闻选出150维的特征
                        ，并去重。这样我们就可以获得大概1000维的特征。
                    特征-->每个新闻构造VSM模型（一个特征矩阵，每一行都是特征向量 ***）
                        特征权重的多重算法：看图，三种
                            1.用是否出现表达
                                最简单
                            2.用tf
                            3.使用TFIDF方法计算各特征的权重得到表示该文本的特征向量
                    现在就可以方便使用KNN/SVM等方法分类的数据
                文本的相似度计算会有进一步的应用，比如文本的分类、聚类等 ***
        ppt-15 一些基础  --over 
            Klassifikation als Aufgabe im IR
                Herleiten von 𝛾来源 & 
                     Manuell
                     Erlernen aus Beispielen
                Beispiele Textklassifikation
                    在垃圾邮件处理中，有哪些基本的处理方法 & p7 
                        maybe we will let you deside if it is spam 
                        check the keyword or the metadata 
                    Ressortzuteilung von Nachrichten
                    Erkennung von Sprache / Zeichensatz
                    Sentimentanalyse
                    Who wrote which Federalist papers?
                    Male or female author?  
                        50 features to distinguish male-authored texts
                        from female-authored texts
                    Positive or negative movie review?
                    What is the subject of this article??
                the formal expression & 
                    Einfach Zuweisung
                    Mehrfache Zuweisung
                        Mehrfachklassifikation bzw. Überschneidende Kategorien
                questions about Maschinelles Lernen &
                    1.how function this method ?use pic to explain
                    2.about overfitting problem (speak more than one min)
                Ergebnisse Eva.
                    richtig oder nicht ? & 
                        explain it about 0.5 min .
                        Korrekte Klassifikationen
                             d aus c1 als c1 klassifiziert
                             d aus c2 als c2 klassifiziert
                        Falsche Klassifikation
                             d aus c1 als c2 klassifiziert
                             d aus c2 als c1 klassifiziert
                    1.using Confusion Matrix & 
                        rem. pic.
                        Verwendung von Evaluationsmaßen wie Recall, Precision und F1 möglich
                    2.Confusion Matrix erweitern p27 pic & 
                        Recall und Precision nicht als Gesamtmaß anwendbar
                        calcu . with 
                            Accuracy 
                                korrekten Ergebnisse über allen vorgenommenen Klassifikationen 
                                sum of diag numbers / sum of all numbers  **
                                calcu Beispiel p28 &
                            Error Rate 
                                calcu 
                            Kritik an Accuracy &
                                if one class has more articals ,  
                                and thus the total acc will be strongly infact by it 
                    3.cross validation 
                        exp & 
                            1.randomly part the dataset to k categraies 
                            2.choose one cat. for evaluation and the others for traning .
                            3.repeat it for n times with diff. choise .
                                n-times k-fold cross-validation  
                                    typischer Wert: 10-times 10-fold cross-validation
            数据调整，特征抽取（分类的第一步）    
                Klassenungleichgewicht
                    three methods & 
                        Undersampling
                             Ausgleich unterschiedlicher Häufigkeiten 
                                zwischen den Klassen durch Eliminierung von
                                Beobachtungen der größeren Klasse L
                        Oversampling
                             Vervielfältigung von Beobachtungen der kleineren Klasse S
                        Hybrid-Verfahren
                             Mischform zwischen Under- und Oversampling

                    Undersampling
                        https://zhuanlan.zhihu.com/p/34782497
                        1.use Undersampling-Rate,随机欠采样
                            Imbalance-Rate & rario between big one and small one 
                            减少多数类样本数量最简单的方法便是随机剔除多数类样本
                        2.NearMiss方法
                            是利用距离远近剔除多数类样本的一类方法，实际操作中也是借助kNN
                            具体 &
                                NearMiss-1：在多数类样本中选择与最近的3个少数类样本的平均距离最小的样本。
                                NearMiss-2：在多数类样本中选择与最远的3个少数类样本的平均距离最小的样本。
                                NearMiss-3：对于每个少数类样本，选择离它最近的给定数量的多数类样本。
                            比较 &
                                NearMiss-1和NearMiss-2方法的描述仅有一字之差，但其含义是完全不同的：
                                    NearMiss-1考虑的是与最近的3个少数类样本的平均距离，是局部的；
                                    NearMiss-2考虑的是与最远的3个少数类样本的平均距离，是全局的。
                                论文中有对这几种方法的比较，得到的结论是NearMiss-2的效果最好
                                    ，不过这也是需要综合考虑数据集和采样比例的不同造成的影响
                        3.Condensed Nearest Neighbors (CNN)
                            一种基于聚类的方法 *
                            NEXT
                            Durch Zufallskomponente kann die erzeugte Untermenge stark variieren
                        4.Underbagging
                            欠采样被整合到AdaBoost.M1中。RUSBoost被证明是比AdaC2，
                                SMOTEBoost，MSMOTEBoost，UnderBagging，
                                EasyEnsemble，BalanceCascade更好，更快，
                                更简单的替代方案，因此成为从偏斜训练数据中学习的可行选择。 
                            NEXT
                            Mit steigender Anzahl der Modelle umso wahrscheinlicher, dass jede Beobachtung in zumindest
                            eines der Modelle einfließt und somit nicht komplett verworfen wird
                    Oversampling
                        Vergrößerter Trainingsdatensatz
                        1.use ratio to enlarge 
                        2.SMOTE-Verfahren (Synthetic Minority Oversampling Technique)
                            NEXT
                                普通的过采样会使得训练集中有很多重复的样本。 
                                产生overfitting问题
                                SMOTE的全称是Synthetic Minority Over-Sampling Technique，译为“人工少数类过采样法”。
                                通过feature space
                                使用线性变换？
                            Distanzmaß: euklidische Distanz bzw. Gower-Distanz für kategorische Attribute
                        ++can be more 
                Feature-Scaling
                    +
                        why two values should be likely :基本思想 & -
                            when x1 :[0-2000]
                                x2: [1-5]
                            feature map 会变成椭圆，对后续处理不好：：求数值的时候会来回跳动 
                        一般我们选择将feature的value落在 [−1,1], 
                            但是我们不需要严苛的要求一定满足[−1,1]，其实左边界在[−3,−13]，右边界在[13,3] 就可以了
                    methods & 
                        1.Standard Scaler
                            标准化
                            处理以后是N(0,1)
                        2.MinMax Scaler
                            Skaliert auf den Bereich 0 bis 1 bzw. -1 bis 1 falls negative Werte vorhanden sind
                            Sensitiv gegenüber Ausreißern-对异常数据敏感（比如有一个很大）
                        3.Robust Scaler
                            wird der Interquartilsabstand 四分位数(engl. interquartile range) verwendet
                            Dadurch robuster gegenübern Ausreißern
                            calcu ?? NEXT
                Feature-Auswahl
                    -r
                        based on frequncy
                        MI formular & calcu. & p65
                        with the help of Chi-square test calcu. & p69
                    主要目的 &
                        Text collections have a large number of features
                        it can Eliminates noise features and avoid overfitting
                        特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。
                        特征选择能剔除不相关（irrelevant）或亢余（redundant）的特征，
                            从而达到减少特征个数，提高模型精确度，减少运行时间的目的
                    Utility measures A(t,c), three different approaches &
                        -r
                            not all features are suitable for classification purpose
                            only this subset as features in text classification

                         Frequency: 𝐴 𝑡, 𝑐 = 𝑁(𝑡, 𝑐)
                            spec. in IR : Selecting terms that are most common in the class(document )
                            but: May have no specific information (such as, Monday, Tuesday …)
                         Mutual information: 𝐴 𝑡, 𝑐 = 𝐼(𝑈𝑡; 𝐶𝑐)
                            这种由于训练集的偶然性导出的不正确的泛化结果称为过学习（overfitting）。解决这个
                            词项的存在与否给类别c的正确判断所带来的信息量
                            选择最大的k个项目
                            选出的词汇对类别的判定起到良好作用
                            formular &
                            calcu. & p65
                         The 𝜒2 test: 𝐴 𝑡, 𝑐 = 𝜒2(𝑡, 𝑐)
                            在统计学中，χ 2统计量常常用于检测两个事件的独立性
                            如果他们是相关的则说明信息是有用的
                            formular &
                            calcu. & p69
                        +后验概率
                                https://blog.csdn.net/u011508640/article/details/72815981
                                尤其是当后验分布没有一个简单的解析形式的时候更是这样：
                                    在这种情况下，后验分布可以使用 Markov chain Monte Carlo 技术来模拟，
                                    但是找到它的模的优化是很困难或者是不可能的
                                最大似然估计（Maximum likelihood estimation, 简称MLE）和
                                    最大后验概率估计（Maximum a posteriori estimation, 简称MAP）
                                    是很常用的两种参数估计方法
                                你有多大把握能相信一件证据？（how much you can trust the evidence）
                                贝叶斯公式：做判断的时候，要考虑所有的因素。 
                                    老板骂你，不一定是你把什么工作搞砸了，可能只是他今天出门前和太太吵了一架。
                                    一个本来就难以发生的事情，就算出现某个证据和他强烈相关，
                                        也要谨慎。证据很可能来自别的虽然不是很相关，但发生概率较高的事情
                                +MAP 
                                    最大似然估计是求参数θ, 使似然函数P(x0|θ)最大。最大后验概率估计则是想求θ使P(x0|θ)P(θ)最大。
                                        求得的θ不单单让似然函数大，θ自己出现的先验概率也得大
                                    MAP其实是在最大化P(θ|x0)
                                    可以减缓实验的不稳定性
                                    更好的估计？ 
                                MLE是把先验概率P(θ)认为等于1，即认为θ是均匀分布。
                                通过互信息去噪后，由于只保留了有效的特征，后验概率加大？？？
                        +
                                https://blog.csdn.net/BigData_Mining/article/details/81279612
                                以上三种不同的角度说明: 从一个事件获得另一
                                    个事件的平均互信息需要消除不确定度,一旦消除了不确定度,就获得了信息。
        ppt-16--Rule-based Text Classification --over 
            +
                通过一系列规则“如果。。。就。。。”，来进行分类
                决策树可以转换成基于规则的分类器
                规则按照优先级一次排列---->决策列表
                规则排序模式
                    （1）基于规则排序：规则的分类能力
                    （2）基于类的排序：相同类别的规则排在一块
                rule evaluation 规则评估
                    1.如果信息增益不理想，丢弃该规则
                    2.和决策树的post-pruning后修剪类似
                        reduced error pruning：
                        确定一条规则
                        在修剪之前和修剪后，分别计算比较在验证集上的错误率
                        如果错误率变高，丢弃该规则
                顺序覆盖
                    步骤：
                    （1）start from an empty rule
                由决策树生成：C4.5rules
            
            ---
            Regelbasierte Systeme
                Form: &
                    Regeln haben Wenn-Dann Form 
                Formale Darstellung &
                Bezug zu Boolschem IR
                    Ati 描述？？
            Manuell erstellte Regeln
                exp & p10
                    ::
                        simple quick and efficient 
                        can be used to filter emails ..
            Evaluation von Regeln
                Grundlegende Maße
                    两个指标 公式 &
                    accuracy
                    Coverage
                        太小：：overfitting,Regel zu speziell,rule too special 
                        match too many test data , also not good 

                    计算 & p15
                    wir mochten hohe Accuracy und niedriger Coverage-Wert
                Drei Qualitätsmaße -- 目的只有一个，总结前面两个成为一个指标
                    formular &
                    p21 ?? NEXT
                     Laplace-Maß 
                        分子分母的一种带有smoothing的结合方式
                     Likelihood Quotienten Test
                        使用概率技巧，分成两种类别
                     FOIL's Information Gain
                        计算改进，相对于原有的rule
                    eg. & p26
            Sequential Covering
                NEXTNEXTNEXT
                    基于FOIL_Gain
                    This sequential covering algorithm is one of the most widespread approaches to learning disjunctive sets of rules
                    在训练集上每学到一条规则，就将该规则覆盖的训练样例去除，然后以剩下的训练样例组成训练集重复上述过程
                    http://www.cse.unsw.edu.au/~cs9417ml/Rule/sequential.html
                    从D中删除cover掉的term：：第一次的时候删除了𝑈𝑚𝑠𝑎𝑡𝑧 
                idea &
                    use graddy search , depth first search 
                Abbruchkriterium für learnRule
                    1.Laplace-Mass begain to decrease 
                    2. keine Accuracy von 1 
                    3. all documents has been removed 
                formular &
                ：：eg p33 ***
                Nachbearbeitung
            Aber:
                Aufbau und Pflege regelbasierter Klassifikationssysteme ist aufwändig und teuer
        ppt-17--Naïve Bayes Text Classification --over 
            ---
            Probabilistische Klassifikation
                +
                 Idee
                 Lernen / Schätzen von Wahrscheinlichkeiten
                Zuordnung eines Dokuments zur wahrscheinlichsten Kategorie
                idea with formular & 
                why Naïve &
                    it assumps that there is no relationship between terms 
                why not Naive &
                    very robost against concept drift 
                        (Veränderung der Definition einer Klasse über die Zeit)
                    very quick runs 
            Multinomial Naïve Bayes
                +corpus 
                    direct proportion to 
                    inverse proportion 
                    They are often in direct proportion to their wealth.
                        - the value of .. is in direct proportion to the number of ..
                    

                
                对于多个doc的formular & 
                Ähnlichkeit zu LM (Vorlesung 12) ,exp &
                Lernphase 
                    exp & 
                        ::
                            calcu. the probability of cj 
                            calcu. the p. of tk given cj 
                                if the term appears in the docu.    

                    Klassifikationsfunktion & formular 
                    eg & 
                
                -r 
                    similar to LM 
                    mianly beased on conditional probability 
                    主要使用后验概率计算
                    用词汇频率进行估计，条件概率

                +
                    NB-Klassifikatoren::steht fur Naïve Bayes 
                    Average Accuracy zwischen 0,81 und 0,84比基于规则的好很多
                    朴素贝叶斯的独立性假设很傻很天真，too simple，sometimes naive，所以预测精度往往不是很高
                    如果没有独立假设？也可以计算？
            Bernoulli Naïve Bayes
                +

                    不用频率，而是Term是否出现评价计算:NB’s main strength is its efficiency
                    因此这里使用的主要是df，文档频率
                    我这有的用公式计算，其他不出现的应该是乘以反向概率 ** 这就是被努力（想想分布也是这样）
                    不是乘以系数的关系，底层模型不一样
                    “Term vorhanden” oder “Term nicht vorhanden” als 
                        Ergebnis der Beobachtung eines Dokumentes
                        Bernoulli-Verteilung
                    Klassifikationsfunktion？ **
                    Anmerkungen
                    Aktuelle Anwendung
                        跨语言信息检索 
                idea with formuar  &
                Klassifikationsfunktion &
                Beispiel &
                Anmerkungen &
                    -r 
                        NB’s main strength is its efficiency
                            uses binary term occurrence features 
                            Multinomial Naive Bayes is in a sense more complex model 
                        suitable for short doc.
                        It has the benefit of explicitly modelling the absence of terms
                        
                    Für kurze Dokumente geeignet
                    Sinnvoll bei negativem Zusammenhang zwischen Termen und Kategorien
                    Etwa bei positiven oder negativen Ergebnisse von medizinischen 
                        Tests um einen Krankheitsbefund zu erstellen
            Aktuelle Anwendung &
                CLIR-System
        ppt-18--Vector Space Text Classification   --over
            +
                mit Cosinusmaß
                Rocchio Klassifikator
                    -r 
                        Assign category to the nearest centroid
                        nearst centroid and far from others 

                        

                    基本思路
                        1.简单得只计算中心
                        2.或者考虑正反例子：中心的位置与正例子近而与反例远
                    缺点：：
                        它认为一个类别的文档仅仅聚集在一个质心的周围，实际情况往往不是如此
                        不可以容错--万一有一个错误的呢！！致命

                        -r
                            there will be sth. wrong , when the classes are not convex  
                                but in practice , classes are rarely distributed in this way 
                                like spheres with similar radii
                            it is not robost with noise 

                    常常被用来做科研中比较不同算法优劣的基线系统（Base Line）
                    用于相关反馈：
                        假定我们要找一个最优查询向量q ，
                        它与相关文档之间的相似度最大且同时又和不相关文档之间的相似度最小
                    0,86 und 0,87
                    Segmentierung des Vektorraumes = Voronoi-Diagramm
                kNN 
                    如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)
                        的样本中的大多数属于某一个类别，则该样本也属于这个类别，
                        其中K通常是不大于20的整数
                    KNN算法中，所选择的邻居都是已经正确分类的对象。
                        该方法在定类决策上只依据最邻近的一个
                        或者几个样本的类别来决定待分样本所属的类别。
                    依赖于k
                    KNN通过依据k个对象中占优的类别进行决策，
                        而不是单一的对象类别决策。这两点就是KNN算法的优势
                    Varianten
            ---
            Klassifikation im Vektorraum
                Vektorraum:vector space
                画图 & p6 
                exp &
                    Aside: 2D/3D graphs can be misleading 
                    ::
                        the vectors are high dimentional vectors 
                        compare them to measure similarity between them.
            Rocchio Klassifikator
                rocchio classification algorithm
                Rocchio relevance feedback is designed to
                    distinguish only two classes, relevant and nonrelevant.
                罗基奥！！
                exp idea &
                    Kategorie des nächstgelegenen Zentroiden zuweisen
                    观察得到的特点：liegen untereinander räumlich dicht
                        beisammen und gleichzeitig zu den anderen Kategorien weiter entfernt
                    Zentroiden liegen im Zentrum der Trainingsdokumente zu einer Kategorie
                    ::
                        it uses centroids to calcu. 
                            the center means :vector average
                            one centroid means one class 
                        boundaries?
                            points with equal distance from two centroids
                            are boundaries of classes .
                formular & p13 
                Beispiel & 
                Probleme bei Rocchio &
                    Fläche des Voronoi-Diagramms sind konvex
                    Probleme bei
                         Unterschiedlicher Größe
                         Unterschiedlicher Dichte
                         Verschlungene / „verzahnte“ Formen
                    Bereits Fehler auf Trainingsdaten
                    它认为一个类别的文档仅仅聚集在一个质心的周围，实际情况往往不是如此
                Segmentierung des Vektorraumes = Voronoi-Diagramm
            KNN
                +参考手写数字识别程序 
                是上面方法的变种扩展 ××
                process & 
                    Zu Dokument k nächstgelegenen Dokumente auswählen und deren Kategorien betrachten
                    am häufigsten vorkommt wird dem zu klassifizierenden Dokument zugewiesen

                    ::
                        the result class will be assigned to the class
                            most common among its k nearest neighbors
                        we find the k nearest neighbors, an then , find the most commen class among them 

                Beispiel &
                1NN: Voronoi-Diagramm mit mehreren Zellen
                Wahl von k? &
                    小的时候是一个类别，增加以后是另一个，不稳定
                    其中K通常是不大于20的整数
                    画图
                    在应用中，我们一般取一个较小的k值，通常采用交叉验证法来选取最优的k值。

                    ::
                        the result may not stable with k 
                            when k is small , the reuslt is one , when it larger , be an other .
                        Normally , k is no larger than 20 
                        use cross validation to choose the value of k 

                变种 & 
                    nennen Sie zwei
                    -r 
                        we can set weight to the neighbors 
                        use the raw vector , instead of normalize them.

                    1.Gewichtung der Nachbarn
                    2.Keine Operationen auf Vektoren
        ppt-19--SVM分类数据  --over 
        +周机器学习
        +基于神经网络的分类
        +
            Skalarprodukts
            Mehrklassenprobleme
            Iteratives Verfahren
                Sonst bei jeder Iteration mindestens ein Fehler  Gewichtsvektor und Bias müssten erneut
                    angepasst werden
                Lernrate: beeinflusst nur Länge von w
                Duale Darstellung
                Kein SVM
                只是一种迭代方法产生超平面，用一个条件，用一定的学习速率进行这个过程
                Wiederholung der Durchläufe bis alle Beispiele richtig klassifiziert werden
                考虑对偶问题
            SVM 
                die möglichst viel Abstand zu den Datenpunkten aus den 
                    verschiedenen Kategorien lässt
                Stützvektoren
                Duale Formulierung
                +二次规划
                    优化(最小化或最大化)多个变量的二次函数，并服从于这些变量的线性约束
                    一种数学问题
                    二次规划是一种特殊的非线性规划
                    根据优化理论，一个点x成为全局最小值的必要条件是满足Karush-Kuhn-Tucker条件（KKT）。
                    当f(x)是凸函数时，KKT条件也是充分条件
                    当二次规划问题只有等式约束时，二次规划可以用线性方程求解。
                    否则的话，常用的二次规划解法有：
                        内点法(interior point)、
                        active set和共轭梯度法等。
                    凸集二次规划问题是凸优化问题的一个特例。
                    对于一般问题，常用的方法有很多，包括
                        interior point,内点法
                        active set 内点法
                        augmented Lagrangian
                        conjugate gradient,
                        gradient projection,
                        extensions of the simplex algorithm
                    计算复杂性
                        当Q正定时，用椭圆法可在多项式时间内解二次规划问题。
                        当Q非正定时，二次规划问题是NP困难的（NP-Hard）。
                        即使Q只存在一个负特征值时，二次规划问题也是NP困难的
                +变种
                    软间隔分类器
                    https://www.jianshu.com/p/8a499171baa9
                    针对噪声问题
                    使用svm库时需要调整不少参数，以高斯核为例，此时至少需要考虑的参数有C和σ
                        ，其中C为训练样本误差项的惩罚系数，该值太大时，会对训练样本拟合很好（bias小）
                        ，但对测试样本效果就较差(variance大)
                +解决曲线分类问题：
                    如何理解？ ？？ ppt？？
                    支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间， 
                        最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开 &
                    Kernel Trick & 解释 核技巧？
                        约束问题中内积ϕi⋅ϕj的运算会非常的大以至于无法承受，因此通常我们会构造一个核函数
                        一般很难构造出完全符合输入空间的核函数，因此我们常用如下几种常用的核函数来代替自己构造核函数
                        核函数的选择在SVM算法中就显得至关重
                    几种常用的核函数
                        线性核函数
                            我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的 &
                        多项式核函数
                            的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算
                            维数是参数parameter in sklearn learn
                        高斯
                            而且其相对于多项式核函数参数要少，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数
                            generate radial areas around training points
                        采用sigmoid核函数
                            支持向量机实现的就是一种多层神经网络 ？？ 
                    参数
                        C:tradeoff between how smooth the decision boundary is and how well it classifies examples
                        Gamma: kernel coefficient for rbf, poly, and sigmoid
        ---
        Klassifikation über Hyperebene
            Darstellung Hyperebene mit Ebenengleichung 
                -r 
                    If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes
                    hyperplane

                formular &
                clacu p7 &
            Vorgehen bei mehr als zwei Kategorien ?Multiclass SVMs?
                exp & p10
                    ::
                        1.Build a classifier for each class
                        2.Given the test document, apply each classifier separately.
                        3.Assign the document to the class with the maximum score
                        example ??
            
        Trainingsphase:Perzeptron--Iteratives Verfahren von Hyperebene
            verfahren exp &
                eg. Wiederholung der Durchläufe bis alle Beispiele 
                    richtig klassifiziert werden
                Gewichtsvektor und Bias müssten erneut angepasst werden bei Fehlern.
                :: 
                    0.start from w=0 and b=0
                    1.use the training set to update the value of w,b
                    2.repeat until all docs. are classified 

                    +the perceptron is an algorithm for supervised learning 
                        of binary classifiers
                    https://en.wikipedia.org/wiki/Perceptron#Multiclass_perceptron
            algo p15 & 
            calcu. p16 & 
            Perzeptron erlernt nur eine Hyperebene
            learn rate:
                control the relationship between weight vector and bias 
        SVM
            möglichst viel Abstand
             Linearer Fall
                Einfach Verfahren &
                    formular p30 & -
                    Duale Formulierung & -
                        mit dem Gradientenverfahren lösbar
                        Lösung dieses Problems = Lagrange-Multiplikatoren α i
                        Trainingsdokument ist ein Stützvektor ???? NEXT
                    Beispiel p33 &
                    Sensitivität
                Erweitertes Optimierungsverfahren
                    Einführung einer Schlupfvariable：：引入松弛变量
                    slack variable
                    formular &
                    Einfluss von C exp & -
                        darstellung mit ein Bild 
                        Je größer C je stärker werden falsch
                            klassifizierte Trainingspunkte bestraft
                        ::
                            The parameter C is a regularization term, 
                                which provides a way to control overfitting
                            in our course , it should be small .
                            small  c has a better effect , Robust against failure
                            
             Kernel Trick--Nichtlineare Entscheidungsgrenzen
                key idea &
                    在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开
                    ::
                        saparate the calsses in high dimentional .
                        It will be easier to do classification .
                types of kernel  & 
                    mehr als 4 
                        RBF Kernel--Gauß‘scher Kernel
                        Polynomial Kernel
                        Sigmoid Kernel
                        Cosinus Similarity Kernel
                        Chi-squared Kernel




