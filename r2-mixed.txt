
    ---
    Lan training
        base::
            generic storage...
                similar. general
            math 
                calculus..
                Introduction to Calculus
                    regular polyhedra 
                    Sets, Functions, Graphs and Limits
                    Elementary Set Theory, Subsets, Set Operations, Coordinate Systems
                    The Exponential and Logarithmic Functions
                    The Hyperbolic Functions
                    Symmetry of Functions, Translation and Scaling of Axes,
                    Equations of Lines
                    Parabola, Ellipse, Hyperbola
                    Conic Sections in Polar Coordinates
                Differential Calculus
                    The Derivative of
                    take the derivative of both sides 
                    And then we take the derivative.!!!!
                        -r now , we are going to take the derivativ 
                Integral Calculus
                    Integrate 
                    Table of Integrals
                    I will integrate the function on that region !!!!
                        -r now , we are going to integrate this function on the rigion 
                    Method of Partial Fractions
                    The Definite Integral
                    Improper Integrals
                    Cylindrical Coordinates
                applications of 
                    Spring-mass System
                    Damping Forces
                    Mechanical Resonance

        ch1 
            Information retrieval 
                is the task, 
                        given a set of documents and a user query, 
                        finding the relevant documents
                        // find the relevant docs , according to the collection and the queries 
                Automated information retrieval systems are used to reduce
                    what has been called information overload
                        // why should we gen. IR system : there are huge amount informaiton that we should manage 
                An IR system is a software that provide access to books, 
                    journals and other documents, stores them and manages the document.
                        // we can access to these books , using the system 
                Web search engines are the most visible IR applications. 
                    // Medical Imaging is the most visible application in CG 
                information retrieval process begins when
                Queries are formal statements of information needs,
                search strings in web search engines
                with different degrees of relevancy.
                opposed to classical SQL queries of a database
                compute a numeric score on how well each object in the database matches the query
                    // the key idea of our sys. is to compute a numeric score to ranking the doc. 
                        so as to provide a very quick access
                The process may then be iterated if the user wishes to refine the query
                Now the world has changed, and hundreds of millions of people engage
                    in information retrieval every day when they use a web search engine or search their email
                    web search engine:: pronu.
                

            unstructured text
                does not have a pre-defined data model
                    // has no data model , when it compared to data in databank 
                typically text-heavy
                an ambiguous word/term/statement 
                This results in irregularities and ambiguities that make it 
                    difficult to understand using traditional programs 
                80-90% of all potentially usable business information may originate in unstructured form
                Other sources have reported similar or higher percentages of unstructured data
                majority of that will be unstructured
                    // xmajority of data that in our daily life are unstructured data 
                The earliest research into business intelligence focused in on unstructured textual data
                    // at the very begining , ir was used into business intelligence
                the extraction and classification of unstructured text
                Approaches in medicine and biomedical research
                biomedical documents include self-organizing map 
                    approaches for identifying topics among documents
            

        ch2 
            what is a model 
                Mathematical models are used to study the properties of 
                    the process, draw conclusions, make predictions
                derived from .. 

            boolean re. is something that very precise
            Boolean model of information retrieval (BIR)
                The BIR is based on Boolean logic and classical set theory 
                the user's query are conceived as sets of terms. 
                    //  bag of words model 
                Retrieval is based on whether or not the documents contain the query terms. 
                    // has ignore the term freq. and just check if the term is included in the doc.
            why inverted 
                postings file or inverted file
                    a mapping from content
                    named in contrast to a forward index, which maps from documents to content
                        // 
                            it is named in contrast to forward index 
                            forward index maps froms documents to content 
                            inverted maps from words to documents 
                    The purpose of an inverted index is to allow fast full-text searches
                        //
                            to enable fast full text research , we generate an inverted index for the collection 
                        It is the most popular data structure used in document retrieval systems
            Phrase Queries
                A word query is a query on a word or phrase
                    // a phrase can be 2 or more words :: the wto black albama ...
                search for documents containing an exact sentence or phrase
                searching for a certain string of text
                using quotation marks (") around a specific phrase to 
                    indicate that you want to search for instances of that search query
                    //
                        there should be quotation marks around this string 


        ch3 
            a morpheme is the smallest meaningful unit in a language
                // meaningful unit 
            stemmer 
                handling of word endings by reducing the words to their word roots  
                    // find the root of words 
                Stemming broadens our results to include both word roots and word derivatives
                        to improve recall
                    // stemmers are used to increse recall in ir sys.
                A stemming algorithm is an algorithm that converts a word to a related form
                conversion of plurals to singulars. 
                It is the most effective and widely used
                it can only be applied to text in the English Language
                    // the porter stemmer
                the user specifies a word in a query but only a variant of this word is present in a relevant document
                This problem can be partially overcome with the substitution of stems for the words
                A stem is the portion of a word that is left after the removal of its affixes (i.e., prefixes and suffixes)
                improving retrieval performance
                affix removal, table lookup, successor variety
                    determination of morpheme boundaries 
                and n-grams.
                might require considerable storage space
                reduce the size of indexing files
                In summary, the successor variety stemming process has 3 parts:
                develop a stemmer that required little or no human processing.
                    // the systems are considered to requiring little human processing 
                    // sys should do little interaction with human 
                it facilitates the construction of an attractive user interface
                to test the correctness of our work
                    // to check if it is correct 
                we have achieved an 80% level of correctness
                Several advantages of the Successor Variety Algorithm can be observed
                    --  without the need to use a dictionary
                    -- it is basically (domain independent)
                Another advantage is that it can be used in several domains; it is basically (domain independent).
                National Computer Conference and Exhibition
            Part-of-Speech 
                The part of speech indicates how the word functions in 
                    meaning as well as grammatically within the sentence.
                    // find the function of the words we are trying to tag in the sentance .
            
        ch4 
            encyclopedia
            Wildcard character
                a wildcard character is a kind of placeholder represented by a single character
                an asterisk 
                can be interpreted as a number of literal characters or an empty string
                used in Regular expressions
                doc* matches doc and document but not dodo
            k gram
                an n-gram is a contiguous sequence of n items from a given sample of text or speech
                n-grams may also be called shingles
                a type of probabilistic language model
                n-gram models are widely used in statistical natural language processing.
            Spaceâ€“time tradeoff
                an algorithm or program trades increased space usage with decreased time
                Using stored knowledge or encoding stimuli reactions as "instincts" 
                    in the DNA avoids the need for "calculation" in time-critical situations
                More specific to computers, look-up tables have been implemented since the very earliest operating systems
            Spelling Correction
                spell checker :: ot check the spell of a word if it is correct
                levenstein distance 
                k gram 
                Context-sensitive process 
                a software feature that checks for misspellings in a text.
                n-grams, to recognize errors instead of correctly-spelled words
                with phonetic information
                Gorin wrote SPELL in assembly language, for faster action
                available on mainframe computers
                PCs with sufficient memory.
                For example, the concatenation of "snow" and "ball" is "snowball
                string theory, string concatenation is a primitive notion.
            Recreational mathematics

        ch5  
            Jaccard with k-Grams
                Jaccard similarity
                    //
                        k-Grams is just a simple concept and can be used in many algos. 
                        Jaccard measures simularity of tow sets using k gram 
                is a statistic used for comparing the similarity and diversity of sample sets. 
                divided by the size of the union of the sample sets
                If A and B are both empty, we define J(A,B) = 1.
                Intersection over Union
                ratio of the size of the symmetric difference 
                Weighted Jaccard similarity and distance
                Probability Jaccard similarity and distance
                Tanimoto similarity and distance
                Paul Jaccard
                Cosine similarity is for comparing two real-valued vectors, 
                Jaccard similarity is for comparing two binary vectors (sets).



        ch6  
            zipf Law 

        ch7 
        ch8 
        ch9 
            Kappa statistic
                a statistic which measures inter-rater agreement
                it is conceptually simpler to evaluate disagreement between two raters
                The seminal paper introducing kappa as a new technique was published by Jacob Cohen 
                between two raters only,cannot measure the aggrement amoung more than two raters 
                Fleiss' kappa
                The Fleiss kappa, however, is a multi-rater generalization of Scott's pi statistic
                Kappa is also used to compare performance in machine learning
                Statistical significance
                type of intraclass correlation

    ---
    some calculations
        calculations
            do intersection for two postings & -
            pseudocode for skiping list &
            Positional index usage & example p38
            â€œProximityâ€ intersection algo  &
            calcu Soundex & -  Example::Soundex of HERMAN H650 p41
            Successor Variety  &calcu - -e READABLE p28
            ç®€å•è®¡ç®— ubung & 
            ç¼–è¾‘è·ç¦»calcu & 
            Jaccard coefficient to Scoringï¼Œhow? & Query q [ides of March] doc1:"Caesar died in Marchâ€:ïƒ  JACCARD(q, d1) = 1/6
            tf idf calcu & - P28
            tf idf calcu & - ä»p41å¼€å§‹è®¡ç®—
                tf idf algo p46 & - NEXT
            Heapsâ€™ law mean? formular &
            Zipfâ€™s law mean? formular &
                Bytewise compression  & calcu - p34
            åŸºäºå—çš„æ’åºç´¢å¼•æ–¹æ³•Blocked Sort-Based Indexing (BSBI) idea & -  algo & - 
            å†…å­˜å¼å•éæ‰«æç´¢å¼•æ„å»ºæ–¹æ³•Single-Pass In-Memory Indexing (SPIMI) algo & - 
            ch08 p27è®¡ç®— & -
            ch08 æˆ‘çš„ç½‘ä¸Šç…§ç‰‡ä¾‹å­ &è®¡ç®— - NEXT
            Precision and Recall
                è®¡ç®—p10 & -
            Accuracy ï¼šï¼šTP TN å ç”¨çš„æ¯”ä¾‹ 
                å…¬å¼ & -
                calcu &
            p28è®¡ç®— & - ubungä¸­æœ‰å¯¹åº”çš„é¢˜ç›® &
            pptä¸Šè®¡ç®— & p37 - ç»™å®šä¸¤ä¸ªåˆ¤æ–­ï¼Œæ±‚Kappaç»Ÿè®¡
                ç½‘é¡µä¸­é‚£ä¸ªè®¡ç®— & - NEXT
            è®¡ç®— çŠ¶æ€æ£€ç´¢å€¼ Retrievalstatuswert & calcu 
            è®¡ç®—ä¾‹å­ & - p165 ä¾‹12-2
            è¾“å…¥ä¸ºTasse Kanneçš„é‚£ä¸ªè®¡ç®— & - NEXT
            ubungä¸­ &
            ch13 æ¬¡é«˜ä»·æ‹å– & and calcuã€‚ p47
            è®¡ç®—PageRankæƒé‡ & - p322 p23å…¬å¼ & -
            Multinomiales NaÃ¯ve Bayes ï¼šidea & formuar  & calcu &
            ch19 calcu. p16 & 

    ---          
    Ubungen (will be merged
        improtant concepts from Ubung 
            u1 
                intersection algorithm x AND y and x OR y algo &
                How should the Boolean query x AND NOT y be handled &
                    Why is the naive evaluation of this query normally very expensive
                what can we achieve p6 &
                    x AND NOT y    ,  x OR NOT y    x+y    , N 
                For a conjunctive query, is processing postings lists in order of size guaranteed to be optimal? &
            u2
                The tokenization of a document is a trivial task
                stemming can lowers precision
                stemming increase or changed recall
                    // how affects the stemmer precision and recall and vocabulary size  &
                Stemming decreases the size of the vocabulary.
                Stemming should be applied to the documents, but not to the queries  
                The postings list of a stop word is usually longer than the postings list of a non-stop word  

                // what other approaches to stemming x3 &
                What are the respective Levenshtein editing operations x3 types of operations
            u3
                The Jaccard coefficient works well for ranking documents  
                Rare terms are less informative than frequent terms. 
                    // which words are more informasive ? &
                The inverted document frequency (idf) has no effect on the ranking for one-term queries  ! &
                There is exactly one way to calculate the tf-idf weights 

                What minimal and maximal values can the following variables have ? -
                Consider the case of a query term that is not in the set of indexed terms,what do we do ?
                What is the ğ‘–ğ‘‘ğ‘“ of a term that occurs in every document?
            u4  
                Zipf's Law states that the ith most frequent term has a collection frequency proportional to 1Î¤ğ‘–
                Heaps' Law assumes that the vocabulary can grow infinitely !!!
                Elias Gamma Coding is a technique for dictionary compression
                Front Coding is a technique for dictionary compression
                Large document collections contain many frequent and few rare terms !!!
                    // contains few frequent items and many rare items  &
                Gamma coding cannot encode the number zero. Is this a problem for compressing postings lists

            u5
                External sorting algorithms are used to sort lists which do not fit in main memory
                It is impossible to calculate the top-k documents for a query without completely reading 
                    the postings list of each query term   
                        // the idea of NRA ? + calcu. the topk docs. without completely reading the postings list 
                Why can the MapReduce algorithm as described in the lecture not be run in this case? &?? next 
                For optimal load balancing, the inverters in MapReduce must get segmented postings
                    files of similar sizes. For a new collection, the distribution is unknown , what should we do ? &
            ---   
            u6 &
                a) The key measure for a search engine is user happiness.
                b) The ğ¹Î² measure combines both, precision and recall, into one number.
                c) The 11-point interpolated average precision projects the precision-recall curve to a single number.
                d) The Mean Average Precision (MAP) is yet another measure for evaluating the result of one query.
                e) Pooling means experts manually judge the relevance of each document in the collection.
                f) The kappa value is 1 if two judges always agree and 0 if they never agree.
                g) Dynamic result summaries can be constructed efficiently from the positional inverted index

                How many relevant documents does the
                    result contain and at which ranks are they?
                kappa measure formua -
            u7 &
                a) The main motivation for relevance feedback and query expansion is to decrease recall.
                b) The basic idea of the Rocchio algorithm is to move the query vector towards the vectors of
                relevant documents and away from the vectors of irrelevant documents.
                c) Relevance feedback is widely available in web search engines, because users can easily
                understand it and are willing to â€œtuneâ€ their query.
                d) Pseudo-relevance feedback can lead to query drift. !!!
                e) In thesaurus-based query expansion, we add terms to a query which are semantically related to
                the query terms specified by the user.
                f) Thesauri can only be built manually, there is no way to build them automatically.

                In Rocchioâ€˜s algorithm, what weight setting for ğ›¼, ğ›½, ğ›¾ does a â€œFind pages like this oneâ€ search
                    correspond to? &
                Why is positive feedback likely to be more useful than negative feedback to an IR system? &
                Why might only using one non-relevant document be more effective than using several? !!! &
            u8
                a) Probabilistic IR is about estimating the probability that a document is relevant to a query.
                b) The odds of an event ğ´ is defined as ğ‘‚ ğ´ = ğ‘ƒ(ğ´)Î¤ğ‘ƒ(ğ´Ò§) .
                c) The classical Binary Independence Retrieval (BIR) model takes term frequencies into account.
                d) The BIR model assumes that terms appear independently from each other in the documents.
                e) The cluster-hypothesis states, term distributions differ between relevant and irrelevant documents.
                f) The system Okapi was a much simpler predecessor of the BIR Model.

                in RSV calcu. :
                    what can be omitted in order to save time? &
                    Which of the calculations done so far have to be done anew for the new query? &
                    The classical BIR model does not consider term frequencies.
                        Is there a way to take these into account
                        as well?
                derived the formulas for the Retrieval Status Value:  
                    We made the assumption that terms appear independent from each other in a document
                        Provide an example of two terms for which this assumption is likely to hold and not .
                    
            u9 & 
                a) A statistical language model (LM) assigns probabilities to strings of symbols from some alphabet.
                b) For every unigram LM, cats hunt mice and mice hunt cats have the same probability.
                c) Language models are completely unrelated to Markov chains.

                d) We use LMs in IR like this: (step 1) we derive a LM from each document, (step 2) we rank all
                    documents by the probability that their LM generates the query.
                e) From a vocabulary containing ğ‘‰ terms we can construct approximately ğ‘‰ ğ‘› ğ‘›-grams.

                f) A document containing ğ‘‘ tokens contains approximately ğ‘‘ ğ‘› ğ‘›-grams.
                    // contains d vocabularies not tokens
                g) In practice, language models in IR consider ğ‘›-grams with ğ‘› â‰¥ 3, i.e., at least tri-grams.
                h) The shorter the query, the more important is smoothing
                    // longer ? or no relationship ??
            u10
                a) The size of the web can easily be determined by crawling the web and counting the pages.
                b) In the context of web search, information needs are the only subclass of user needs.
                c) Shingling is a technique for detecting near duplicates of web pages.

                d) The average out-degree of all web pages is higher than their average in-degree.
                    why -- may be equal 
                    rem !!! &
                e) The PageRank algorithm ranks web pages by the number of occurrences of the query terms.
                f) The PageRank of a web page is query-dependent.

                g) The HITS-algorithm provides two scores per web page: an authority score and a hub score.
                h) The HITS-scores of a web page are query-dependen

                what happends as Î± becomes close to 1? &
            u11  
                a) Classification is about assigning a document to one (or more) out of several predefined categories.
                b) The accuracy of a classifier is always one minus its error rate.
                c) When using supervised learning for classification, we first train a classifier on unlabeled
                    documents.
                d) Overfitting means that a classifier is too general.
                e) With ğ‘›-times ğ‘˜-fold cross-validation, we partition a set of ğ‘› labeled documents into ğ‘˜ (nearly)
                    equally-sized subsets and for each subset ğ‘†ğ‘– we train a classifier on the documents in ğ‘†ğ‘– and
                    evaluate its performance by applying it to the documents in ğ‘†ğ‘–.
                f) Given a set of labeled documents, the sequential covering algorithm determines a set of rules for
                    rule-based text classification
                g) With probabilistic classification, a document ğ‘‘ is assigned to category ğ‘ğ‘– if the probability ğ‘ƒ ğ‘ğ‘– ğ‘‘)
                    is the maximum of the probabilities ğ‘ƒ ğ‘ğ‘˜ ğ‘‘) for all categories ğ‘ğ‘˜.
                h) Feature selection can reduce the training time of a classifier, but it cannot improve its quality.
                i) The Rocchio approach for vector-based classification assumes that the document vectors in each
                    category are close to each other, but distant from the document vectors in the other categories.
                j) The ğ‘˜NN classifier assigns a doc. ğ‘‘ to the ğ‘˜ categories whose centroid vectors are closest to ğ‘‘Ô¦.
                k) The idea of support vector machines (SVMs) is to separate the vector space using an optimal
                    hyperplane and to assign a document ğ‘‘ to one of two classes depending on whether ğ‘‘Ô¦ lies on the
                    one or on the other side of the hyperplane.
                l) The SVM approach can only be applied to linearly separable datasets

                difference between  Bernoulli and multinomial ? &
                    
        answer to ubung // or can ref. to the answer docs 
            u1 
                calculate (NOT y) first as a new postings list, which takes O(N) time. 
                    where N is the total number of documents in the collection
                bounded by the number of documents 
                not optimal , just a gready method 
                
            u2
                t t t t f t 
            u3
                f f t f 
            u4  
                see ubung pdf 
            u5
                partition by docid as well as term for very frequent terms
                using a machine, to find the distribution of terms starting from various alphabets

            u6
            u7 
                But, in practice, it has been shown to be most useful for increasing recall 
                might not be conveyed properly to the IR system which results in low precision output

            u8
            u9
            u10
            u11   
                multinomial model uses term freq. where Bernoulli uses document freq. to calcu. the probability.

                multinomial calcu. the probability by the number of occurrences of the in the collection 
                The Bernoulli model , on the other hand , just calculates the probability by the number of
                    documents which contain the word 

    ---                
    key concepts: can be found below 
        ----lev 1 Basic Part + Boolean  
            freq. error
                Stemming,wildcard queries,phrase quries,Spelling Correction x3
            ch1 -
                was ist IR? &
                what are unstructured text (usually text)? & -  
                what is data, knowledge and Information &
                IR system architecture::structure of IR system  ç¤ºæ„å›¾ p15  & -
                usage of IR Systems &  
                compare IR and databank & -  x6

            ch2 -
                Why is grep not the solution & p6 -- x3
                To answer the query with help of Term-document incident matrix ï¼Ÿ example & - p8
                why not used? & -
                why inverted ? Inverted Index & 
                what is posting list? & - p11
                posting list å»ºç«‹è¿‡ç¨‹ 3ä¸ªæ­¥éª¤ &
                dicå’Œpostingå¦‚ä½•å­˜å‚¨ &
                What is the best order for processing this query? & 
                how to skip ? &
                why skip lists are not used now  ? & x2
                Westlawç‰¹æ€§ & x2
                how to use Bi-word index to do Phrase Queries &
                Phrase Queries methods x3
                key idea of Extended Bi-words &
                why Bi words not used ? &
                what is Positional index &
                how can position index be used to Proximity search & 
                    advantages of positional index ::
                        1. doesnt require much space 
                        2. can do proximity search   
                what is the Combination scheme of those two  & -
                What are â€œgoodâ€ bi-words & -
            
            ch3
                how to determine lan in one text? & -
                what is & 
                    Term,Morphem,Inflection ,
                    Derivation,Kompositum,Noun Phrase (NP)
                promblems for reading words in IR? & *5
                what is &
                    Grammatical markings(POS Tags),Stemming,
                    Lemmatization,Case Folding,Stop words
                key idea of Sondex & 
                ways of Stemming & x3
                Diceâ€™s coefficient and Jaccard diff. &
                Successor Variety &
                    advantages of it & x2
                n-gram stemmers:  & ::statistics statistical 0.8
                Porter's Stemmer explain & -
                Stemming?or not &
                Dangerâ€™s of stemming &
                Part-of-Speech Tagging what is &
                how can we do Part-of-Speech Tagging & x2
                edit distance & 

            ch4
                è¯å…¸çš„æ•°æ®ç»“æ„what datastructures can be used for dictionaries &
                é€šé…ç¬¦æŸ¥è¯¢::Wildcard queries  x3 & - ä¾‹å­ï¼šï¼š[hel*o] look up  
                Wildcard queries vs Phrase Queries &
                why not used 
                permuted index and kgram æ¯”è¾ƒ & -
                    why not used ?
                Spelling Correction ä¸‰ç§ &
                it was expensive , so how to solve this ? 
                Issues when using Spelling correction  & Problems 
            
        ----lev 2 Basic Rank
            ch5
                compare: rank or not & -
                Why rank & x1
                Jaccard coefficient to Scoringï¼Œhow? & 
                    problems: & -
                    https://datascience.stackexchange.com/questions/15862/how-to-compute-the-jaccard-similarity-in-this-example-jaccard-vs-cosine
                Cosine similarity and Jaccard similarity differ.
                what is tf idf &
                tf-idf weighting , how & 
                tf-idf formular & -
                tf-idf compare & - those two 
                under what condition weight is 0? & x2
                The vector space models before and now  & x3
                why Euclidean distance is a bad idea & -
                differ. weghts &
                    Inc.Itn 
                    Itn.bnn
                    åˆ†åˆ«ä»£è¡¨å•¥æ„æ€ï¼Œå…¬å¼åˆ—å‡ºæ¥   
            ch6
                why compression & x2 -
                what is Lossy vs. lossless compression & - -
                Heapsâ€™ law mean?
                the meaning for IR systems &
                zipf Law mean ï¼Ÿ
                Dictionary as a string & ä¸»è¦saveçš„æ˜¯å“ªé‡Œçš„ç©ºé—´
                why 3B here needed &
                Dictionary as a string with blocking & ä¸»è¦saveçš„æ˜¯å“ªé‡Œçš„ç©ºé—´
                ä¸ºå•¥blockä¸èƒ½å¤ªå¤§ & 
                how to Tradeoff &
                Front coding & 
                how to store gaps & x2
                Variable length code & 
                Variable byte code  &
                Gamma Codes for gap encoding & calcu -p39 13 å››ä¸ªæ­¥éª¤ &
                Gamma:some advantages & - explain - x2
                Gamma seldom used in practice & - explain
                
                
            ch7
                External Memory Sort &
                ç¼©å†™çš„å«ä¹‰ &
                åŸºäºå—çš„æ’åºç´¢å¼•æ–¹æ³•
                    Blocked Sort-Based Indexing (BSBI) idea & -  algo & - 
                å†…å­˜å¼å•éæ‰«æç´¢å¼•æ„å»ºæ–¹æ³•
                    Single-Pass In-Memory Indexing (SPIMI) algo & - 
                Cachingç­–ç•¥ x2 &
                zwei Arten von Rechnerâ€Knoten :  & - two types of nodes 
                advantages or dis- of distributed index construction & - NEXT
                Mathods of Distributing &
                why can parallel? & - 
                MapReduce Architecture & - NEXT 
                Delta-Index: how it works ? & when put the delta index to Disk: &
            
            ch8
                Term-at-a-Time (TAAT): and DAAT &
                NRA ï¼š No Random Accesses function explain & -
                    ç»“æŸæ¡ä»¶ &
                    p29å…¬å¼ & - x5 mink,unseen,worst,best,high(t)

            ch9
                how to Measure the happiness ? & - p5  explain x4 2+2
                Precision and Recall
                    å…¬å¼ p9 & -
                F-Measure Precision/recall tradeoff
                    formula & - p13
                Cost-based Measure & calcu
                Accuracy ï¼šï¼šTP TN å ç”¨çš„æ¯”ä¾‹ 
                    å…¬å¼ & -
                    calcu &
                Macro average (precision) and Micro average (precision) 
                    p19 calcu & å…¬å¼æœ‰é—®é¢˜? è²Œä¼¼æ²¡æœ‰ 
                PR curve å‘ˆé”¯é½¿å½¢çš„åŸå› ? & -
                what are included in benchmarks & x3
                Kappaç»Ÿè®¡ why ? &
                Poolingç­–ç•¥--è¯„ä»·æ‰€æœ‰æ–‡æ¡£å·¥ä½œé‡æ˜¯å¾ˆå¤§çš„ï¼Œå¯ä»¥åªè¯„ä»·æ£€ç´¢çš„ &
                A/B Testing &
                what to descreption in summaries &
                ç”ŸæˆåŠ¨æ€æ‘˜è¦çš„ç›®æ ‡æ˜¯é€‰å‡ºæ»¡è¶³å¦‚ä¸‹æ¡ä»¶çš„ç‰‡æ®µ &  p49


        ----lev 3 Ranking system
            ch10
                four different examples with picture & - for user feedback -next
                idea of Rocchio Algorithm to do feedback & 
                    centroid , opt. query formular &
                    æœ‰å›¾æ¥è§£é‡Šè¿™ä¸ªå…¬å¼ & -ï¼š
                Used in practice &
                    what effect parameter has & -
                Positive versus negative feedback & -
                When Does Relevance Feedback Work? & ??
                Evaluation of feedback:how many times is enough to do that &
                problemsï¼Œä¸èµ·ä½œç”¨çš„åŸå›   & - x3
                ä¸ºç›¸å…³åé¦ˆé—®é¢˜ &
                but it workd well? &
                what can we do to é—´æ¥ç›¸å…³åé¦ˆ &
                æŸ¥è¯¢æ‰©å±•(Query expansion) methods & x3 
                åŸºäºç»Ÿè®¡å­¦çš„ç»“æœï¼Œç›¸å…³æ€§çŸ©é˜µ
                    calcu & - NEXT p48 

            ch11
                äº‹ä»¶çš„ä¼˜åŠ¿ç‡ï¼ˆ oddsï¼‰ & f
                çŠ¶æ€æ£€ç´¢å€¼ Retrievalstatuswert ï¼ŒRetrieval status value &    NEXT TUI 
                    å…¬å¼
                    æ„ä¹‰ & 
                Test auf UnabhÃ¤ngigkeit formula &
                æ¦‚ç‡æ£€ç´¢æ¨¡å‹ idea &
                äºŒå€¼ç‹¬ç«‹æ¨¡å‹BIR & å…¨ç§°
                Okapi formular & calcu & 
                æ–‡æ¡£è¯„åˆ†çºµå‘æ¯”è¾ƒï¼š & -
                æ¦‚ç‡IRä¼˜ç¼ºç‚¹ & - NEXT explain x2 x2 

            ch12
                è¯­è¨€æ¨¡å‹ idea & -
                what is language of LM &
                åŒåœˆèŠ‚ç‚¹å¯¹åº”çš„æ˜¯ &
                æè¿°æœ‰é™è‡ªåŠ¨æœº EA & - explain
                è®¡ç®— P(Katzen fangen MÃ¤use) & - 
                äºŒå…ƒè¯­è¨€æ¨¡å‹ä¸ºå•¥ä¸ç”¨ & - exp
                kann man LM durch Markow-Ketten darstellen & - Horizontï¼ˆå‘å‰çœ‹çš„è§†é‡ï¼‰?
                retrival function p20å…¬å¼ & -
                Jelinek-Mercer Smoothing & -
                Multiplikation kleiner Werte FÃ¼hrt zu Rechenungenauigkeiten & - how to handle &
                LMs vs. BIR x1
                    è”ç³» & - NEXT
                    åŒºåˆ« & - NEXT
                LMs vs. Vector-Space-Modell
                    è”ç³» & - NEXT
                    åŒºåˆ« & - NEXT

            ch13  web basic , +++++++++
                ä¸ºä»€ä¹ˆ Webçº·ç¹æ‚ä¹±ã€å˜åŒ–è¿…é€Ÿ & x1
                web IRç‰¹ç‚¹ & x3
                é™æ€é¡µé¢ä¸åŠ¨æ€  & - exp  
                magure the size of the web & Webè§„æ¨¡ä¼°è®¡ &
                æ•´ä¸ªWebæœ‰å‘å›¾ç»“æ„ï¼Ÿ & - x6
                ä½œå¼Šé—®é¢˜ï¼šå‡ ç§æ–¹æ³•æ’åä½œå¼Šï¼Ÿ & - x2
                googleçš„ç”¨æˆ·ä½“éªŒç‰¹ç‚¹ï¼š & -  x3
                æ™®é€šçš„ Web æœç´¢æŸ¥è¯¢ä¼¼ä¹å¯ä»¥åˆ†æˆå“ªä¸‰å¤§ç±»ï¼Ÿ  & -
                Mercator é‡‡é›†å™¨ äº”ä¸ªæ¨¡å— &
                pptä¸­ä¾‹å­ & - ç†è§£ï¼Œç»™å›¾ç”»å‡ºæ¥è·¯å¾„
                webä¸­çš„é‡å¤é—®é¢˜ & x2
                ç½‘ç»œæœç´¢çš„å…·ä½“å½¢å¼ç‰¹ç‚¹ & 
                ç½‘ç»œæ–‡æ¡£ç‰¹ç‚¹ï¼Œä¸»è¦çš„å››ä¸ªç‰¹æ€§ & -
                Crawlingï¼Œä¸¤ä¸ªç‰¹ç‚¹ï¼Ÿ & 
                How are ads ranked  & - exp x2+n
                æ¬¡é«˜ä»·æ‹å– & and calcuã€‚ p47 --second price auction 
                    Anybody can participate and bid on keywords
                ä¸»è¦çš„ä¸¤ä¸ªé—®é¢˜ 
                    Keyword arbitrage
                    Violation of trademarks
                å¸¸è§çš„SpamæŠ€æœ¯ & x2


            ch14
                é“¾æ¥åˆ†æçš„æ„ä¹‰ & - 
                PageRankè¿‡ç¨‹å‡è®¾ï¼š & - x2
                é¢å‘ä¸»é¢˜çš„ PageRank:ç®€å•åšæ³• &
                å¤„ç†æ··åˆçš„æƒ…å†µï¼Ÿ &
                pagerank ç‰¹ç‚¹ & - x2
                HITSå¾ªç¯è¿­ä»£å…¬å¼ & - 
                    è®¡ç®—ä¼˜åŒ–ï¼Ÿ & - è¶…é“¾å¯¼å‘çš„ä¸»é¢˜æœç´¢
                    idea & - 
                    é—®é¢˜ï¼š  & - x1
                    hits ç‰¹ç‚¹ & - x2

        ----lev 4 ML classification
            ch15  
                åœ¨åƒåœ¾é‚®ä»¶å¤„ç†ä¸­ï¼Œæœ‰å“ªäº›åŸºæœ¬çš„å¤„ç†æ–¹æ³• & p7  x2
                Maschinelles Lernenï¼Œhow it works  &ç”»å›¾ 
                *æ€»ä½“ï¼šåˆ†ç±»çš„è¯„ä»· & x3 + 2
                    basic 
                        Accuracy and  Error Rate
                        Recall, Precision and F1 
                        formula & -
                    erweitern
                        Confusion Matrix  p27 pic ç”»å›¾ 
                        cross validation x3
                *Undersampling , oversampling,Hybrid-Verfahren & -- deal with data skew
                ä¸‹é‡‡æ ·æ–¹æ³•è¯´å‡ºåå­—å°±å¯ä»¥ & x4
                Oversampling & x2 è¯´å‡ºåå­—ï¼Œç®€ç§°ï¼Œå…¨ç§°
                Feature-Scaling x3
                *Feature-Auswahl x3 &

            ch16 -ç®—æ³•ç»†èŠ‚ï¼Œå¦‚ä½•ç»§ç»­ä¼˜åŒ– 
                Form der Regeln  &
                *Evaluation von Regeln
                    Grundlegende MaÃŸe,ä¸¤ä¸ªæŒ‡æ ‡ å…¬å¼ &,è®¡ç®— & p15
                        å¤ªå¤§å¤ªå°ï¼Ÿ
                    Drei QualitÃ¤tsmaÃŸe ,formular &,p21 ?? NEXT
                        combine the two values 
                        eg. & p26
                *Sequential Covering,
                    idea &,
                    formular &,eg p33 &&&,
                    *ç®—æ³• p30 
                    Nachbearbeitung x2+ x2
            ch17 -å…¬å¼æœ‰ç‚¹è´¹åŠ²
                *è´å¶æ–¯åˆ†ç±»ï¼š
                    why NaÃ¯ve & x1
                    why not Naive & x2
                Multinomiales NaÃ¯ve Bayes ï¼šidea & formuar  & calcu & 
                    ä¸‰ä¸ªå±‚çº§çš„å…¬å¼ 
                        *ç¬¬ä¸€å±‚æ¬¡-
                        åŸºæœ¬cj d, p ï¼Œ tk cj ä¼°è®¡ï¼Œ gamma (d) 
                        how to de smoothingï¼Ÿ
                    +ä»£ç å¯¹åº” & æ•°ç»„...
                    +MLå®é™…ä¸Šå­˜å‚¨çš„æ˜¯ä»€ä¹ˆï¼Œå­¦ä¹ çš„æ˜¯å•¥
                    +term freq =1 çš„æ—¶å€™å°±æ˜¯Bernoulli NaÃ¯ve Bayesï¼Ÿ false ! why 
                Bernoulli NaÃ¯ve Bayes ï¼š idea & formuar  & 
                    ä¹Ÿæ˜¯ä¸‰ä¸ªå±‚æ¬¡
                    smoothingï¼Ÿ why + 2 in the denominator 
                    Anmerkungen & x2 advantages 
                Aktuelle Anwendung & x1
            ch18
                *Klassifikation im Vektorraum
                    Vektorraum:vector space
                    exp &
                Rocchio Klassifikator 
                    idea & 
                    formuar  & 
                    *Probleme bei Rocchio & x2
                kNN 
                    process & x2 
                    *Wahl von k? & x2-3
                    å˜ç§ &  x2
                
            ch19 -svm ç»†èŠ‚
                Darstellung Hyperebene mit Ebenengleichung ::p9
                Vorgehen bei mehr als zwei Kategorien ? Multiclass SVMs?,exp & p10 x3steps
                    ::when there are more than two classes x2
                *Trainingsphase:Perzeptron
                    what is R ,ita &
                    verfahren exp & x3steps
                    if learn rate is large ? &
                    formula of perceptron &
                        alpha factor in the formular what is that &
                    dual form &
                    what have we learned ? &
                SVM
                    *SVM Linearer Fall
                        è·ç¦»çš„è¡¨è¾¾å¼ &
                        Einfach Verfahren &
                            formular p30 & -
                                it can be solved with Lagrange multiplier or quadratic  programming
                                    -Well-studied solution algorithms
                                why larger than 1 ? (constraint formula )
                            //Duale Formulierung & -
                        Beispiel p33 &
                        

                    *Erweitertes Optimierungsverfahren
                            using slack variable in SVMï¼šï¼šå¼•å…¥æ¾å¼›å˜é‡
                                formular & p38
                                Einfluss von C exp & -
                                    tradeoff parameter (chosen by cross-validation)
                                what is m :: number of slacks?
                                condition ? &

                    *è¯´åå­—-Kernel Trick--Nichtlineare Entscheidungsgrenzen
                        key idea &
                        kernel types & x4
                            RBF Kernel--GauÃŸâ€˜scher Kernel
                            Polynomial Kernel
                            Sigmoid Kernel
                            Cosinus Similarity Kernel
                            Chi-squared Kernel
                        how to choose ? 10k
        
    -----
    full notes below
        ppt-1-Einfurung --over --c
            was ist IR? 
                Information Retrieval (IR) is 
                    -finding material in
                    -an unstructured text , 
                    -satisfies an information needs
                    -Searches can be based large collections (usually stored on computers)
                personal::
                    everything in our daily life , around us can not be searched without this knowledge/technique
            unstructured nature (usually text)? & - ä¸‰ä¸ªlevel
                public 
                    articles -- 
                        scientific articals 
                        from press news  
                        Webseiten
                --
                Office
                    documents (eg. Spreadsheets)
                    e mails 
                --
                personal 
                    multimedia material (z.B. Fotos und Videos)
                    user information  (z.B. on Facebook)
            information need--InformationsbedÃ¼rfnis des Benutzers
                Vage (z.B. Was kann man in Berlin unternehmen?)
                not PrÃ¤zise Anfragen in (relational en) Datenbanken
            æ¯”è¾ƒï¼ŒAbgrenzung IR und Datenbank & - 
                p31
            GroÃŸe Datenmengen mit rapidem Wachstum
            Typische Anwendung des Text-Retrievals sind heute Web-Suchmaschinen
            Was ist Information? 
                what is Daten, Wissen Information &

                    1.data : coded information that computer can understand   æœ€åŸå­çš„ä¸€ä¸ªæ¦‚å¿µ
                    2.knowledge : data with semantic å¾ˆå¤§çš„ä¸€ä¸ªèŒƒå›´
                    3.informaiton : part of knowledge , that can answer a specific question . useful knowledge  ä»‹äºä¸­é—´

                    1.Datenï¼šMaschinell verarbeitbare, kodierte Informationen å…·æœ‰ç»“æ„çš„æ•°æ® 1999-11-05 ïƒ Zeichenkettenformat: ZZZZ-ZZ-ZZ
                    2.Wissen ist Daten mit Semantikï¼šGesamtheit aller Kenntnisse eines Sachgebietes å‡ å¥è¯ï¼Œå…·æœ‰è¯­ä¹‰ä¿¡æ¯çš„æ•°æ® 
                        "I need one Euro for the car park.â€œ, "I will
                        be home about 6.30 if I can catch the 4.54.
                    3.Information ï¼šTeilmenge des Wissens um spezielle Aufg
                        Information ist nutzbares Wissen   ï¼Œ Wann war ich in Berlin?
            Aufbau eines IR-Systems  ç¤ºæ„å›¾ p15  & 
                    DokumentverarbeitungåŒ…å«ä»€ä¹ˆï¼Ÿ 
                    Anfragen åŒ…å«å•¥ 
                    Matching und Ergebnisliste
                    Feedback
                IRç³»ç»Ÿçš„æ¦‚å¿µæ¨¡å‹æ˜¯ç³»ç»Ÿçš„åŸºæœ¬æ–¹æ³•
                Vektorraummodellå‘é‡ç©ºé—´æ¨¡å‹
                Probabilistisches Retrievalæ¦‚ç‡æ¨¡å‹
                    Wahrscheinlichkeit
            compare IR and databank &
                äº”ä¸ªæ–¹é¢
                    matching , Model , query language - query fault (miss spelling)
                    result 

                    corpus:
                        exact match ... probability accuracy ...
                        natural langu. ... structured lan. .. relevant ranking ... 
                ä¸€ä¸ªé‡ç‚¹:å¯¹è±¡ä¸åŒ
                    æŸ¥è¯¢çš„æ˜¯unstructured text and structured resources 
            Elementare Fragestellungen 
                åŸºç¡€é—®é¢˜ p23 &
                InformationsbedÃ¼rfnis?
                Dokumenteå»ºæ¨¡ï¼Ÿ
                effizient?
                GÃ¼te?
            Anwendungsbeispiele & x2
                Bildersuche
                Suche in Open-Source-Projekten
                Stack Overflow ,Programming QA site
                Soundcloud , Musikempfehlungen
        ppt-2-Bool --over --c
            Dokument ist entweder relevant oder nicht
            Suchbegriffe kÃ¶nnen durch boolsche Operatoren verknÃ¼pft werden å¯ä»¥è¿æ¥
            Boolean Retrieval
                the simplest model
                The search engine returns all documents that satisfy the Boolean expression
                Why is grep not the solution & p6 --
                    can not deal with negation expression 
                    can not support near operations 
                    slow on large collections

                    ï‚§ Slow (for large collections) å¤§é‡æ•°æ®æ—¶å€™å¾ˆæ…¢
                    ï‚§ line-oriented, IR is document-oriented é¢å‘è¡Œçš„æ£€ç´¢
                    ï‚§ can not express negation [NOT CALPURNIA] is non-trivial
                    ï‚§ do not support near operations (e.g., find the word ROMANS near COUNTRYMAN ) not feasible å¯¹äºå…¶ä»–æ“ä½œæ²¡æœ‰æ”¯æŒ
                æå‰ä¸ºæ–‡æ¡£å»ºç«‹ç´¢å¼•
                Term-document incidence matrix 
                    æ˜¯å¦å‡ºç°ï¼Œå‡ºç°å°±æ˜¯1 å¦åˆ™æ˜¯0çš„ä¸€ä¸ªçŸ©é˜µ
                    Entry is 1 if term occurs 
                    Incidence vectors äº‹ä»¶å‘é‡
                        æ¦‚å¿µ : 0/1 vector for each term
                    To answer the query with help of Term-document incident matrix ï¼Ÿ example & -
                        Do a (bitwise) AND on the three vectors
                    why not used? & - 
                        Bigger collections Canâ€™t build the incidence matrix! **
                        needs huge storage 
                        æ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µ
                Inverted Index
                    ideaï¼šWe only record the position of 1s
                    why inverted ? Inverted Index & 
                        Normally, we want to index form docs to terms . But now , we use terms as dictionary and docID as posting list
                        å€’æ’ï¼Ÿä¸€èˆ¬æ˜¯æ–‡æ¡£ç´¢å¼•åˆ°å•è¯çš„ï¼Œè¿™é‡Œæ˜¯ä»è¯é¡¹åå‘æ˜ å°„åˆ°æ–‡æ¡£çš„ For each term t *
                        ::For each term t, we store a list of all documents that contain t
                    postingsï¼Ÿ ç´¢å¼•æ¡ç›®
                    what is posting list? & - p11     
                        For each term t, we store a list of all documents that contain t       
                    posting list å»ºç«‹è¿‡ç¨‹ 3ä¸ªæ­¥éª¤ & p14
                        ç”»å›¾  - 1,2,3,4   4çš„æ­¥éª¤æœ‰ä¸‰ä¸ª
                        Tokenizeå˜æˆè¯æ¡ è¯æ±‡åˆ‡åˆ†
                        Do linguistic preprocessingå¤§å°å†™è½¬æ¢ä¹‹ç±»çš„
                        å®ä¾‹ä¸­ä»…ä»…æ˜¯å¤§å°å†™è½¬æ¢ ï¼Ÿ 
                        Sort posting ï¼Ÿ æŒ‰ç…§å­—æ¯é¡ºåºæ’åº
                        Create postings lists, determine doc freq
                        æ¨ªç€ä¸‰ä¸ªé¡¹ç›®columnï¼šterm doc.freq. posting lists  
                    å…¶ä»–ç»†èŠ‚ï¼š
                        ä¸¤ä¸ªæ–‡ä»¶Split the result into dictionary and postings file
                            dicå’Œpostingå¦‚ä½•å­˜å‚¨ & -
                                ::
                                    we want to keep dictionary in memory 
                                    and store postings as files on disk
                        Size of postings much larger than size of dictionary ïƒ  dictionary is commonly kept in memory, postings on disk
                        åŸç†è§„å¾‹ï¼šå…¶å®æ–‡ç« ä¸­ç”¨è¯å¹¶ä¸å¤šï¼Œäººç±»ç¤¾ä¼šå­¦ç°è±¡
                        How much space do we need for dictionary and index? 
                        index compression: how can we efficiently store and process indexes for large collections?
                Queries 
                    å’Œå»ºç«‹è¿‡ç¨‹ç›¸å¯¹
                        Intersecting two posting lists
                            For each of the terms, get its postings list, then AND them together
                            complicity:This is linear in the length of the postings lists 
                            postings lists should be sorted
                        do intersection for two postings & -
                            ç”»å›¾ä¹‹åå†æè¿°
                            pseudocode 
                            code 
                Query Optimization
                        What is the best order for processing this query? & 
                            Start with the shortest postings list, then keep cutting further
                            more general , Process in increasing order of or sizes
                        Optimized with sort 
                                pseudocode 
                                code 
                            Recallå›å¿†  
                        using skip lists how ? &
                            we do better than this (sub-linear time)?
                                Skip pointers allow us to skip postings that will not figure in the search results
                                This makes intersecting postings lists more efficient
                            Intersection with Skip Pointers 
                                pseudocode & -
                                code 
                                For postings list of length pï¼Œ use æ ¹å·p evenlyspaced skip pointers
                                harder in a dynamic environment because of updates
                                å¦‚æœè¿‡äº†æ€ä¹ˆåŠï¼Ÿ
                                NEXT
                            why skip lists are not used now &
                                1.cpus are faster 
                                2.can be slow when the list is always in changing 
                Boolean Search
                    can answer Boolean expression
                    precise: Document matches condition or notç²¾ç¡®çš„
                    Primary commercial retrieval method for three decadesä¸‰åå¹´æ¥ä¸»è¦é‡‡ç”¨å•†ä¸šæ£€ç´¢æ–¹æ³•
                    Many professional searchers (e.g., lawyers) still like Boolean queries
                    You know exactly what you are getting
                    Many search systems you use are also Boolean: spotlight, email, intranet etc
                        email searching : when you searching an email content ,it use bool search at usual 
                    Westlaw
                        Commercially successful Boolean retrieval: Westlaw
                        Largest commercial legal search service
                        The service was started in 1975
                        ç‰¹æ€§ & -
                            Proximity operators
                            1.Space is disjunction, not conjunction! This was the default in search pre-Google
                            2.Long, precise queries
                                Precision, transparency, control
                            Boolean queries return set of matching documents 
                                order the returned results, scoring function
                            Search for compounds or phrases needs to increase the number of the index
                Phrase Queries å¯¹äºè¯ç»„çš„æ”¯æŒ
                    â€œThe inventor Stanford Ovshinsky never went to universityâ€ should not be a match
                    About 10% of web queries are phrase queries
                    It is no longer suffices to store docIDs in postings lists
                    Two ways of extending the inverted index
                        Bi-word index 
                            explain & -
                                Index every consecutive pair of terms in the text as a phrase
                                Each of these bi-words is now a vocabulary term
                                add to dictionary (larger scale )
                                change the queries to AND form to do Phrase queries:: not only Two-word queries  
                            bi-words as vocabulary terms
                            A long phrase like â€œstanford university palo altoâ€ 
                                --can be divided to some Bi-words with AND between them 
                            can be occasional false positives
                            but:we should do 
                                post-filtering 
                                of hits to identify subset actually 4-word phrase?
                        Extended Bi-words
                                key idea & -
                                    Bucket the terms into  Nouns (N) and Articles/prepositions (X) ***
                                    use it to match larger words , with arbitry length
                                    more efficient as Bi words 
                                Parse each document and perform partof-speech tagging
                                Classify words as nouns, verbs, etc.
                                    Now deem any string of terms of the form
                                    ï¿½ï¿½âˆ—ï¿½ to be an extended bi-word
                            DIS:
                                very large term vocabulary   
                        why Bi words not used ? &
                            1.false positives
                            2.large term vocabulary 
                        what is Positional index &
                            use the positional informaiton to determine a phrase query 
                            commonly used 
                            a more efficient alternative to 
                            Each posting is a docID and a list of positions
                            Example in page 38: & -
                                pay attention to positon: 
                                    in the 4th doc, TO has a position of 16
                                    and BE has a positoin of 17
                                    this means that they are close to each other in doc 4
                                    and it can be used as resault 
                            Proximity search :: k word proximity search ä¸´è¿‘æ£€ç´¢
                                E.g. Find all documents that contain EMPLOYMENT and PLACE within 4 words of each other
                                +stop words: a the in or ....
                                â€œProximityâ€ intersection algo 
                                    pseudocode & -
                        Combination scheme
                            how & -
                                Include frequent bi-words as vocabulary terms in the index
                                Do all other phrases by positional intersection
                            extremely frequent :: Bi-words
                            Many bi-words are extremely frequent: â€œMichael Jacksonâ€, â€œBarack Obamaâ€ etc
                            For these bi-words, increased speed compared to positional postings intersection is substantial
                            What are â€œgoodâ€ bi-words & -
                                Phrase where the 
                                    individual words are common 
                                    but::the desired phrase is comparatively rare
                                        explain as follow :
                                        Adding â€œBarack Obamaâ€ as a phrase index entry may
                                            only give a speedup factor to that query of
                                        about 3 (most documents that mention either
                                            word are valid results) whereasâ€¦
                                        - â€¦adding â€œThe Whoâ€ as a phrase index entry may speed up that query by a factor of 1,000
        ppt-3-Getting Term --over --c
            How to get terms out of documents
                challenges
                    General and Non-english
                        What format is it in (pdf, word, excel, html etc.)?
                        Reading direction: from left to right, right to left, column-wise
                        What character set is in use? - UTF-8, ASCII, ISO-8859-1,
                        determine lan? & -
                            use frequent words (German: der, die, das, und, ein, einer,
                            Englisch: the, a, and, or, one)
                        What is the document unit
                            Answering the question â€œwhat is a document?â€ is not trivial 
                        Definitions  
                            Term,Morphem,Inflection & - NEXT
                            Derivation:Forming a new word from an existing word
                            Kompositum: consists of more than one stem è¯å¹²
                            Noun Phrase (NP):noun as its head word
                    Tokenization problems
                        string into words or tokens
                        Needed to apply further processing, e.g. stemming and lemmatization
                        promblems for reading word? & - NEXT
                        promblems for reading words in IR? & *5
                            //
                                numbers 
                                no white space between words.
                                diff. meaning of words . eg Bush 
                            Short Forms
                            Orthographic word: string of characters with â€˜whitespaceâ€™ at each end;
                            Word form: have, has, had, having are word forms of the lemma have
                            One word or two or several
                                ï‚§ Hewlett-Packard
                                ï‚§ State-of-the-art
                                ï‚§ Co-education
                            Numbers
                                ï‚§ 3/20/91
                                ï‚§ 20/3/91
                                ï‚§ Mar 20, 1991
                            Apostrophes can be 
                                part of a word, 
                                a part of a possesive 
                                just a mistake
                            Capitalized words: different meanings
                            chinese:No whitespaces between words
                            chinese:Ambiguous segmentation
                            japanese:End user can express query entirely in hiragana!
                    Normalization
                        in indexed text as well as query terms into the same form
                            eg:We want to match U.S.A. and USA
                        we can 
                            do asymmetric expansion,but less efficient
                            Normalization and language detection interact
                        some Terminology & -
                            Grammatical markings
                                bakes is a (grammatically) inflected form
                            Stemming:æ‰¾è¯å¹²
                                algorithms work by cutting off the end or the beginning of the word
                            Lemmatizationï¼šè¯å½¢è¿˜åŸï¼Œå•å¤æ•° 
                                æ›´åŠ å®Œæ•´ï¼Œä¸ä¼šä»…ä»…å»æ‰å‰ç¼€
                            Case Folding
                                Reduce all letters to lower case
                            Stop words = extremely common words
                        More Classing techï¼Ÿ
                            phonetic equivalentsï¼šBeijing and Peking
                            Thesauriï¼šSemantic equivalence, car = automobile
                            calcu Soundex & -  
                                æ ¹æ®è¯­æ³•è§„åˆ™å†™ä¸€ä¸²æ•°å­—
                                è€Œåæ˜¯ä¸ªstepçš„ç¼©å‡å·¥ä½œï¼ˆæ¶ˆé™¤é‡å¤ï¼Œï¼Œï¼‰
                    stemming algorithms
                        ï‚§ Table lookup approach 
                            &explain -
                            root forms and inflected forms wrote in a table 
                            Building lookup tables is very labour-intensive
                            High probability that these tables may miss out some exceptional cases
                        ï‚§ Successor Variety 
                            idea 
                                a method to do stemming .
                                find the boundaries of the words with the help of the Varieties of the suffix .
                            &calcu -
                            distribution of phonemes 
                            Successor variety of substrings of a term will decrease as more characters are added ----> boundry is reached 
                            based on Corpus to find characters following 
                            see Pic P28
                            æ˜¯è¯å¹²çš„æ ‡å¿—ï¼šsharply increase **
                            Example
                                ï‚§ Test Word: READABLE
                                ï‚§ Corpus: ABLE, APE, BEATABLE, FIXABLE, READ,
                                READABLE, READING, READS, RED, ROPE, RIPE
                        ï‚§ n-gram stemmers
                            based on shared unique n-grams 
                                to calculate Association measures 
                            Diceâ€™s coefficient Diceâ€™s coefficient (similarity)
                            &calcu -
                            å®ç°stemming: **
                                Once such a similarity matrix is available
                                terms are clustered using a single link clustering method
                        ï‚§ Affix Removal Stemmers
                        Porter's Stemmer
                            idea 
                                use a rule table to remove suffix in words
                            explain & -
                            åŸç†å¾ˆç®€å•ï¼Œå…¶å®å°±æ˜¯ä¸€ä¸ª rule table **
                            Most used in IR, probably because of its
                                balance between simplicity and accuracy
                                (1980)
                            the defacto standard algorithm used for English stemming
                            Porter extended his work by developing Snowball, a
                                framework for writing stemming algorithms 
                                http://snowball.tartarus.org
                            Rules are composed of a pattern (left) and a string (right side)
                            
                            alternate vowel-consonant sequences
                            5 phases of reduction rules applied sequentially **
                                Step 1 deals with plurals and past participles
                                Steps 2 to 5 deal with English-specific suffixes
                                ç®€å•è®¡ç®—
                                    éœ€è¦çŸ¥é“CVCVå…¬å¼
                                    éœ€è¦çŸ¥é“mæ•°å€¼è¿›è¡Œåˆ¤æ–­
                                    è§ä¾‹é¢˜ & -
                    Lemmatization
                        to base form
                            am, are, is â†’ be
                        Inflectional morphology vs. derivational morphology æ›²æŠ˜å½¢æ€å­¦vsæ´¾ç”Ÿå½¢æ€å­¦
                        ä¸ºå•¥ä¸ºæ£€ç´¢å¸¦æ¥çš„å¥½å¤„éå¸¸æœ‰é™ & - NEXT
                        vs Stemmer:: 
                            Stm just cutting affix.
                            Lemmatization adds some alphabets.
                    Stemming?or not &
                        ä¸€èˆ¬æ¥è¯´ï¼Œè¯å¹²åŒ–ä¼šæé«˜æŸäº›æŸ¥è¯¢çš„æ•ˆç‡ï¼Œè€Œé™ä½å…¶ä»–æŸ¥è¯¢çš„æ•ˆç‡
                        Benefits of stemming depend on the language å±ˆæŠ˜è¯­æ›´æœ‰å¥½å¤„
                        Dangerâ€™s of stemmingï¼šï¼š[information retrieval] vs. [information on Golden Retrievers] é”™è¯¯å¾—å¤„ç†äº†ä¸€äº›æƒ…å†µ
                            increses the probability of false positiv
                    Part-of-Speech Tagging &
                        Labeling each word 
                        Different approaches
                            ï‚§ Rule-based Tagger
                            ï‚§ Stochastic POS Tagger
                            - Simplest stochastic Tagger
                                Each word is assigned its most frequent tag 
                                Learning from examples : we can training it 
                                WSJ (Wall Street Journal) in the Penn Treebank around 1.2 Million tokens
                                Wall Street Journal corpus:p42 as a example 
                                    a conditional probability = 0.019
                                    ç»™ä¸€ä¸ªæ¡ä»¶æ¦‚ç‡è¡¨ï¼Œè¿›è¡ŒPOS & -
                            - HMM Tagger
                    ç¼–è¾‘è·ç¦»
                        https://nlp.stanford.edu/IR-book/html/htmledition/img152.png
                        å†™å‡ºä¼ªä»£ç å°±éƒ½æ‡‚äº† & -
                            åŠ¨æ€è§„åˆ’ä¿è¯æœ€åæ˜¯æœ€ä¼˜çš„
                            æ¨ªå‘å˜åŠ¨æ˜¯delæ“ä½œï¼Œçºµå‘å˜åŠ¨æ˜¯insertæ“ä½œ
                            è¦ä¼šæ ¹æ®è¡¨æ ¼å†™å…·ä½“æ“ä½œ**
                                ä¸€å…±å››ç§æ“ä½œ
                            ppt4-p21
        ppt-4-Tolerant Retrieval  --over --c
            +BG 
                å‰é¢çš„å·¥ä½œæ— æ³•æ”¯æŒetst æŸ¥è¯¢
                å®¹é”™å¼æ£€ç´¢ä¹Ÿéœ€è¦æ”¯æŒä»»æ„å¤šå­—ç¬¦åŒ¹é…* &
                k-gram ç´¢å¼•ç»“æ„å¦‚ä½•æ”¯æŒé€šé…ç¬¦çš„æ£€ç´¢
                è¯å…¸çš„æ•°æ®ç»“æ„ &
                    å¯ä»¥é‡‡å–å‰ç¼€æ•° 
                    ä¸ºäº†åŠ å¿«å¯¹å­—å…¸çš„æ£€ç´¢é€Ÿåº¦
                    https://blog.csdn.net/lsjseu/article/details/12234769
                    ç¤ºæ„å›¾
                ç”¨æˆ·æœ‰æ—¶å€™å¯¹å•è¯çš„ä¸ç¡®å®šæ€§ï¼Œå¯èƒ½ä¼šè¾“å…¥ä¸€äº›é€šé…ç¬¦æ“ä½œ
                    é‡‡ç”¨Bæ ‘
                        å­—ç¬¦ä¸²å‰ç¼€æœç´¢ä¸€ç›´æ˜¯Bæ ‘çš„ä¼˜ç‚¹ï¼Œè€Œå­—ç¬¦ä¸²åç¼€æœç´¢ï¼Œ
                        æˆ‘ä»¬å¯ä»¥é‡‡ç”¨åå‘Bæ ‘ï¼Œé‚£ä¹ˆå¯¹äºè¿™ç§ab*cdï¼Œé‚£ä¹ˆæˆ‘å°±å¯ä»¥é€šè¿‡Bæ ‘å’Œåå‘Bæ ‘çš„ç»“åˆï¼Œæœ€åå–ä¸€ä¸ªäº¤é›†
                    è½®æ’ç´¢å¼•è§£å†³æ–¹æ¡ˆ
                        å¯¹ä¸€ä¸ªå•è¯å»ºç«‹è½®æ’ç´¢å¼•ã€‚å‡å¦‚æœ‰helloï¼Œæˆ‘ä»¬å»ºç«‹ä¸‹åˆ—è½®æ’å•è¯è¡¨ï¼Œè€ƒè™‘å„ç§æƒ…å†µ
                        è½®æ’ç´¢å¼•çš„ç¼ºç‚¹å¾ˆæ˜¾ç„¶ï¼šè¯å…¸é›†åˆå¤ªå¤§ã€‚
                    kgram
                        ä¸¾ä¸ªä¾‹å­ï¼Œå¯¹äºä¸€ä¸ªå•è¯castleï¼Œå®ƒçš„3-gramåŒ…æ‹¬ï¼š$caã€casã€astã€stlå’Œtl$ã€‚ï¼ˆåœ¨å¼€å§‹å’Œç»“å°¾å¤„æ·»åŠ $ï¼‰
                        åœ¨k-gramä¸­ï¼Œå…¶è¯å…¸ç”±è¯æ±‡è¡¨ä¸­æ‰€æœ‰å•è¯çš„k-gramå½¢å¼ç»„æˆã€‚æœ€åä¸æ“ä½œä¸€ä¸‹ï¼Œæ³¨æ„ä¸‹å›¾å¼•å…¥äº†ä¸€ä¸ªåè¿‡æ»¤çš„è¿‡ç¨‹ï¼ˆå‰”é™¤ä¸€äº›ä¸æ»¡è¶³æ¡ä»¶çš„å•è¯å§ï¼‰ã€‚
                        k-gramç´¢å¼•å¯èƒ½ä¼šè¿”å›å¾ˆå¤šä¼ªæ­£ä¾‹ï¼Œéœ€è¦åšè¿‡æ»¤å¤„ç†ã€‚
                    æ„Ÿè§‰æœ€å¥½é‡‡ç”¨bæ ‘
                ç”¨æˆ·å¯èƒ½å­˜åœ¨é”™è¯¯è¾“å…¥ï¼Œè¿™æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ç”¨æˆ·çš„è¾“å…¥ç»™å‡ºä¸€äº›æ ¡æ­£çš„æç¤ºæ–¹æ¡ˆ
                    edit distance ::
                        idea of it :
                            measure the steps converting from one word to an other.
                            -based on inserting or deleting methods 
                    https://blog.csdn.net/lsjseu/article/details/12234769
                    åŸºäºç¼–è¾‘è·ç¦»çš„æ ¡æ­£
                        è¿ç”¨DP &
                        åœ¨é”®ç›˜ä¸ŠæŠŠâ€œaâ€æ•²æˆâ€œsâ€çš„å¯èƒ½æ€§å¤§äºï¼ŒæŠŠâ€œaâ€æ•²æˆâ€œuâ€ï¼Œå› ä¸ºaå’Œsé å¾—å¾ˆè¿‘ï¼Œ
                            è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨æ±‚å­—ç¬¦ä¸²ç¼–è¾‘è·ç¦»çš„æ—¶å€™ï¼Œç»™å®šä¸åŒçš„æƒé‡ã€‚
                        å› ä¸ºæˆ‘ä»¬è¦åŒå­—å…¸çš„æ¯ä¸ªå•è¯è¿›è¡Œæ±‚ç¼–è¾‘è·ç¦»ã€‚è¿™æ˜¯æˆ‘ä»¬
                            å°±å¯ä»¥é€šè¿‡ç»™å®šä¸€äº›é¢å¤–çš„æ¡ä»¶æ¥å‡å°‘åŒ¹é…å•è¯çš„ä¸ªæ•°ï¼Œæ¯”å¦‚æˆ‘ä»¬å‡è®¾é¦–å­—æ¯è‚¯å®šè¦å¯¹ã€‚
                    k-gramçš„æ ¡æ­£æ–¹æ³•
                    ä¸Šä¸‹æ–‡æ•æ„Ÿçš„æ‹¼å†™æŠ€æœ¯
                    åŸºäºå‘éŸ³æŠ€æœ¯çš„æ ¡æ­£
                        åŸºäºå‘éŸ³çš„æ ¡æ­£æŠ€æœ¯ï¼ˆSoundexç®—æ³•ï¼‰

            ---------------------------------------------------------------------------------------
            Dictionaries
                æ•°æ®ç»“æ„ï¼Ÿ & -
                    Some IR systems use hashes, some use trees 
                    ä¸¤è€…å¦‚ä½•é€‰ç”¨ï¼Ÿ
                    will it keep growingã€‚ã€‚ã€‚
                hash:
                    Each vocabulary term is hashed into an integer
                    Lookup in a hash is faster than lookup in a tree
                    BUT:
                        No prefix, infix or suffix search 
                        no tolerant retrieval
                        Need to rehash everything periodically if vocabulary keeps growing
                tree:
                    B-trees mitigate the rebalancing problem
                    Search is slightly slower than in hashes: O(logM), only on balanced trees 
            é€šé…ç¬¦æŸ¥è¯¢Wildcard queries
                1.using B-tree immedately ::intersect
                    ä¸‰ç§æƒ…å†µ & -
                        Trailing wildcard query-----B-tree
                        Leading wildcard query-----inverse B-tree
                        middle ------intersect the two term sets 
                    +btree 
                        å¤šè·¯æœç´¢æ ‘
                        æµ·é‡æ•°æ®æœç´¢
                        ç£ç›˜ï¼Œæ•°æ®åº“
                    
                2.permuted index è½®æ’ç´¢å¼•
                    åŸç† & -
                        Store each of these rotations in the dictionary, in a B-tree 
                        hello--å¢åŠ 4ä¸ªterm
                        add terms to dictionary 
                    where $ is a special symbol
                    Problem : quadruples the size of the dictionary
                        compared to a regular B-tree (empirical number)
                    Permuterm index doesnâ€™t require post-filtering
                    [hel*o] look up & -   ****
                        [o$hel*]
                        å…·ä½“æŸ¥è¯¢è¦ä¼šç”¨xè¡¨ç¤º & -
                3.kgram 
                    More space-efficient than permuterm index
                    ä¾‹å­ & -
                        Query [mon*] can now be run as [$m AND mo AND on]  ****
                        do postings-filter
                        but also many â€œfalse positivesâ€ like â€œMoonâ€ 
                        change the queries 
                    execute a large number of Boolean queries
                æ¯”è¾ƒ & -
                    k-gram index should do postings filter
                    permuted index tasks huge anount of spaces to store dic.
            Spelling Correction
                26%: Web queries (Wang et al. 2003)
                Correcting documents and queries
                The general philosophy in IR is: do not change the documents
                we use the the smallest distance to the misspelled word
                using :A standard dictionary (Websterâ€™s, OED etc.)
                basic operations that convert s1 to s2
                based on Distance 
                    Edit distance 
                        The edit distance between string s1 and
                            string s2 is the minimum number of basic
                            operations that convert s1 to s2
                        +Damerau-Levenshtein 
                            includes transposition as a fourth possible operation
                        Computation & -
                        Algorithm & -
                    Weighted edit distance
                        Typewriter distanceï¼šåŸºäºé”®ç›˜
                        Confusion Matrix for Spelling Errors p31 
                        Key Idea: we seek the string(s) in S of least edit distance from q
                    Problem:
                        Computing the edit distance from q to each string in S is inordinately expensive
                Invoke the k-gram index to assist --with low edit distance to the query
                    åŸºäºkgramçš„æ‹¼å†™æ£€æŸ¥ & -
                    1.å°†queryæ‹†åˆ†kgram 
                    2.æ ¹æ®kgramé˜ˆå€¼åšäº¤å‰ ï¼š only vocabulary terms that differ by at most 3 k-grams p33
                    3.then, Jaccard coefficient 
                        Declare a match if ğ½ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘ ğ‘, ğ‘¡ > ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘
                Context-sensitive spelling correction
                    hit-based spelling correction--åŸºäºä»¥å¾€æŸ¥è¯¢
                        The correct query â€œflew from munichâ€ has the most hits
                        BUTï¼š we have to test 7 Ã— 20 Ã— 3 different variants -- too many 
                        SOLVE: we can log queries
                Issues when using Spelling correction 
                    & - 
                    Spelling correction is potentially expensive
                    Avoid running on every query?
                        Maybe just on queries that match few documents
                how to write 
                    & - NEXT
                    Spelling corrector in 21 lines Python code
                    Peter Norvig: How to write a spelling corrector (http://norvig.com/spell-correct.html)
            Soundex
                key idea of Sondex & 
                    for each word , we can calcu. a Sondex code ,
                        and we can use this code to check the similarity of their pronunciation
                Especially applicable to searches on the names of people **
                Soundex algorithm 
                    è½¬æ¢æˆå››ä½ä»£ç 
                    è®¡ç®— & -
                    words with same pronunciation will generate the same code 
                    åŸç† 
                        Vowels are viewed as interchangeable **
                        Consonants with similar sounds (e.g., D and T) are put in equivalence classes
                        ï¼šå‘éŸ³ç±»ä¼¼çš„è¾…éŸ³æ”¾åœ¨ä¸€ä¸ªç­‰ä»·ç±»ä¸­
                        Better alternatives for phonetic matching in IR
        ----!!constructing!!
        ppt-5-Scoring, Term Weighting --over --c
            +btree:æ˜¯ä¸€ç§è‡ªå¹³è¡¡çš„æ ‘ï¼Œèƒ½å¤Ÿä¿æŒæ•°æ®æœ‰åº
                Bæ ‘ï¼Œæ¦‚æ‹¬æ¥è¯´æ˜¯ä¸€ä¸ªä¸€èˆ¬åŒ–çš„äºŒå‰æŸ¥æ‰¾æ ‘ï¼ˆbinary search treeï¼‰ä¸€ä¸ªèŠ‚ç‚¹å¯ä»¥æ‹¥æœ‰æœ€å°‘2ä¸ªå­èŠ‚ç‚¹
                Bæ ‘é€‚ç”¨äºè¯»å†™ç›¸å¯¹å¤§çš„æ•°æ®å—çš„å­˜å‚¨ç³»ç»Ÿï¼Œä¾‹å¦‚ç£ç›˜ã€‚Bæ ‘å‡å°‘å®šä½è®°å½•æ—¶æ‰€ç»å†çš„ä¸­é—´è¿‡ç¨‹ï¼Œä»è€ŒåŠ å¿«å­˜å–é€Ÿåº¦ã€‚
                Bæ ‘è¿™ç§æ•°æ®ç»“æ„å¯ä»¥ç”¨æ¥æè¿°å¤–éƒ¨å­˜å‚¨ã€‚è¿™ç§æ•°æ®ç»“æ„å¸¸è¢«åº”ç”¨åœ¨æ•°æ®åº“å’Œæ–‡ä»¶ç³»ç»Ÿçš„å®ç°ä¸Š
            
            compare: & -
                    Boolean--
                        too many results::Most users donâ€™t want to wade through 1000s of results
                            Feast or famine
                            Not good for the majority of users
                            itâ€™s too much work
                        skillsï¼šï¼ša lot of skill to come up with a query 
                    Why rank &ranked--
                        With ranking, large result sets are not an issue
                            Doesnâ€™t overwhelm the user
                        doesnt need too many skills 
                            more relevant results are ranked higher than less relevant results
            the most basic approach to scoring -- Jaccard coefficient to Scoring
                äº¤é›†å¤„ä»¥å¹¶é›†åˆï¼ˆä¸ªæ•°ï¼‰
                calcu  & -
                    one query : [ides of March]
                    and two docs
                problems: & -
                    Normalization with æ ¹å·
                    it Doesnâ€™t consider term freq
                    it Doesnâ€™t consider how rare items
            verbesserung : tf-idf weighting how & as vectors
                    what is tf idf &
                    basic calculations  
                        BOW:Bag of Words : do not consider order 
                        tf: number that t in doc d 
                            count vector
                            formular & -
                                p19
                            sum over terms to Score for a document-query pair
                            The score is 0 if none of the query terms found in d
                            challenge & -
                                Relevance does not increase proportionally with term frequency
                            ->should be promoted with log function 
                            we can score it only using the tf .
                        idf : number of docs that t occurs in 
                            where comes from the idea ? 
                                explain & - p21
                            ->should be used with log func 
                            to â€œdampenâ€ the effect of idf
                            calcu p27 & -
                            idf affects the ranking of documents for queries with at least two terms
                            idf has no effect on ranking for one-term queries
                            why not collection frequency ? & -
                                Which word is a better search term (and should get a higher weight)?
                                that means , this term should appear in some docs with more freq and less freq in others 
                                so we can combine the collection freq at the same time to get a better score for queries  
                        we use the log10 transformation for both term frequency and document frequency
                        tf-idf:Best known weighting schema in information retrieval
                            Combines local (term frequency) and global (inverse document frequency) weights with product 
                                of its tf weight and its idf weight
                            character 
                                ï‚§ Increases with the number of occurrences within a document (term frequency)
                                ï‚§ Increases with the rarity of the term in the collection (inverse document frequency)
                                ï‚§ Is 0 if tf = 0 or df = N
                        calculate scores 
                            Assign a tf-idf weight for each term t in each document d
                            vectors are generated
                            calcu & - P28
                        tf-idf compare &
                            idf is a global weight whereas tf is a local weight
                    The vector space model
                        at first : 0/1 
                        then: æ¬¡æ•° count 
                        now : tf-idf weights 
                            document vectors with weights

                        it was sparse too ,most entries are zero
                        we should rank relevant documents higher than non-relevant documents
                        Vector space similarity
                            why Euclidean distance is a bad idea & -
                                because Euclidean distance is large for vectors of different lengths
                                Use angle instead of distance
                                è·ç¦»è¿‘ä½†æ˜¯ä¸ç›¸ä¼¼ï¼Œçœ‹è§’åº¦æ›´åˆç†
                            should be calculated with angle cos 
                            normalization
                            Cosine similarity : equivalently, the cosine of the angle between
                            after calculations , we can get Cosine Metric
                            calcu & - ä»p41å¼€å§‹è®¡ç®—
                    Practical implemention
                        algo p46 & - NEXT
                    Variant tf-idf functions
                        tf 
                            augmented
                        df 
                            prob idf
                        Normalization
                            pivoted unique
                            byte size
                    tf-idf example: Inc.Itn 
                        use different weightings for queries and documents & -
                        è®¡ç®—é¢˜ä¸­ä½¿ç”¨ltn.bnnè®¡ç®— 
                        NEXT
                            åˆ†åˆ«æ˜¯ï¼šï¼š term document normalization
                            term::log boolean(b)
                            doc:: idf(t) none(n)
                            normal:: cosine(c) none(n) 
        ppt-6-index Compression --over --c
            +corpus 
                unary code -
                    key steps : calculate offset and length and combine them as one code 
                    unary code can be applied to any distribution 
                Ï’ codes always has odd length
                Gamma code is universal


            ---
            effizient Dokumente findet?
            compress the postings component
            Hardware    
                Disk I/O is block-based
                Block sizes: 8 KB to 256 KB
            WHY ? & -
                Use less disk space ïƒ  saves money
                Keep more stuff in memory - increases speed
                    Decompression algorithms should be fast
            Lossy vs. lossless compression & -
                Discard some information in Losy compression
                All information is preserved in lossless
                    we mostly do in index compression
                    index--- dictionary + posting lists
            Term statistics
                Heapsâ€™ law
                    Estimates vocabulary size M as a function of collection size number:
                    M is the size of the vocabulary, T is the number of tokens in the collection
                    Typical values for the parameters k and b are: 30 â‰¤ k â‰¤ 100 and b â‰ˆ 0.5
                    Heapsâ€™ law is linear in log-log space (line with a slop of about 0.5)
                    it was a Empirical law
                    formular & -
                    Tokens can repeat 
                    For Reuters : das functioniert
                    fit is good in general
                    including numbers and spelling errors affect the result 
                    it means & -
                        no maximum vocabulary size reached    
                        the size of the dictionary is quite large for large collections
                Zipfâ€™s law
                    formular & -
                    it means & -
                    In natural language, there are a few very frequent terms and many very rare terms
                    formular 
            Dictionary compression
                at first :for Reuters: (20+4+4) Ã— 400,000 = 11.2 MB
                resolve Fixed-width bad 
                    explain & -
                        Most of the bytes in the term column are wasted
                        We canâ€™t handle larger strings
                        Average length of a term in English: 8 characters 
                    2.Dictionary as a string & -
                        ç”»å›¾ **
                        need to know how to caklcu 3 here 
                        400,000 Ã— (4 + 4 + 3 + 8) = 7.6 MB 
                    3.Dictionary as a string with blocking & -
                        We eliminate k âˆ’ 1 term pointers
                        We save 4 Ã— 3 âˆ’ (3 + 4 Ã— 1) = 5 bytes per block
                        Total savings: 400,000/4 Â· 5 = 0.5 MB
                        from 7.6 MB to 7.1 MB
                        blockä¸å¯ä»¥å¤ªå¤§çš„åŸå›  & -
                            Slightly slower
                            we should Tradeoff between compression and the speed of term lookup
                    4.Front coding & -
                        A sequence of terms with identical prefix (e.g. â€œautomatâ€) 
                        Consecutive entries: common prefixes
                        the first byte of each entry encodes the number of characters
                        with blocking and front coding: 5.9MB
                        å†™å‡ºç¤ºæ„ä¸²ä¸² & -
                            æ³¨æ„ä¸¤ä¸ªç¬¦å·ä¸åŒï¼
                            è¢«å¿˜äº†æ•°å­—ï¼
            Postings compression
                The postings file is much larger than the dictionary
                For Reuters (800,000 documents), we would use 32 bits per docID when using 4-byte integers
                Alternatively, we can use log2 800,000 â‰ˆ 19.6 < 20 bits per docID
                goal : less than 20 bits per docID 
                åŸºæœ¬æ€è·¯ï¼šusing gaps 
                    Postings for frequent terms are close together
                    Gaps between postings are short
                    Postings list using gaps: COMPUTER - 283154, 5, 43, â€¦
                Variable length encoding 
                    we need a variable encoding method that uses fewer bits for short gaps
                    Two solutions
                        ï‚§ Bytewise compression - Variable length encoding 
                            Dedicate 1 bit (high bit) to be a continuation bit c
                            At the end c=1 others c=0
                            Postings are stored as a byte concatenation see P34
                            still : For a small gap (e.g. 5) VB use a whole byte this can be promoted later 
                            the algorithm & - NEXT
                            exmaple p34 & -
                            do not sensitiv to computer memory alignment matches 
                        ï‚§ Bitwise compression - Elias gamma encoding 
                            Gamma Codes for gap encoding
                            Unary code ï¼šRepresent n as n 1s with a final 0  & -
                                3= 1110
                            Gamma Codes 
                                calcu & -
                                    13? p39
                                    a pair of length and offset
                                    13 â†’ 1101 â†’ 101 = offset
                                    Encode length in unary code: 1110
                                    length = 1110 and offset = 101 --ã€‹ 1110101
                                    Length of the entire code is 2 x âŒŠlog2 GâŒ‹ + 1 bits
                                    Ï’ codes are always of odd length
                                    steps :
                                        1.write as Binary code 
                                        2.calcu. offset 
                                        3.calcu. len of offset as unary
                                        4.combine them together 
                                some advantages & - explain -
                                    3.prefix-free
                                    2.optimal within a factor of 2
                                    1.universal
                                        independent of the distribution of gaps
                                        it is parameter-free
                                    
                                Gamma seldom used in practice & - explain
                                    1.it is not aligned and thus potentially less efficient
                                postings, gamma encoded 101MB
            p42 å›é¡¾
            less reality ? NEXT
        ppt-7-Index Construction --over --c
            +corpus 
                    -distributed index construct system
                how can we construct index efficiently ?
                Performance characteristics typical of systems in 2007
                to motivate IR system
                Access to data in memory is much faster than access to data on disk
                    -we can access the data faster when we keep it in mem. 
                takes a few clock cycles    
                    -when we want ot access data on disk , a few clock cycles can be taken  
                    -it takes a few clock cycles to access it 
                We call the technique of keeping frequently used disk data in main memory caching
                    -the cashing technique will be used in this expriment. 
                if it is stored as one chunk
                    -when the data are localted as the same chunk , ...
                Operating systems generally read and write entire blocks. Thus, reading 
                    a single byte from disk can take as much time as reading the entire block.
                    -it takes the same time to ...
                We call the part of main memory where a block being read or written is stored a buffer
                    -buffer is part of 


            å»ºç«‹å€’æ’ç´¢å¼•einen invertierten Index dafÃ¼r erstellen
            ç´¢å¼•æ„å»ºç®—æ³•çš„è®¾è®¡å—ç¡¬ä»¶çš„é…ç½®æ‰€åˆ¶çº¦
            ç»ƒä¹ é¢˜ç›®å’Œè¿™ä¸ªä¸æ˜¯å¾ˆå¤§å…³ç³»ï¼Œæ‰€ä»¥æ›´è¦çœ‹pptï¼ï¼ï¼ï¼ï¼
            Static Indexing
                æ“ä½œç³»ç»Ÿå¾€å¾€ä»¥æ•°æ®å—ä¸ºå•ä½è¿›è¡Œè¯»å†™ã€‚å› æ­¤ï¼Œä»ç£ç›˜è¯»å–ä¸€ä¸ªå­—èŠ‚å’Œè¯»å–ä¸€ä¸ªæ•°æ®å—
                    æ‰€è€—è´¹çš„æ—¶é—´å¯èƒ½ä¸€æ ·å¤šã€‚æ•°æ®å—çš„å¤§å°é€šå¸¸ä¸º 8 KBã€ 16 KBã€ 32 KB æˆ– 64 KBã€‚æˆ‘ä»¬
                    å°†å†…å­˜ä¸­ä¿å­˜è¯»å†™å—çš„é‚£å—åŒºåŸŸç§°ä¸ºç¼“å†²åŒºï¼ˆ bufferï¼‰
                NaÃ¯ve Indexierung
                    Termvorkommen im Hauptspeicher halten und dort sortieren
                    Wie groÃŸ darf eine Dokumentensammlung sein,mit 8 GB Hauptspeicher
                        calcu & - NEXT
                        134.216 Dokumente 13 wan< 80 wan
                        unter Zuhilfenahme von SekundÃ¤rspeicher
                        Sortieren mit SekundÃ¤rspeicher 
                            Zahl der Ebene ? & 
                    External Memory Sort &
                åŸºäºå—çš„æ’åºç´¢å¼•æ–¹æ³•Blocked Sort-Based Indexing (BSBI)
                    idea & - NEXT
                        ç”»å›¾
                        1.åˆ†å—è¯»å…¥
                        2.sortierte Teilfolgen von Termvorkommen,als
                            Ebene 0 von External Memory Sort in zweitem Durchlauf
                        3.æœ€åä¸€æ­¥ï¼š
                            å†…å­˜ä¸­ç»´æŠ¤äº†ä¸º 10 ä¸ªå—å‡†å¤‡çš„è¯»ç¼“å†²åŒºå’Œä¸€ä¸ªä¸ºæœ€ç»ˆåˆå¹¶ç´¢å¼•å‡†å¤‡çš„å†™ç¼“å†²åŒº
                            æ¯æ¬¡è¿­ä»£ä¸­ï¼Œåˆ©ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆå³å †ç»“æ„ï¼‰æˆ–è€…ç±»ä¼¼çš„æ•°æ®ç»“æ„é€‰æ‹©æœ€å°çš„æœªå¤„ç†è¯é¡¹ ID (term id)è¿›è¡Œå¤„ç†
                            åˆå¹¶ç»“æœå†™å›ç£ç›˜ä¸­
                            éœ€è¦æ—¶ï¼Œå†æ¬¡ä»æ–‡ä»¶ä¸­è¯»å…¥æ•°æ®åˆ°æ¯ä¸ªè¯»ç¼“å†²åŒºã€‚
                        algo & - NEXT
                    å®é™…ä¸­çš„å¾ˆå¤šè¯­æ–™åº“è¿œæ¯”è¿™ä¸ªè¯­æ–™åº“è¦å¤§ï¼Œå°±éœ€è¦è¿™ç§æŠ€æœ¯å»ºç«‹ç´¢å¼•
                    é—®é¢˜ï¼š
                        ä½†æ˜¯éœ€è¦ä¸€ç§å°†è¯é¡¹æ˜ å°„æˆå…¶ ID çš„æ•°æ®ç»“æ„ã€‚
                        ä¸­é—´æ–‡ä»¶å¾ˆå¤§ intermediate files
                å†…å­˜å¼å•éæ‰«æç´¢å¼•æ„å»ºæ–¹æ³•Single-Pass In-Memory Indexing (SPIMI)
                    ä¸€ç§æ›´å…·æ‰©å±•æ€§çš„ç®—æ³•
                    posting list åŠ¨æ€å¢é•¿çš„ä¸€ç§ç­–ç•¥ï¼Œåœ¨å†…å­˜æ»¡äº†ä»¥åï¼Œæ’åºå¹¶å†™å…¥ç£ç›˜ã€‚åç»­åˆå¹¶ä¸based on block are the same 
                    pseudocode & - NEXT
                    ä¼˜åŠ¿ï¼š
                        èŠ‚çœäº†idå­˜å‚¨çš„å¼€é”€
                        èŠ‚çœå†…å­˜
                    ä¸ºä½¿å€’æ’è®°å½•è¡¨æŒ‰ç…§è¯å…¸é¡ºåºæ’åºæ¥åŠ å¿«æœ€åçš„åˆå¹¶è¿‡ç¨‹ï¼Œè¦å¯¹è¯é¡¹è¿›è¡Œæ’åºæ“ä½œï¼ˆç¨‹åºç¬¬ 11 è¡Œï¼‰
                    ç”±äºäº‹å…ˆå¹¶ä¸çŸ¥é“æ¯ä¸ªè¯é¡¹çš„å€’æ’è®°å½•è¡¨å¤§å°ï¼Œç®—æ³•ä¸€å¼€å§‹ä¼šåˆ†é…ä¸€ä¸ªè¾ƒå°çš„å€’æ’è®°å½•è¡¨ç©ºé—´ï¼Œæ¯æ¬¡å½“è¯¥ç©ºé—´æ”¾æ»¡çš„æ—¶å€™ï¼Œå°±ä¼šç”³è¯·åŠ å€çš„ç©ºé—´
                    å‹ç¼©å¯¹äºè¿™ç§æ˜¯æœ‰æ•ˆçš„
                    ç®—æ³•å¤æ‚åº¦æ˜¯çº¿æ€§çš„å…³äºTï¼špairs that can be held into main memory
                    assign a small space at first . Then, dopple it when necessary.
                    Fill main memory as a block
                Caching
                    reduziert dadurch die Antwortzeiten eines IR-Systems
                    & -
                    ï‚§ Least Recently Used (LRU) schafft Platz fÃ¼r neues Element, indem es das am lÃ¤ngsten unbenutzte
                        Element entfernt
                    ï‚§ Least Frequently Used (LFU) schafft Platz fÃ¼r neues Element, indem es das am seltensten benutzte
                        Element entfernt
                    Cache-Hit-Ratioï¼š die aus dem Cache beantwortet werden kÃ¶nnen
                Verteilte IR-Systeme
                    Fargen:
                        auf mehreren Rechnern abzulegen
                        invertierten Index schneller aufzubauen
                    zwei Arten von Rechnerâ€Knoten :  & -
                        Master und Slaves 
                        Other structures :Grid
                    Mathods of Distributing ***
                        Term-Partitionierter:
                            jeder Rechner-Knoten speichert -- eine Teilmenge der Terme
                        Dokument-Partitionierter:
                            jeder Rechner-Knoten speichert -- Teilmenge der Dokumente
                        compare and see the advantages or dis- & - NEXT
                        Vorteil und Nachteil nennen 
                            Term:
                                NEXT
                            Document:
                                å¦‚æœSlaveå®•æœºï¼Œå‡ ä¹ä¸å—å½±å“
                                å’Œæ¯ä¸ªSlaveéƒ½è¦äº¤æµ
                    parallelisierbar? & - NEXT Grund dafur  explain
                        Ohne Probleme parallel auf verschiedenen Rechnern stattfindenï¼Ÿ 
                        Grund nennen -- P39
                    BUT: & - explain
                        Character of a distributed computer: 
                            cluster Can unpredictably slow down or fail
                    Master machine assigns each task to an idle machine from a pool
                    MapReduce programming model
                        users modify map and a reduce function
                        MapReduce Architecture & - NEXT
                        NEXT 
                        MapReduceæ˜¯å¯ä»¥ä¸²è”çš„
            åŠ¨æ€ç´¢å¼•æ„å»ºæ–¹æ³•
                How to keep the index up-to-date as the collection changes
                Drei Methode 
                    1.NaÃ¯ver Ansatz
                        jedes neue anzupassen 
                        Problem:
                            Lange Zugriffszeit
                    2.Re-Indexierung
                        RegelmÃ¤ÃŸige vollstÃ¤ndiger Neuaufbau 
                        Problem :
                            Es geht nur FÃ¼r kleine oder wenig dynamische Dokumentensammlungen
                            Ineffizienz
                            hoche Speicherbedarf
                    3.Delta-Index
                        -r
                            zusÃ¤tzlichen invertierten Index
                            in an additional inverted index store 
                            use this additional index to deal with queries 
                            put the delta index to disk when necessary 
                            or do it regularly

                        beschreibung:
                            in einem zusÃ¤tzlichen invertierten Index im Hauptspeicher
                            GelÃ¶schte Dokumente werden in einer Liste
                            WÃ¤hrend der Anfragebearbeitung : Haupt- und Deltaâ€Index verschmolzen
                            put the delta index to Disk:
                                gesamt n Terme in einem Document und T ist die Lange des Indexes 
                            bis zu T/n mal miteinander verschmolzen?
                                Complxity &
                        Logarithmisches Verschmel
                            put the delta index to Disk: with a flexible length 2^m*n
                            NEXT
        ppt-8-QueryProcessing --over --c
            å¦‚ä½•åŠ é€Ÿæ¯”è¾ƒå‡ºTop-kçš„ç®—æ³•
            å®é™…ä¸Šçš„æ¯”è¾ƒè¿‡ç¨‹,åˆ©ç”¨ç´¯åŠ å™¨
            å½“æ—¶åœ¨å›¾ä¹¦é¦†çœ‹æ‡‚çš„
            Wie kann man effizient :
                wert ermittelnï¼šå¯ä»¥åˆ©ç”¨ç´¯åŠ å™¨ï¼Œä¼˜å…ˆé˜Ÿåˆ—åŠ é€Ÿè¿™ä¸ªè¿‡ç¨‹
                die Top-k Dokumente bestimmen
            ä¸€è¡Œä¸€è¡ŒTermè¯»å–ï¼Œä½†æ˜¯è¿™é‡Œæœªå¿…è¦è¯»åˆ°æœ€å
            1.Term-at-a-Time (TAAT): die in Vorlesung 5 gegeben 
                ä¸€è¡Œä¸€è¡ŒTermè¯»å–ï¼Œä½†æ˜¯è¿™é‡Œæœªå¿…è¦è¯»åˆ°æœ€å
                ä¸ç”¨æ’åº
                å¿…é¡»å…¨è¯»å®Œ
                p8å›é¡¾è®¡ç®—
                calcu & -

            2.DAAT
                ä¸€æ¬¡ä¸€ä¸ªDocument
                p10å›é¡¾è®¡ç®—
                calcu & -
            NRA ï¼š No Random Accesses
                explain & -
                    NRA ist ein allgemeines Verfahren
                    frÃ¼hzeitig beenden  and korrekten Top-k Dokumenten finden
                    monotonen Aggregationsfunktionen få•è°ƒèšåˆå‡½æ•°
                posting listéœ€è¦æ’åºé™åº
                æœ€å·®ï¼šå½“å‰æ•°å€¼
                best(d)ï¼šå½“å‰æ•°å€¼+éå½“å‰docçš„åˆ—çš„ä¸¤ä¸ª/nä¸ªæ•°å€¼ä¹‹å’Œ 
                unseen = å½“å‰åˆ—æ•°å€¼ä¹‹å’Œ
                æ¡ä»¶ï¼š
                    unseen â‰¤ mink: æœªçŸ¥çš„éƒ½ä¸è¡Œäº†ï¼Œç°åœ¨å·²ç»å‡ºæ¥topkäº†
                    best â‰¤ minkï¼šå½“å‰docä¸è¡Œäº†ï¼Œä¸å¯èƒ½å…¥é€‰äº†
                p27è®¡ç®— & -
                p29å…¬å¼ & -
                æˆ‘çš„ç½‘ä¸Šç…§ç‰‡ä¾‹å­ &è®¡ç®— - NEXT
        ppt-9-Evaluation --over --c
            Unranked evaluation
                ç»™å®šæµ‹è¯•é›†ï¼š
                ï‚§ User Happiness
                    However, the key measure for a search engine is user happiness
                    how to Measure the happiness ? & - p5  explain
                        individual users:
                            relevant quality
                            speed of response time ,
                            size of index, 
                            uncluttered UI

                        for Web search engine:
                        for Ecommerce:
                        ...
                ï‚§ Precision and Recall
                    explain & - NEXT explain
                        æ­£ç¡®ç‡ï¼šç»“æœä¸­ç›¸å…³æ–‡æ¡£æ‰€å çš„æ¯”ä¾‹ï¼Œæ˜¯ä¸æ˜¯æœ‰ä¸ç›¸å…³çš„ï¼Ÿ
                        å¬å›ç‡ï¼šæ˜¯æ‰€æœ‰ç›¸å…³éƒ½è¿”å›äº†å—ï¼Ÿè¿”å›çš„ç›¸å…³æ–‡æ¡£å æ‰€æœ‰ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
                    Web æ£€ç´¢ç”¨æˆ·å¸Œæœ›ç¬¬ä¸€é¡µçš„æ‰€æœ‰çš„ç»“æœéƒ½æ˜¯ç›¸å…³çš„ï¼Œ
                        ä¹Ÿå°±æ˜¯è¯´ä»–ä»¬éå¸¸å…³æ³¨é«˜æ­£ç¡®ç‡ï¼Œ
                        è€Œå¯¹æ˜¯å¦è¿”å›æ‰€æœ‰çš„ç›¸å…³æ–‡æ¡£å¹¶æ²¡æœ‰å¤ªå¤§çš„å…´è¶£
                    ç›¸ååœ°ï¼Œä¸€äº›ä¸“ä¸šçš„æœç´¢äººå£«ï¼ˆå¦‚å¾‹å¸ˆåŠ©æ‰‹ã€æƒ…æŠ¥åˆ†æå¸ˆç­‰ï¼‰
                        å´å¾€å¾€é‡è§†é«˜å¬å›ç‡ï¼Œæœ‰
                        æ—¶ç”šè‡³å®æ„¿å¿å—æä½çš„æ­£ç¡®ç‡ä¹Ÿè¦è·å¾—é«˜çš„å¬å›ç‡
                    å¯¹æœ¬æœºç¡¬ç›˜è¿›è¡Œæœç´¢çš„ä¸ªäººç”¨æˆ·ä¹Ÿå¸¸å¸¸å…³æ³¨å¬å›ç‡
                    å…¬å¼ p9 & -
                    è®¡ç®—p10 & -
                ï‚§ F-Measure
                    Precision/recall tradeoff
                    å°±æ˜¯ä¸€ä¸ªè°ƒå’Œå¹³å‡å€¼ï¼Œå…¬å¼ & -
                    increase recall by returning more documents
                    A system that returns all documents has 100% recall!
                    Itâ€™s easy to get high precision for very low recall
                    use harmonic mean
                    ä¸€ä¸ªèåˆäº†æ­£ç¡®ç‡å’Œå¬å›ç‡çš„æŒ‡æ ‡æ˜¯ F å€¼ï¼ˆ F measureï¼‰ï¼Œå®ƒæ˜¯æ­£ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼
                        é»˜è®¤æƒ…å†µä¸‹ï¼Œæ­£ç¡®ç‡å’Œå¬å›ç‡çš„æƒé‡ç›¸ç­‰
                        Î² < 1 è¡¨ç¤ºå¼ºè°ƒæ­£ç¡®ç‡ï¼Œè€Œ Î² > 1 è¡¨ç¤ºå¼ºè°ƒå¬å›ç‡
                        Values of Î² < 1 emphasize precision, while values
                            of Î² > 1 emphasize recall
                        ä¸ºä»€ä¹ˆä½¿ç”¨è°ƒå’Œå¹³å‡è€Œä¸æ˜¯å…¶ä»–ç®€å•çš„å¹³å‡æ–¹æ³•ï¼ˆå¦‚ç®—æœ¯å¹³å‡ï¼‰æ¥è®¡ç®— F å€¼å‘¢ & - NEXT
                            æ²¡æœ‰æœ€å¤§æœ€å°æ•°å€¼é™åˆ¶
                            è°ƒå’Œå¹³å‡å€¼å¾€å¾€å°äºç®—æœ¯å¹³å‡å’Œå‡ ä½•å¹³å‡å€¼ï¼Œå¹¶ä¸”å¸¸å¸¸ä¸ä¸¤ä¸ªæ•°çš„è¾ƒå°å€¼æ›´æ¥è¿‘
                            a kind of smooth minimum
                        Example & - p13
                ï‚§ Cost-based Measure
                    As an alternative to precision and recall
                    åƒåœ¾é‚®ä»¶çš„è®¡ç®—ï¼ŒåŠ æƒçš„å’Œ
                    example & -
                ï‚§ Accuracy
                    ç²¾ç¡®ç‡ï¼šæ ¹æ®ä¸Šè¿°ä¸¤ä¸ªï¼Œä»¥åŠè¡¨æ ¼ï¼šç²¾ç¡®ç‡æŒ‡æ ‡åœ¨å¾ˆå¤šæœºå™¨å­¦ä¹ é—®é¢˜ä¸­çš„ä½¿ç”¨éå¸¸æ™®é
                            ä½†æ˜¯ï¼šä¸å‡è¡¡æ€§ï¼Œæ¯”å¦‚é€šå¸¸æƒ…å†µä¸‹ï¼Œè¶…è¿‡ 99.9%çš„æ–‡æ¡£éƒ½æ˜¯ä¸ç›¸å…³æ–‡æ¡£ã€‚
                            TP TN å ç”¨çš„æ¯”ä¾‹ å…¬å¼ & -
                            Why is accuracy not a useful measure
                                Normally over 99.99% of the documents are in the non-relevant category
                                explain &
                            é—®é¢˜bugï¼šHow to build a 99.9999% accurate search 
                                engine on a low budget
                                ï‚§ The Snoogle search engine 
                                below always returns 0 results (â€œ0 matching results foundâ€),
                                regardless of the query
                Macro average (precision) and Micro average (precision) 
                    p19 calcu & å…¬å¼æœ‰é—®é¢˜  
            Ranked evaluation
                ï‚§ Precision-Recall-Curve
                    ROC Curve
                    ROC æ›²çº¿é€šå¸¸èµ·äºå·¦ä¸‹è§’è€Œé€æ¸å‘å³ä¸Šè§’å»¶ä¼¸ã€‚ä¸€ä¸ªå¥½çš„ç³»ç»Ÿï¼Œæ›²çº¿å›¾çš„å·¦éƒ¨ä¼šæ¯”è¾ƒé™¡å³­
                    åœ¨å¾ˆå¤šé¢†åŸŸï¼Œä¸€ä¸ªæ™®éä½¿ç”¨çš„æŒ‡æ ‡æ˜¯è®¡ç®— ROC æ›²çº¿ä¸‹çš„é¢ç§¯
                    è¿‘å¹´æ¥ï¼Œå¾€å¾€åº”ç”¨åœ¨åŸºäºæœºå™¨å­¦ä¹ çš„æ’åºæ–¹æ³•ä¸­ï¼ˆå‚è€ƒ 15.4 èŠ‚ï¼‰çš„æŒ‡æ ‡â€”â€”
                    CGï¼ˆ cumulative gainï¼Œç´¯ç§¯å¢ç›Šï¼‰
                    first k elements in the result list
                    For each such sets (with size k), 
                        precision and recall values can be plotted
                    å‘ˆé”¯é½¿å½¢çš„åŸå› ? & -
                        k+1ç¯‡æ–‡æ¡£ä¸ç›¸å…³ï¼Œå¬å›ç‡ä¸å˜ï¼Œæ­£ç¡®ç‡ä¸‹é™
                        å¦‚æœè¿”å›çš„ç¬¬(k+1)ç¯‡æ–‡æ¡£ç›¸å…³ï¼Œé‚£ä¹ˆæ­£ç¡®ç‡å’Œå¬å›ç‡éƒ½ä¼šå¢å¤§

                        in top-k hits ::ã€€
                            assume that k+1 is not relevant .
                            from k to k+1 , recall will not be changed , but precision has decreased
                ï‚§ 11-point interpolated average precision
                    å®šä¹‰ä¸€ä¸ª 11 ç‚¹æ’å€¼å¹³å‡æ­£ç¡®ç‡ï¼Œ
                        ç”¨äºæµ“ç¼©ä¿¡æ¯ ***
                    transform this information down to a few numbers
                    Each point corresponds to a result for the top k ranked hits
                    éå†recallè·å¾—precision è€Œåå–å¹³å‡å€¼ ***
                from multiple queries ï¼Ÿæœ‰çš„information needå®¹æ˜“æœ‰çš„éš¾
                    TRECä¸­æœ€å¸¸è§„çš„æŒ‡æ ‡æ˜¯MAPï¼ˆ mean average precisionï¼Œå¹³å‡æ­£ç¡®ç‡å‡å€¼ï¼‰
                    MAPè¢«è¯æ˜å…·æœ‰éå¸¸å¥½çš„åŒºåˆ«æ€§ï¼ˆ discriminationï¼‰å’Œç¨³å®šæ€§ï¼ˆ stabilityï¼‰
                    MAPå¯ä»¥ç²—ç•¥åœ°è®¤ä¸ºæ˜¯æŸä¸ªæŸ¥è¯¢é›†åˆå¯¹åº”çš„å¤šæ¡æ­£ç¡®ç‡â€”å¬å›ç‡æ›²çº¿ä¸‹é¢ç§¯çš„å¹³å‡å€¼ã€‚
                    MAP: Average of several average precision values
                        Gold standardï¼Ÿï¼Ÿ
                            å¯èƒ½å°±æ˜¯ä¼°è®¡æœ‰50%æ˜¯ç›¸å…³çš„
                            ä¹Ÿå°±æ˜¯è¯´è¿™äº›åˆ¤å®šæ„æˆæ‰€è°“çš„â€œ æ ‡å‡†ç­”æ¡ˆâ€ é›†åˆ** ä¸ä¸€å®šå¤šå°‘ï¼Œæ˜¯é¢˜ç›®æ¡ä»¶
                        p28è®¡ç®— & -
            Evaluation benchmarks
                to mwasure effectiveness
                ï‚§ Standard relevance benchmarks
                    what is that actually & -
                        documents
                        information needs
                        Human relevance assessments
                    eg
                        TREC:1.89 million documents, mainly newswire articles, 
                            450 information needs
                        Five largest classes in the Reuters-21578
                è¯„ä»·è¿‡ç¨‹
                    Poolingç­–ç•¥--è¯„ä»·æ‰€æœ‰æ–‡æ¡£å·¥ä½œé‡æ˜¯å¾ˆå¤§çš„ï¼Œå¯ä»¥åªè¯„ä»·æ£€ç´¢çš„
                    ç»™å®šä¿¡æ¯éœ€æ±‚é›†åŠæ–‡æ¡£é›†ï¼Œéœ€è¦ç»™å‡ºå®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§åˆ¤å®šæƒ…å†µï¼Œè¿™æ˜¯ä¸€é¡¹éœ€è¦äººå·¥å‚ä¸
                    çš„è´¹æ—¶è´¹åŠ›çš„å·¥ä½œ
                ï‚§ relevance assessmentséœ€è¦ä¸€è‡´æ€§--Kappa statistics,è®¡ç®—judgeæ˜¯ä¸æ˜¯å˜äº†ï¼Ÿ
                    measure consistency .
                    Kappa åªæ˜¯ä¸€ä¸ªç»Ÿè®¡é‡ï¼Œå­˜åœ¨æŠ½æ ·è¯¯å·®ã€‚
                    ç»Ÿè®¡å­¦ï¼Œä¸€è‡´æ€§æ£€éªŒ
                    http://sofasofa.io/forum_main_post.php?postid=1000321&
                    xiangqin ä¾‹å­ 
                    pptä¸Šè®¡ç®— & p37 - ç»™å®šä¸¤ä¸ªåˆ¤æ–­ï¼Œæ±‚Kappaç»Ÿè®¡
                    ç½‘é¡µä¸­é‚£ä¸ªè®¡ç®— & - NEXT
                    Relevance assessments are only usable if they are consistent
                    As a rule of thumbï¼š 0.8 0.67ï¼Ÿ
                    Marginal relevanceï¼Ÿ 
                        æœ€å¤§è¾¹ç•Œç›¸å…³æ³•ï¼ˆMaximal Marginal Relevanceï¼‰ & - NEXT
                ASK Consumer
                Consumer (Data) Science
                    Ongoing studies of user behavior in the lab
                    åˆ©ç”¨ç”¨æˆ·ä¿¡æ¯è¿›è¡Œé»‘ç›’æµ‹è¯•
                    ï‚§ A/B Testing
                        two versions of the same product
                        å…¶å®å°±æ˜¯ä¸¤ä¸ªç‰ˆæœ¬çš„äº§å“
                        ä¾‹å¦‚å¥¥å·´é©¬çš„ç«é€‰å‹Ÿæç½‘ç«™ã€‚è¿™ä¸ªç½‘ç«™æœ€æ ¸å¿ƒçš„ç›®æ ‡æ˜¯ï¼šè®©ç½‘ç«™çš„è®¿å®¢å®Œæˆæ³¨å†Œå¹¶å‹Ÿæç«é€‰èµ„é‡‘
                        è¿™ä¸ªå›¢é˜Ÿå½“æ—¶åšäº†ä¸€ä¸ªéå¸¸æˆåŠŸçš„å®éªŒï¼šé€šè¿‡å¯¹6ä¸ªä¸åŒé£æ ¼çš„ä¸»é¡µè¿›è¡ŒABæµ‹è¯•ï¼Œ
                            æœ€ä¼˜çš„ç‰ˆæœ¬å°†ç½‘ç«™æ³¨å†Œè½¬åŒ–ç‡æå‡äº†40.6%ï¼Œ
                            è€Œè¿™40.6%çš„æ–°å¢ç”¨æˆ·ç›´æ¥å¸¦æ¥äº†é¢å¤–çš„5700ä¸‡ç¾é‡‘å‹Ÿæèµ„é‡‘
                        A/Bæµ‹è¯•å¸¦æ¥çš„æ”¶ç›Šä¼šè¿œé«˜äºA/Bæµ‹è¯•çš„å®æ–½æˆæœ¬
                        â€œå‡è®¾æŠŠæ³¨å†Œæµç¨‹ä¸­çš„å›¾ç‰‡æ ¡éªŒç æ–¹å¼ï¼Œæ”¹æˆçŸ­ä¿¡æ ¡éªŒç çš„æ–¹å¼ï¼Œæˆ‘ä»¬çš„æ³¨å†Œè½¬åŒ–ç‡å¯èƒ½æå‡10%â€ã€‚
                        åŸºäºè¿™ä¸ªå‡è®¾ï¼Œæˆ‘ä»¬ä¼šè®¾è®¡å¯¹åº”çš„A/Bæµ‹è¯•
                        widget variations
                    ï‚§ Interjudge agreement
                    ï‚§ Query Logs
                        Logs can be used to tune / evaluate search engines
                        Aggregate clicks to reduce noise 
                        Click deviation &é¿å…éšæœºç‚¹å‡»ï¼Ÿ
            Result summaries
                Presenting Results
                    as a list with discreption
                    what to descreption? & -
                        document title, url, some metadata
                    Two basic kinds result Summaries
                        static
                            NLPï¼ˆ natural language processingï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰é¢†åŸŸä¸­å­˜åœ¨å¤§é‡æ›´å¥½çš„æ–‡æœ¬æ‘˜è¦æ–¹æ³•ã€‚
                            åœ¨æ›´å¤æ‚çš„ NLP æ–¹æ³•ä¸­ï¼Œç³»ç»Ÿå¯ä»¥é€šè¿‡è‡ªåŠ¨å…¨æ–‡ç”Ÿæˆ
                                æ–¹å¼æˆ–è€…å¯¹åŸæ–‡æ¡£ä¸­å¥å­è¿›è¡Œç¼–è¾‘æˆ–ç»„åˆçš„æ–¹æ³•æ¥è‡ªåŠ¨ç”Ÿæˆæ‘˜è¦å¥å­
                            basic method: a subset of the document
                            better: Emphasizing sentences with key terms
                            or: complex NLP to synthesize/generate a summary
                        dynamic 
                            é€šå¸¸è¿™äº›çª—å£ä¼šåŒ…å«ä¸€ä¸ªæˆ–è€…å¤šä¸ªæŸ¥è¯¢è¯é¡¹
                            åŠ¨æ€ä¸­æœ‰é™æ€å†…å®¹éƒ¨åˆ†
                            keyword-in-context (KWIC) snippets
                            è§£å†³è®¡ç®—å¤æ‚é—®é¢˜ï¼šï¼šé¢„å…ˆè¿›è¡Œç£ç›˜ç¼“å­˜
                            ç”ŸæˆåŠ¨æ€æ‘˜è¦çš„ç›®æ ‡æ˜¯é€‰å‡ºæ»¡è¶³å¦‚ä¸‹æ¡ä»¶çš„ç‰‡æ®µ & - NEXT p49
                                (i) åœ¨æ–‡æ¡£ä¸­æœ€å¤§é™åº¦åœ°åŒ…æ‹¬è¿™äº›è¯é¡¹çš„ä¿¡æ¯ï¼›should contain several of the query terms as many as possible
                                (ii) å†…å®¹è¶³å¤Ÿå®Œæ•´ï¼Œæ–¹ä¾¿ç”¨æˆ·é˜…è¯»ç†è§£ï¼›as a phrase,should be complete sentences
                                (iii) è¶³å¤ŸçŸ­ï¼Œæ»¡è¶³æ‘˜è¦åœ¨ç©ºé—´ä¸Šçš„ä¸¥æ ¼é™åˆ¶ã€‚short enough , beacause the space is limited 
                            Summary should answer the query, so we donâ€™t 
                                have to look at the document
                            Detail?
                                cannot construct a dynamic summary from the positional inverted index 
                                è¶…è¿‡å›ºå®šå‰ç¼€é•¿åº¦çš„æ–‡æ¡£åœ¨äº§ç”ŸåŠ¨æ€æ‘˜è¦æ—¶åªåŸºäºæ–‡æ¡£å‰ç¼€æ¥å®ç°
                                +NEXT
                            We need to cache documents
        -----!!improving!!ä¿¡æ¯æ£€ç´¢ä¸­æœ€é‡è¦çš„ç‰ˆå—----æ–‡æ¡£è¯„åˆ†ï¼Œè¯„åˆ†å˜æ¢ï¼Œé’ˆå¯¹ä¸åŒçš„ç³»ç»Ÿè¯„åˆ†
        ppt-10-Relevance Feedback and Query Expansion ç¬¬ 9 ç«  ç›¸å…³åé¦ˆåŠæŸ¥è¯¢æ‰©å±• --over --c
            a quick review:
                ç”¨æˆ·å¯¹ç›¸å…³æ€§ç»™å‡ºåé¦ˆæ„è§
                é’ˆå¯¹ä¸€ä¹‰å¤šè¯ç°è±¡ **
            ç›¸å…³åé¦ˆ--â€œlocalâ€(relevance feedback)
                é€šè¿‡æŸ¥è¯¢çš„åˆå§‹åŒ¹é…æ–‡æ¡£å¯¹åŸå§‹æŸ¥è¯¢è¿›è¡Œä¿®æ”¹
                ç›¸å…³åé¦ˆ
                    ç”¨æˆ·ä¼šå¯¹åˆæ¬¡æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§ç»™å‡ºåé¦ˆæ„è§
                    æäº¤åé¦ˆåçš„æ£€ç´¢ç»“æœï¼Œå…¶æ­£ç¡®ç‡å¾—åˆ°æ˜¾è‘—æé«˜
                    Rocchio ç®—æ³•æ˜¯ç›¸å…³åé¦ˆå®ç°ä¸­çš„ä¸€ä¸ªç»å…¸ç®—æ³•
                    åŸºäºæ¦‚ç‡çš„ç›¸å…³åé¦ˆæ–¹æ³•
                    Ad hoc retrievalè‡ªç»„ç»‡æ£€ç´¢
                    four different examples with picture & -
                    åæ¥æ’åå‘å‰äº† **
                    document : term vector 
                    ï‚§ Rocchio Algorithm
                        based on the vector space model
                        key idea & -
                            find a new query vector : The optimal query vector  :
                            make the  similarity with relevant documents  bigger ! 
                            change the q vector !!?
                        origionally , the query vector does not separate 
                            relevant / non-relevant ***
                        formular & - p17 NEXT
                            Dr and Dnr: 
                                sets of known relevant and 
                                non-relevant documents respectively
                            Why parameter & -
                                If we have a lot of judged documents, 
                                we want a higher Î²/Î³
                        æœ‰å›¾æ¥è§£é‡Šè¿™ä¸ªå…¬å¼ & -ï¼šä¸¤ä¸ªå›¾ p29
                            å‡è®¾: we find document besed on query 
                            vector and the docs returned åœ¨ q 
                            ä¸ºä¸­å¿ƒçš„åœ†ä¸­ï¼š è®¡ç®—ç›¸ä¼¼åº¦çš„æ—¶å€™æ˜¯æ¬§æ°è·ç¦»ï¼Ÿ 
                        it separates relevant and non-relevant documents p28
                    ï‚§ Positive versus negative feedback & -
                        Positive feedback is more valuable than negative feedback ***
                        Many systems (e.g. image search at the beginning) 
                        allow only positive feedback ïƒ  is Î³ = 0
                        Most IR systems set Î³ < Î²
                    ï‚§ Assumptions / Evaluation / Problems
                        Assumptions åº”è¯¥åšå“ªäº›å‡è®¾ï¼Ÿ & - NEXT
                            sufficient knowledge to be able to make an initial query
                                can be various reasons why initial query may fail 
                                æ˜¯ä»€ä¹ˆåŸå› åœ¨ä¸€å¼€å§‹çš„æœç´¢ä¸­å¤±è´¥ï¼Ÿ & -
                                    Misspellings
                                    ..
                                    ..p30
                            Term distribution in all non-relevant documents will be different
                            At least five judged documents can give stable results 
                        Evaluation 
                            one round of relevance feedback is Ok & -
                            we can Pick one of the evaluation measures from last lecture 
                            æ ¹æ®ä»¥å¾€ç»§ç»­è¯„ä»· & ! -
                            ä¾‹å¦‚ï¼š
                                Compute p@10 for original query q0
                                Compute p@10 for modified relevance feedback query q1
                                ç„¶åå†å¤„ç†è¿™ä¸¤ä¸ªæ•°æ®ï¼Œå¾—åˆ°ä¸€ä¸ªè¯„ä»·æ•°æ®
                            åˆ¤æ–­é¢˜ï¼š & -
                                Fair evaluation must be on â€œresidualâ€ documents (not yet judged by user) ***
                                in most cases : q1 is spectacularly better than q0
                        problemsï¼Œä¸èµ·ä½œç”¨çš„åŸå›   & - NEXT
                            â€œExcite web search engineâ€
                                :Initially provided full relevance feedback,However
                            no incentive to give feedback
                            users do not understand .
                                Web search users are only rarely concerned with increasing recall
                            this method creates long modified queries
                ä¼ªç›¸å…³åé¦ˆ
                    it assumes that topk documents are relevant .
                    can improve quaity without interaction with users  
                    it automates the â€œmanualâ€ part of true relevance feedback
                    ç”¨æˆ·ä¸éœ€è¦è¿›è¡Œé¢å¤–çš„äº¤äº’å°±å¯ä»¥è·å¾—æ£€ç´¢æ€§èƒ½çš„æå‡ã€‚
                    å‡è®¾æ’åé å‰çš„ k ç¯‡æ–‡æ¡£æ˜¯ç›¸å…³çš„ï¼Œæœ€ååœ¨æ­¤å‡è®¾ä¸Šåƒä»¥å¾€ä¸€æ ·è¿›è¡Œç›¸å…³åé¦ˆã€‚
                    å®ƒä¸å¯èƒ½å®Œå…¨é¿å…è‡ªåŠ¨åŒ–æ“ä½œæ‰€å¸¦æ¥çš„é£é™©
                        it can leads to  query drift & -
                    pseudo-relevance feedback is effective on average & -
                        use the notation : relevance feedback (PsRF)
                é—´æ¥ç›¸å…³åé¦ˆ Indirect relevance feedback
                    -r     
                        implicit feedback from user data 
                            eg. location of users , time of the query ...

                    Less reliable than explicit feedback
                    users are often reluctant to provide explicit feedback 
                    understand it  & -
                        Web æœç´¢å¼•æ“ä¸€æ ·çš„å…·æœ‰é«˜è®¿é—®é‡çš„ç³»ç»Ÿä¸­ï¼Œæ”¶é›†ç”¨æˆ·çš„å¤§é‡éšå¼åé¦ˆä¿¡æ¯æ˜¯ååˆ†å®¹æ˜“çš„
                        å¦‚æœç”¨æˆ·æµè§ˆçš„æ¬¡æ•°è¶Šå¤šï¼Œé‚£ä¹ˆå®ƒçš„æ’åä¹Ÿè¶Šé«˜ã€‚
                        è¿™å®é™…æ˜¯ç‚¹å‡»æµæŒ–æ˜ï¼ˆ clickstream miningï¼‰è¿™ä¸ªé€šç”¨é¢†åŸŸçš„ä¸€ç§å½¢å¼ã€‚
                        ä¸€ä¸ªéå¸¸ç›¸å…³çš„æ–¹æ³•ç”¨äºä¸ Web æŸ¥è¯¢ç›¸åŒ¹é…çš„å¹¿å‘Šæ’åº
                    ç»´æŠ¤ä¿¡æ¯è¿‡æ»¤å™¨ï¼ˆå¦‚æ–°é—»è¿‡æ»¤å™¨ï¼‰
            æŸ¥è¯¢æ‰©å±•(Query expansion)
                Web æœç´¢å¼•æ“ä¼šç»™å‡ºç›¸å…³çš„æ¨èæŸ¥è¯¢ï¼Œç„¶åç”¨æˆ·å¯ä»¥é€‰æ‹©å…¶ä¸­çš„æŸä¸ªæ¨èæŸ¥è¯¢è¿›è¡Œæœç´¢
                    "also try :"
                ä½†æ˜¯æ€»çš„æ¥è¯´æŸ¥è¯¢æ‰©å±•ä¸å¦‚ç›¸å…³åé¦ˆæŠ€æœ¯æˆåŠŸã€‚å½“ç„¶ï¼Œå®ƒçš„ä¼˜ç‚¹æ˜¯æ›´å®¹æ˜“ä¸ºç”¨æˆ·æ‰€ç†è§£
                    ä¼å›¾è§£å†³"ç¿»è¯‘è½¬æ¢â€ çš„é—®é¢˜ï¼Œå³ç”¨æˆ·å¦‚ä½•çŸ¥é“æ–‡æ¡£ä½¿ç”¨å“ªäº›è¯é¡¹
                types of it : & -
                    1.based on query log mining
                        example give 
                            example 1:
                                After issuing the query [herbs], 
                                users frequently search for [herbal remedies]
                            example 2:
                                Users searching for [flower pix] 
                                    frequently click on the URL photobucket.com/flower
                                Users searching for [flower clipart] 
                                    frequently click on the same URL
                    2.æ‰‹å·¥ç¼–è¾‘
                        built up sets of synonymous 
                        Example for manual thesaurus: PubMed
                    3.åŸºäºç»Ÿè®¡å­¦çš„ç»“æœï¼Œç›¸å…³æ€§çŸ©é˜µ
                        Automatically derived thesaurus (e.g., 
                            based on co-occurrence statistics)
                        idea & -
                        using term-document matrix A , 
                            we can generate co-occurrence statistics 
                        semantically related with t
                        Widely used in specialized search engines 
                            for science and engineering
                        we use global resource, i.e. a 
                            resource that is not query-dependent
                        æ„å»ºåŒä¹‰è¯è¯å…¸ï¼šuse thesaurus
                        geneerate from 
                            Co-occurrence is more robust, grammatical relations are more accurate &
                            processï¼š   
                                A is a term-document matrix 
                                C = AA^T
                                cij is the number of times two terms ti and tj co-occur
                                For each ti, pick terms with high values in C
                                with a larger number being better
                                calcu & - NEXT p48 
                                we can calculate the 
                                    Nearest neighbors ** & -
                                    with this method 
                Query expansion:it may be as good as pseudo-relevance feedback
        ppt-11-Probabilistic Information Retrieval ç¬¬ 11 ç«  æ¦‚ç‡æ£€ç´¢æ¨¡å‹ --over --c
            file:///G:/i.Note-%E8%BF%87%E4%BA%94%E5%85%B3%E6%96%A9%E5%85%AD%E5%B0%86/2%E8%AF%BE%E4%B8%9A%E5%85%B3/TUD%E8%AF%BE%E7%A8%8B/WS1819%E7%AC%94%E8%AE%B0/%E8%87%AA%E5%AD%A6%E8%AF%BE%E4%BB%B6/pic/IR/ppt-11/Viewer.html
            ä¸€ç§ç›‘ç£çš„ï¼ŒåŸºäºæ¦‚ç‡çš„ï¼Œè¿”å›æ’åçš„æ£€ç´¢ç­–ç•¥
                è®­ç»ƒåˆ†ç±»å™¨
            probability ranking principleï¼Œæ¦‚ç‡æ’åºåŸç†
            æ–‡æ¡£å’ŒæŸ¥è¯¢éƒ½è¡¨ç¤ºä¸ºè¯é¡¹å‡ºç°ä¸å¦çš„å¸ƒå°”å‘é‡
            è®¸å¤šä¸åŒçš„æ–‡æ¡£å¯èƒ½éƒ½æœ‰ç›¸åŒçš„å‘é‡è¡¨ç¤ºã€‚
            ç‹¬ç«‹æ€§å‡è®¾å’Œå®é™…æƒ…å†µå¾ˆä¸ç›¸ç¬¦ï¼Œä½†åœ¨å®é™…ä¸­å¸¸å¸¸å´èƒ½ç»™å‡ºä»¤äººæ»¡æ„çš„ç»“æœ
            SchÃ¤tzung der Wahrscheinlichkeit, dass ein Dokument d 
                fÃ¼r eine Anfrage q relevant ist
            Beispiel
                1.æ”¶é›†è¡¨æ ¼ å‘¨å›´æ˜¯ç»Ÿè®¡ä¿¡æ¯,æ ¹æ®ç”¨æˆ·ç»™å‡ºçš„åé¦ˆï¼Œåªè¦æœ‰åé¦ˆå°±å¯ä»¥
                2.Berechnung des Termgewichts
                    Gewichtungsfunktionï¼šformular  & -
                3.è®¡ç®— çŠ¶æ€æ£€ç´¢å€¼ Retrievalstatuswert
                    logç›¸åŠ 
                    è®¡ç®— & -
            åŸºç¡€æ¦‚ç‡çŸ¥è¯†
                    äº‹ä»¶çš„ä¼˜åŠ¿ç‡ï¼ˆ oddsï¼‰   **
                        å®ƒæä¾›äº†ä¸€ç§åæ˜ æ¦‚ç‡å¦‚ä½•å˜åŒ–çš„â€œ æ”¾å¤§å™¨â€ ï¼ˆ multiplierï¼‰
                    æ— å…³æ€§åˆ¤æ–­ï¼Œè®¡ç®—æ¡ä»¶æ¦‚ç‡ 
                        pptä¸Šçš„è®¡ç®—
                    ct æ˜¯æŸ¥è¯¢è¯é¡¹çš„ä¼˜åŠ¿ç‡æ¯”ç‡ï¼ˆ odds ratioï¼‰çš„å¯¹æ•°å€¼ã€‚
                        ç›¸å…³å’Œä¸ç›¸å…³ï¼Œä¸¤ä¸ªä¼˜åŠ¿ç‡çš„æ¯”å€¼ï¼Œæœ€åå¯¹è¿™ä¸ªå€¼å–å¯¹æ•°
                    å¦‚æœè¯é¡¹åœ¨ç›¸å…³å’Œä¸ç›¸å…³æ–‡æ¡£ä¸­çš„ä¼˜åŠ¿ç‡ç›¸ç­‰ï¼Œ ctå€¼ä¸º 0ã€‚
                    å¦‚æœè¯é¡¹æ›´å¯èƒ½å‡ºç°åœ¨ç›¸å…³æ–‡æ¡£ä¸­ï¼Œé‚£ä¹ˆè¯¥å€¼ä¸ºæ­£ã€‚ 
                    ct å®é™…ä¸Šç»™å‡ºçš„æ˜¯æ¨¡å‹ä¸­è¯é¡¹çš„æƒé‡ï¼Œç²’åº¦æ¯”è¾ƒå°ï¼ŒæŸ¥è¯¢æ–‡æ¡£çš„å¾—åˆ†æ˜¯RSVdï¼Œæ˜¯ctçš„æ±‚å’Œ
                    å¾—åˆ†é«˜åˆ™ç›¸å…³æ€§å¤§ï¼Œæ ¹æ®è¯„åˆ†å¯ä»¥è¿›è¡Œæ’åºå‡ºkä¸ªé¡¹ç›®
                    Beispiel--Basic Model 
                    Einfache Wahrscheinlichkeit P(A) 
                    Theorem von Bayes 
                        Umkehren von Schlussfolgerungenç›¸å
                        priori-Wahrscheinlichkeitå…ˆéªŒæ¦‚ç‡
                        Wahrscheinlichkeit fÃ¼r ein Ereignis B unter der BedingungåéªŒæ¦‚ç‡
                    Chance / Quote (Odds) statt Wahrscheinlichkeiten
                        Wenn P(A1) > P(A2), dann ist auch O(A1) > O(A2)
                    UnabhÃ¤ngige Ereignisseç‹¬ç«‹æ´»åŠ¨
                        zwei WÃ¼rfel geworfen
                        Test auf UnabhÃ¤ngigkeit &calcu - p
            Binary Independence Retrieval Model (BIR) **
                äºŒå€¼ç‹¬ç«‹æ¨¡å‹BIR &
                Binary Independence Retrieval Model (BIR)
                a model to measure rank documents .give each document a weight ,value .
                ï‚§ Theorie und Definitionen
                    RSVdï¼šRetrievalstatuswert eines Dokuments
                    sim(dm,qk) = RSVç›¸ä¼¼åº¦è¯„åˆ†
                    Anfrage-Vektor und Dokument-Vektor
                    Wahrscheinlichkeit der Relevanz, wenn eine Anfrage q und ein Dokument d gegeben sind?
                ï‚§ Retrievalstatuswert eines Dokuments (RSV)
                    æ¨å¯¼ï¼ï¼ï¼ è¦è€ƒï¼Ÿï¼Ÿ & - å¤§æ¦‚å…¶å°±å¯ä»¥ NEXT ä»¥åç»†åŒ–
                        äº’è¡¥æ¦‚ç‡çš„è½¬æ¢KomplementÃ¤re Umformung der Wahrscheinlichkeit
                        NEXT
                        Anwendung eines Logarithmus
                            Grund? 
                ï‚§ Termgewichtungsfunktion
                    r : Wahrscheinlichkeit, dass der Term ti 
                        in einem fÃ¼r die Anfrage q relevanten Dokument d vorkommt
                    n : Wahrscheinlichkeit, dass der Term ti 
                        in einem fÃ¼r die Anfrage q nicht relevanten Dokument d vorkommt
                    å…¶å®å°±æ˜¯åŠ å’Œè€Œå·²ï¼Œåé¢æ‰æœ‰å‚æ•°ä¿®æ­£
                    +
                        åº”ç”¨ï¼š  
                            Interaktives Relevance Feedbackäº¤äº’ç›¸å…³åé¦ˆ
                            è‡ªåŠ¨å¤„ç†
                        ä»¥R éRçš„åˆ†å¸ƒä¸ºåŸºç¡€auf der Basis der Verteilung 
                            in relevanten und nichtrelevanten Dokumenten
                ï‚§ Probabilistisches Relevance Feedback
                    å¯ä»¥é€šè¿‡ï¼ˆä¼ªï¼‰ç›¸å…³åé¦ˆæŠ€æœ¯ï¼Œä¸æ–­è¿­ä»£ä¼°è®¡è¿‡ç¨‹æ¥è·å¾— ptçš„æ›´ç²¾ç¡®çš„ä¼°è®¡ç»“æœ
                    è¿™ä¸ªè¿‡ç¨‹å«å‚æ•°ä¼°è®¡ï¼šParameterschÃ¤tzung durch Relevance Feedback (contâ€™d)
                    wenn es positiv ist ,in relevanten Dokumenten grÃ¶ÃŸer ist als in nicht relevanten Dokumenten
                    å®é™…ä¸­å¾€å¾€è¦å¯¹ä¸Šè¿°ä¼°è®¡è¿›è¡Œå¹³æ»‘ï¼Œæ­¤æ—¶å¯ä»¥å¯¹åŒ…å«å’Œä¸åŒ…å«è¯é¡¹çš„æ–‡æ¡£æ•°ç›®éƒ½åŠ ä¸Š0.5 --Parameterkorrektur
                    ä¹Ÿå¯ä»¥åŸºäºä¼ªç›¸å…³åé¦ˆçš„æ–¹æ³•æ¥å®ç°ä¸Šè¿°ç®—æ³• Rekursive ParameterschÃ¤tzung
                        ç”¨æˆ·å¯¹æŸä¸ªæ–‡æ¡£å­é›† V çš„ç›¸å…³æ€§åˆ¤æ–­
                        V å¯ä»¥åˆ’åˆ†æˆä¸¤ä¸ªå­é›†ï¼š VR and VNR æ˜¯å¦ç›¸å…³
                        è¯é¡¹ t å‡ºç°åœ¨ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
                        NEXT è¿­ä»£è¿‡ç¨‹å®ä¾‹ç®—æ³• & - NEXT p47 ä¹¦ä¸­p158
                            AnfangsschÃ¤tzung ï¼Ÿ 
                        Erzeugung neuer Termgewichte
                        Korrekturwerte
                    è¿™é‡Œå¯¹logæ˜¯æ•°å€¼æ±‚å’Œï¼Œè€Œä¸æ˜¯æ±‚ç§¯ï¼Œtf-idfæ˜¯æ±‚æœº
            Okapi 
                +
                    ç”¨æ¥å¯¹åŒ¹é…æ–‡æ¡£è¿›è¡Œæ’åºçš„å‡½æ•°
                    BM æ˜¯ Best Matching (æœ€ä½³åŒ¹é…) çš„ç¼©å†™
                    åœ¨æ¦‚ç‡æœç´¢çš„æ¡†æ¶ä¸‹è¢«æå‡ºçš„ã€‚Okapi æ˜¯ç¬¬ä¸€ä¸ªä½¿ç”¨è¿™ç§æ–¹æ³•çš„ä¿¡æ¯è·å–ç³»ç»Ÿçš„åç§°
                    æ–‡æ¡£è¯„åˆ†çºµå‘æ¯”è¾ƒï¼š & -
                        1.tf-idf 
                        2.åŸºäºBIRäºŒå€¼æ¦‚ç‡æ¨¡å‹
                            æ²¡æœ‰è€ƒè™‘æ–‡æ¡£é•¿åº¦
                        3.åŸºäºOkapi
                            åŸºäºè¯é¡¹é¢‘ç‡ã€æ–‡æ¡£é•¿åº¦ç­‰å› å­æ¥å»ºç«‹æ¦‚ç‡æ¨¡å‹çš„ä¸€ç§æ–¹æ³•
                            Termgewichtungsfunktionenä¸åŒï¼Œå¼•å…¥æ›´å¤šå‚æ•°
                    Okapiæ˜¯tf idfä»¥åŠä¸€äº›å‚æ•°çš„ç»„åˆè€Œå·²
                    å¦‚æœæœç´¢è¯ä¸­åŒ…å«æ¯”è¾ƒç‹¬ç‰¹çš„è¯ï¼Œåˆ™ä¼šæå‡åˆ†æ•°ï¼›æœç´¢è¯åœ¨ä¸€ä¸ªæ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°è¶Šé«˜ï¼Œåˆ†æ•°ä¹Ÿä¼šæ›´é«˜ï¼›ä½†æ–‡æ¡£è¶Šé•¿ï¼Œåˆ†æ•°ä¼šè¶Šä½ã€‚
                    BM25è¾ƒæ–°å‡çº§ç‰ˆ
                        BM25Fï¼ŒæŠŠæ–‡æ¡£ç»“æ„å’Œé”šæ–‡æœ¬ä¹Ÿè€ƒè™‘è¿›æ¥ã€‚å¦ä¸€ä¸ªå« 
                        BM25+ï¼Œåªæ˜¯åœ¨ä¸Šé¢å…¬å¼ä¸­çš„æ–¹æ‹¬å·é‡ŒåŠ äº†ä¸€ä¸ª Î´ï¼Œç”¨æ¥å¼¥è¡¥åŸæ¥å…¬å¼å¯¹è¶…é•¿æ–‡æ¡£çš„ä¸å…¬ã€‚
                        Erweiterung der BM25 Gewichtung fÃ¼r den Fall dass sehr lange Abfragen auftreten
                    å¦å¤–çš„æ¨¡å‹ï¼š
                        ElasticSearch/Lucene çš„åˆ†æ•°è®¡ç®—
                            ElasticSearch åº•å±‚é‡‡ç”¨äº† Luceneï¼Œ
                                è€Œ Lucene çš„åˆ†æ•°è®¡ç®—ç»¼åˆäº†å¸ƒå°”æ¨¡å‹
                                (Boolean model), TF-IDF, ä»¥åŠçŸ¢é‡ç©ºé—´æ¨¡å‹ã€‚
                    BM25æ˜¯ä¸€ç§BOWï¼ˆbag-of-wordsï¼‰æ¨¡å‹
                    BM25ç®—æ³•é¦–å…ˆç”±Okapiç³»ç»Ÿå®ç°ï¼ˆOkapiæ˜¯ä¼¦æ•¦åŸå¸‚å¤§å­¦å®ç°çš„ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿï¼‰ï¼Œæ‰€ä»¥åˆç§°ä¸ºOkapi BM25
                    åœ¨è®¡ç®—IDFæ—¶ï¼Œå¦‚æœè¢«æŸ¥è¯¢çš„è¯è¯­ä¸åœ¨è¯­æ–™åº“ä¸­ & -
                        å°±ä¼šå¯¼è‡´åˆ†æ¯ä¸ºé›¶ï¼Œæ‰€ä»¥é€šå¸¸ä¼šåŠ ä¸Šä¸€ä¸ªè¾ƒå°çš„æ•°ä»¥ä¿è¯åˆ†å¸ƒä¸ä¸ºé›¶
                    æ˜¯å¯¹tf-idfçš„å‡çº§ï¼šOkapi BM25 æ˜¯åˆ°ç›®å‰ä¸ºæ­¢è¢«è®¤ä¸ºæœ€å…ˆè¿›çš„æ’åç®—æ³•ä¹‹ä¸€
                    é¥±å’Œåº¦ï¼š
                        æ—¢ç„¶ä¸¤ä¸ªä¸¤ä¸ªæ–‡æ¡£éƒ½æ˜¯å¤§ç¯‡å¹…è®¨è®ºæ£’çƒçš„ï¼Œé‚£ä¹ˆâ€œæ£’çƒâ€è¿™ä¸ªè¯å‡ºç° 40 
                        æ¬¡è¿˜æ˜¯ 80 æ¬¡éƒ½æ˜¯ä¸€æ ·çš„ã€‚äº‹å®ä¸Šï¼Œ30 æ¬¡å°±è¯¥å°é¡¶å•¦ï¼
                BM11, BM15, BM25éƒ½å±äºBM25ç®—æ³•å®¶æ—
                Erweiterung der BM25 Gewichtung fÃ¼r den Fall dass sehr lange Abfragen auftreten
                å…¬å¼*2 & - NEXT
                æ€»ç»“æ¯”è¾ƒ 
                    åœ¨å“ªé‡Œä½¿ç”¨ï¼Ÿ & - NEXT explain 
                        Suche Ã¤hnlicher Dokumente
                        ohne besondere Verfahren multilingual
                    Vgl. der klassischen IR-Modelle & - NEXT explain
                    Besides the big error in estimating the
                        probabilities the classification is still
                        correct
                    æ¦‚ç‡IRä¼˜ç¼ºç‚¹ & - NEXT explain
                        vor 
                            it can be very quick 
                            good theoretical background 
                        nach 
                            Need to guess the initial ranking
                                user have to mark some of docs as relevant or not
                            the term frequency will be ignored
                            //Independence assumption
        ppt-12-Language Models for IR ç¬¬ 12 ç«  åŸºäºè¯­è¨€å»ºæ¨¡çš„ä¿¡æ¯æ£€ç´¢æ¨¡å‹  --over --c
            +lan corpus
                a double circle indicates a (possible) finishing state.
                    here can we find a double circle . it means the state is finished  
                idea: 
                    for each doc. we generate a LM
                    then ,we can judge which doc is likely to generate the query 
                    it is a method to measure the relevance of a document d and a query ...
                A language model is constructed -for each document -in the collection
                some of the strings in the language it generates    
                    these strings can be gen. by the LM here 
                After generating each word, we decide whether to stop or to loop around
                a mathematical model of computation
                The FSM can change from one state to another in response to some external inputs
                    with some input data , state can be changed from to an other
                    the number of stats has a limit
                    external input can cause transition
                initial state, and the conditions for each transition
                A combination lock is a type of locking device
                This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows)
                An automaton is a finite representation of a formal language that may be an infinite set
                Automata play a major role in theory of computation, compiler construction, artificial intelligence, parsing and formal verification. 
                to calcu. , we Multiply the probabilities together. 
                MATH
                    dividing is the opposite of multiplying
                    Numerator and denominator!!!
                    decimals, percentages and fractions. 
                    +https://www.cs.cmu.edu/~venkatg/teaching/codingtheory/




            +    
                idea & -
                æŒ‰ç…§æ¨¡å‹ç”ŸæˆæŸ¥è¯¢çš„æ¦‚ç‡æ¥è¿›è¡Œæ’å
                å¯¹æ¯ä¸ªæ–‡ç« éƒ½ç”Ÿæˆä¸€ä¸ªè¯­è¨€æ¨¡å‹ & - exp -
                    ::
                        we gener. a Lan. Model for each doc.
                        the LM can be used to calcu. the relevant between doc and query 
                        then , we can rank the docs 
                æœ‰ç©·è‡ªåŠ¨æœº
                    è¯­è¨€ï¼šæ‰€æœ‰å¯èƒ½çš„å­—ç¬¦ä¸²çš„å…¨é›† & - exp
                        ::the lan. means all possible strings that LM can accept 
                    åŒåœˆèŠ‚ç‚¹å¯¹åº”çš„æ˜¯ï¼ˆå¯èƒ½çš„ï¼‰ç»ˆæ­¢çŠ¶æ€
                è¯­è¨€æ¨¡å‹ï¼š 
                        æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªç”Ÿæˆä¸åŒè¯é¡¹çš„æ¦‚ç‡åˆ†å¸ƒ
                        è¿˜éœ€è¦ä¸€ä¸ªåœ¨ç»ˆæ­¢çŠ¶æ€åœæ­¢çš„æ¦‚ç‡
                        ä¸€å…ƒè¯­è¨€æ¨¡å‹çš„å•çŠ¶æ€æœ‰ç©·è‡ªåŠ¨æœº ä¹¦ä¸­ see p164
                        ä¸€ä¸ªå…·ä½“çš„å­—ç¬¦ä¸²æˆ–æ–‡æ¡£çš„æ¦‚ç‡å¾€å¾€éå¸¸å°
                        åœæ­¢æ¦‚ç‡ï¼šä¸ä¼šå½±å“æ–‡æ¡£çš„æ’åº
                        ä¸€å…ƒè¯­è¨€æ¨¡å‹ 
                            è®¡ç®—æ¡ä»¶æ¦‚ç‡æ—¶ä¸è€ƒè™‘å‰ä¸€ä¸ªè¯é¡¹çš„å‡ºç°æƒ…å†µ
                            è®¡ç®—ä¾‹å­ & - p165 ä¾‹12-2
                            ä¾‹é¢˜ï¼š
                                ä½†æ˜¯é€šå¸¸åœ¨æ¦‚ç‡åº”ç”¨ä¸­
                                    å®é™…ä¸Šå¾€å¾€é‡‡ç”¨å¯¹æ•°æ±‚å’Œçš„è®¡ç®—æ–¹æ³•
                                    å› ä¸ºè®¡ç®—æœºç²¾åº¦é—®é¢˜
                                ä¸¤ä¸ªä¸€å…ƒè¯­è¨€æ¨¡å‹çš„éƒ¨åˆ†æ¦‚ç‡èµ‹å€¼
                            å¤§éƒ¨åˆ†å·¥ä½œéƒ½åªä½¿ç”¨äº†ä¸€å…ƒæ¨¡å‹
                            åœ¨ä¸€å…ƒè¯­è¨€æ¨¡å‹ä¸­ï¼Œè¯å‡ºç°çš„å…ˆåæ¬¡åºæ— å…³ç´§è¦
                        äºŒå…ƒè¯­è¨€æ¨¡å‹ 
                            è®¡ç®—æ¡ä»¶æ¦‚ç‡æ—¶åªè€ƒè™‘å‰ä¸€ä¸ªè¯é¡¹çš„å‡ºç°æƒ…å†µ
                        åŸºäºæ–‡æ³•çš„è¯­è¨€æ¨¡å‹ &
                            å¯¹äºè¯¸å¦‚è¯­éŸ³è¯†åˆ«ã€æ‹¼å†™æ ¡å¯¹ã€æœºå™¨ç¿»è¯‘ç­‰éœ€è¦
                                æ ¹æ®ä¸Šä¸‹æ–‡æ¥æ±‚è¯é¡¹æ¡ä»¶æ¦‚ç‡çš„åº”ç”¨éå¸¸é‡è¦
                            NEXT
                        +äºŒé¡¹å¼åˆ†å¸ƒï¼Œå¤šé¡¹å¼åˆ†å¸ƒ
                            äºŒé¡¹å¼ï¼šæ¯ä¸ªäº‹ä»¶å–å€¼æ˜¯2ï¼Œç»„æˆä¸€ä¸ªåºåˆ—çš„æ¦‚ç‡
                            å¤šé¡¹å¼ï¼šæ¯ä¸ªäº‹ä»¶å–å€¼æ˜¯1/Nï¼Œç»„æˆä¸€ä¸ªåºåˆ—çš„æ¦‚ç‡
                            äºŒé¡¹å¼æ¦‚ç‡å’ŒäºŒå··å±•å¼€å¼ç³»æ•°å…³ç³» &
                Einsatz im IR--æŸ¥è¯¢ä¼¼ç„¶æ¨¡å‹
                    è€ƒè™‘æŸ¥è¯¢ï¼Ÿ
                        æˆ‘ä»¬å¯¹æ–‡æ¡£é›†ä¸­çš„æ¯ç¯‡æ–‡æ¡£ d æ„å»ºå…¶å¯¹åº”çš„è¯­è¨€æ¨¡å‹ Md
                        æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†æ–‡æ¡£æŒ‰ç…§å…¶ä¸æŸ¥è¯¢ç›¸å…³çš„ä¼¼ç„¶ P(d|q) æ’åºã€‚
                        ä½¿ç”¨è´å¶æ–¯å…¬å¼ï¼Œ åªéœ€è¦å¯¹P(q|d)æ’åº ï¼Œå®ƒæ˜¯åœ¨æ–‡æ¡£ d å¯¹åº”çš„è¯­è¨€æ¨¡å‹ä¸‹ç”Ÿæˆ q çš„æ¦‚ç‡
                        å…·ä½“æ€ä¹ˆæŸ¥ï¼Œæ¦‚ç‡ä¹˜ç§¯ & - exp    
                            ::
                                we multiply the probabilities together to estimate the result
                                it is usually approximated by considering each term from the retrieved document 
                                the model is unknown 
                            æ¯ç¯‡æ–‡æ¡£åœ¨ä¼°è®¡ä¸­éƒ½æ˜¯ä¸€é—¨ç‹¬ç«‹çš„â€œ è¯­è¨€â€
                            ä¸€ä¸ªå…·ä½“çš„å­—ç¬¦ä¸²æˆ–æ–‡æ¡£çš„æ¦‚ç‡å¾€å¾€éå¸¸å° 
                    +æœ€å¤§ä¼¼ç„¶ä¼°è®¡   
                        æ­£æ€åˆ†å¸ƒï¼Œå–å¾—ä¸€äº›ç‚¹ï¼Œåè¿‡æ¥ä¼°è®¡å‚æ•°ï¼Œä¼°è®¡å‡½æ•°çš„çŠ¶å†µ 
                        ä¸ä¸€å®šå­˜åœ¨ï¼Œä¹Ÿä¸ä¸€å®šå”¯ä¸€ã€‚
                    åœ¨ IR ç¯å¢ƒä¸‹ï¼Œè¿˜æœ‰å…¶ä»–ä½¿ç”¨ LM çš„æ€è·¯
                    é›¶æ¦‚ç‡é—®é¢˜
                        ç”±äºå¾ˆå¤šè¯åœ¨è®­ç»ƒæ•°æ®ä¸Šå­˜åœ¨ç¨€ç–æ€§ï¼Œæ‰€ä»¥åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ—¶å¯èƒ½ä¼šé‡åˆ°é›¶æ¦‚ç‡é—®é¢˜
                        ä½¿ç”¨å¹³æ»‘
                            ç®€å•å¹³æ»‘ï¼Œå½’ä¸€åŒ–
                            å¼•å…¥å‚ç…§æ¦‚ç‡åˆ†å¸ƒå‡½æ•°
            -------
                EinfÃ¼hrung
                    Statistische Sprachmodelle
                        æœ¬è´¨ä¸Šæ˜¯å•¥ & - exp
                            ::essentially a method to rank docu.
                        ç”¨äºæ€»ç»“è¿™æ–‡ç« æ˜¯ä»€ä¹ˆä¸ªè§„å¾‹ç”Ÿæˆçš„
                        ï¼šbeschreibt die Erzeugung von Texten

                        åº”ç”¨
                            ï‚§ Erkennung gesprochener Sprache
                            ï‚§ Part-of-Speech-Tagger
                            ï‚§ Digitalisierung handschriftlicher Texte
                    Funktionsweise
                        æè¿°æœ‰é™è‡ªåŠ¨æœº EA & - explain
                            Doppelkreis kennzeichnet ï¼Ÿ
                                den (mÃ¶glichen) Endzustand desEA (=Ende der Zeichenkette)
                            ç®­å¤´ä»£è¡¨å•¥...
                            ::
                                stands for transformation(transition) 
                                double circle means finishing state
                        è®¡ç®— P(Katzen fangen MÃ¤use) & - 
                        Mehrere Modelle M1, M2, ..., M åˆ¤æ–­è®¡ç®— & -
                            å¤§Oå‡ ç‡ï¼Ÿ
                            pptä¸Šæœ‰ç‚¹é—®é¢˜
                        unigram language model
                            P(Hunde, fangen, Katzen) = P(Katzen, Hund, fangen) 
                        äºŒå…ƒè¯­è¨€æ¨¡å‹ä¸ºå•¥ä¸ç”¨ & - exp
                            Zu speziell, zu aufwÃ¤ndig
                            ä½¿ç”¨bedingte Wahrscheinlichkeiten
                            ::
                                it uses Conditional probability , complex/difficult to compute 

                    Modelle / Markow-Ketten
                        ä½¿ç”¨é©¬å°”ç§‘å¤«é“¾
                            ä¸€ä¸ªé©¬å°”ç§‘å¤«è¿‡ç¨‹æ˜¯çŠ¶æ€é—´çš„è½¬ç§»ä»…ä¾èµ–äºå‰nä¸ªçŠ¶æ€çš„è¿‡ç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¹‹ä¸ºné˜¶é©¬å°”ç§‘å¤«æ¨¡å‹
                            kann man LM durch Markow-Ketten darstellen & -
                    Horizontï¼ˆå‘å‰çœ‹çš„è§†é‡ï¼‰
                        durch Analyse n- vorangegangener WÃ¶rter
                        è§†é‡ n
                            - n = 2 : Trigramm LM
                            - n = 1 : Bigramm LM
                            - n = 0 : Unigramm LM
                Einsatz im IR
                    Ranking mit absteigender Wahrscheinlichkeit 
                    Aberï¼šAnfrage q zu kurz fÃ¼r LM
                    Vereinfachungen, Annahmen
                        fest fÃ¼r Anfrage q (kann somit weggelassen werden) 
                        :å¯¹æ¯ä¸ªqæŸ¥è¯¢ï¼Œæ˜¯å›ºå®šçš„ï¼Œå› æ­¤å¯ä»¥å»æ‰
                        FÃ¼r jedes Dokument d ein Sprachmodelle Md ableiten
                        ï¼šæ¯ä¸ªå½¢æˆä¸€ä¸ªè¯­è¨€æ¨¡å‹ & -
                        Pï¼ˆd|qï¼‰Wird zum Zeitpunkt der Anfrage berechnet
                        BUT:Zu wenig Daten fÃ¼r komplexes Language Models
                            losung:ä½¿ç”¨ä¸€å…ƒæ¨¡å‹
                        Multinomialverteilung 
                        ï¼šå¤šé¡¹åˆ†å¸ƒï¼ˆç”¨äºè®¡ç®—ä¸€å…ƒæ¨¡å‹æ¦‚ç‡ï¼‰
                    SchÃ¤tzen von Wahrscheinlichkeiten
                        SchÃ¤tzer Ã¼ber relative HÃ¤ufigkeit 
                            p20å…¬å¼ & -
                            å…¶å®å°±æ˜¯ç”¨tf/dä»£æ›¿è‡ªåŠ¨æœºä¸­çš„æ¦‚ç‡
                    Smoothing
                        um Nullwerte zu verhindern
                        1.ä½¿ç”¨Gesamtkorpus 
                            idea & -
                        2.Jelinek-Mercer Smoothing 
                            idea & -
                                Linearkombination 
                                ï¼šå±€éƒ¨çš„æœ‰æ—¶å€™æ˜¯0 æ‰€ä»¥å¼•å…¥å…¨å±€çš„åšå¹³æ»‘
                            p23å…¬å¼ & -
                                å‚æ•°å¤§å°çš„å½±å“
                                Hoher Wert: kaum / kein Smoothing
                                Niedriger Wert: fast nur Korpuswerte
                                è¿™é‡Œçš„é›¶é˜¶æŒ‡çš„æ˜¯å…¨å±€æ¦‚ç‡ 
                                ä¸€ä¸ªdocumentæœ¬æ¥æ˜¯ç›¸å…³çš„ï¼Œä½†æ˜¯å› ä¸ºæŸ¥è¯¢é•¿ï¼ŒæŸ¥è¯¢ä¸­
                                    å¾ˆå¤šåœ¨æ–‡ç« ä¸­æœ‰å¯èƒ½å¹¶æ²¡æœ‰å‡ºç°ã€‚
                                    æ‰€ä»¥è¯´å¹³æ»‘ï¼Œå¯¹äºé•¿çš„æŸ¥è¯¢æ˜¯æœ‰åˆ©çš„ **
                                    Viele Anfrageterme: stÃ¤rkeres Smoothing
                                    Wenige Anfrageterme: schwaches Smoothing
                            è¾“å…¥ä¸ºTasse Kanneçš„é‚£ä¸ªè®¡ç®— & - NEXT
                                åŸæ¥çš„è¡¨æ ¼ï¼Œæ¨ªå‘åŠ èµ·æ¥æ˜¯1ï¼Œä½†æ˜¯æœ‰å¾ˆå¤š0
                            Alternative Vorgehensweisenï¼Ÿè¿˜æœ‰å•¥æ›¿ä»£çš„ä¸
                                & NEXT
                        +å„ç§å¹³æ»‘æ–¹æ³•
                            åŠ æ³•å¹³æ»‘
                                æœ€æœ´ç´ çš„æ€æƒ³å°±æ˜¯å¯¹æ‰€æœ‰äº‹ä»¶çš„å‡ºç°æ¬¡æ•°åŠ 1
                            å¤å¾·-å›¾çµï¼ˆGood-Turingï¼‰ä¼°è®¡
                            Katzå¹³æ»‘
                            Jelinek-Mercerå¹³æ»‘
                                æ‹¿bi-gramä¸¾ä¾‹ï¼Œå¦‚æœä¸€ä¸ªbi-gramæ²¡æœ‰å‡ºç°è¿‡ï¼Œ
                                ä¸€ä¸ªæœ´ç´ çš„æ€æƒ³æ˜¯é€€ä¸€æ­¥ï¼Œçœ‹ä¸€ä¸‹uni-gramçš„æ¦‚ç‡ï¼Œ
                                ç„¶åå¤§æ¦‚æ¨æµ‹ä¸€ä¸‹bi-gramçš„æ¦‚ç‡
                            è´å¶æ–¯å¹³æ»‘
                    å®é™…åº”ç”¨
                        Multiplikation kleiner Werte FÃ¼hrt zu Rechenungenauigkeiten & -
                            Summe statt Produkt
                            Logarithmieren des Wertes rechnen
                            :using log function to calcu. **
                        Term-at-a-time
                            ä¸€è¡Œä¸€è¡Œç´¢å¼•è¡¨è¿›è¡Œå¤„ç†
                            Akkumulatoren fÃ¼r die Dokumente
                            æ¯ä¸ªdocä¸€ä¸ªç´¯åŠ å™¨ 
                            NEXT
                Zusammenfassung 
                    LMs vs. BIR
                        è”ç³» & - NEXT
                            term are independent
                        åŒºåˆ« & - NEXT
                            each document forms a class vs - only two classes relevant or not 
                            the smoothing method is different 
                    LMs vs. Vector-Space-Modell
                        è”ç³» & - NEXT
                            considering term frequency 
                            //also normalized ,and has no relation to the length of the document 
                            the smoothing of it has something to do with idf 
                        åŒºåˆ« & - NEXT
                            vector model:: based on geometric similarity vs .. based on probability model 
                            //normalized in different ways 
                    Language-Modeling-Ansatz besser als Vector-Space-Modell
                    zwei Alternative Vorgehensweisen
                    Jedes Dokument bildet eine eigene Klasse (LM) 
                    vs. Klassen (relevant und nicht relevant) 
                    Jedes Dokument bildet eine eigene Klasse (LM) VS. die durch Menschen definiert werden (BIR) 
                    Jelinek-Mercer Smoothing vs. Addition kleiner Werte :: Jelinek-Mercer Smoothing vs. Addition kleiner Werte
        ppt-13-Web Search Basics ç¬¬ 19 ç«  Web æœç´¢åŸºç¡€ --over --c
            å…³äºwebä¸€äº›åŸºæœ¬çš„ä¸œè¥¿
            webåŒ…æ‹¬å¾ˆå¤šå¾ˆå¤šæ–¹é¢ï¼Œæ¯ä¸ªæ–¹é¢éƒ½æœ‰å¾ˆå¤šå¯ä»¥è¯´ & -
            ä¸ºä»€ä¹ˆçº·ç¹æ‚ä¹±ã€å˜åŒ–è¿…é€Ÿçš„ Webä¸åŒ
                æ— æ³•é›†ä¸­æ§åˆ¶çš„æ— ä¸­å¿ƒçš„ç½‘é¡µå†…å®¹å‘å¸ƒæœºåˆ¶
                ::
                    there is no central control within the web 
            web IRç‰¹ç‚¹ & - exp

                    very large
                    lots of duplicates
                    lots of spam 


                    ç½‘é¡µæ°‘ä¸»åŒ–ï¼šé€ æˆ
                        ç½‘é¡µä¸­å­˜åœ¨å¤§é‡è¯­æ³•å’Œé£æ ¼ä¸Šçš„å·¨å¤§å·®å¼‚ 
                    Web ä¸­åŒ…å«çœŸç†ã€è°è¨€ã€çŸ›ç›¾å’Œå¤§é‡çŒœæµ‹ã€‚ 
                    ::
                        1.different pages have different styles and maybe diff. gramma
                            write in diff. ways 
                        2.There are truths and lies on the Internet 
                æˆ‘ä»¬åº”è¯¥ç›¸ä¿¡å“ªäº› Web ç½‘é¡µï¼Ÿ
                å¯¹æŸä¸ªç”¨æˆ·å¯ä¿¡çš„ç½‘é¡µå†…å®¹ä¸ä¸€å®šå¯¹å…¶ä»–ç”¨æˆ·å¯ä¿¡
            ç´¢å¼•è§„æ¨¡ä¼°è®¡
                æŸä¸ªæœç´¢å¼•æ“ä¸­ç´¢å¼•çš„ç½‘é¡µæ•°ç›®æ˜¯å¤šå°‘ï¼Ÿ
                æ‰€è°“é™æ€ç½‘é¡µï¼ˆ static web pageï¼‰ï¼ŒæŒ‡çš„æ˜¯é‚£äº›å†…å®¹ä¸ä¼šå› è¯·æ±‚ä¸åŒè€Œä¸åŒçš„ç½‘é¡µ
                ä¸€ä¸ªåŠ¨æ€ç½‘é¡µç”Ÿæˆçš„ä¾‹å­ã€‚è¿™ç§é¡µé¢çš„ä¸€ä¸ªæ ‡å¿—æ˜¯URL ä¸­é€šå¸¸åŒ…å«å­—ç¬¦â€œ ?â€ ã€‚
                åŠ¨æ€ç½‘é¡µæœ‰æ•°æ®è¯·æ±‚çš„è¿‡ç¨‹
                æµè§ˆå™¨å‘é€æœ‰å…³ AA129 æ¬¡èˆªç­çš„è¯·æ±‚ç»™ Web åº”ç”¨æœåŠ¡å™¨ï¼ŒæœåŠ¡å™¨ä»åç«¯æ•°æ®åº“
                    ä¸­è·å–ä¿¡æ¯å¹¶ä¸”ç”Ÿæˆä¸€ä¸ªåŠ¨æ€ç½‘é¡µè¿”å›ç»™æµè§ˆå™¨
                é™æ€é¡µé¢ä¸åŠ¨æ€  & - exp  
                    åŠ¨æ€é¡µé¢ï¼ˆ dynamic pageï¼‰
                        é€šå¸¸æ˜¯ç”±åº”ç”¨æœåŠ¡å™¨åº”ç­”æ•°æ®åº“çš„æŸ¥è¯¢éœ€æ±‚æ—¶äº§ç”Ÿçš„ 
                    é™æ€ç½‘é¡µï¼ˆstaticï¼‰
                        æ²¡ç”¨post getæ–¹æ³•çš„å°±æ˜¯
                    ::
                        need to query the database 
                        load the web content without post or get method 
            webå›¾
                ç”»ä¸€ä¸ªç®€å•çš„ & -
                    é™æ€ HTML ç½‘é¡µé€šè¿‡è¶…é“¾æ¥äº’ç›¸è¿æ¥è€Œæˆçš„æœ‰å‘å›¾
                    æ¯ä¸ªé¡¶ç‚¹ä»£è¡¨ä¸€ä¸ªç½‘é¡µï¼Œï¼¡ç½‘é¡µä¸Šæœ‰ä¸€ä¸ªè¶…é“¾æ¥æŒ‡å‘ï¼¢ 
                è¯¥æœ‰å‘å›¾å¯èƒ½ä¸æ˜¯ä¸€ä¸ªå¼ºè¿é€šï¼ˆ strongly connectedï¼‰å›¾ï¼Œä¹Ÿå°±è¯´ï¼Œä»ä¸€ä¸ªç½‘é¡µå‡ºå‘ï¼Œæ²¿ç€è¶…é“¾æ¥å‰è¿›ï¼Œæœ‰å¯èƒ½æ°¸è¿œä¸ä¼šåˆ°è¾¾å¦å¤–æŸä¸ªç½‘é¡µã€‚ &
                åœ¨ä¸€ç³»åˆ—ç ”ç©¶ä¸­å¾—åˆ°çš„ç½‘é¡µçš„å¹³å‡å…¥åº¦å¤§æ¦‚ä» 8 åˆ° 15 å·¦å³ä¸ç­‰
                è¯¥å›¾ä¸æ˜¯å¼ºè¿é€šå›¾ï¼Œå› ä¸º B ä¸å¯èƒ½åˆ° A
                æœ‰å¤§é‡ç ”ç©¶è¡¨æ˜è¿™ä¸ªåˆ†å¸ƒæ»¡è¶³å¹‚åˆ†å¸ƒå®šå¾‹ï¼ˆ power lawï¼‰ï¼Œå…·æœ‰å…¥åº¦ä¸ºiçš„ç½‘é¡µæ€»æ•°ç›®æ­£æ¯”äº 1/iÎ±ï¼Œç ”ç©¶ä¸­ä¸€ä¸ªæœ‰ä»£è¡¨æ€§çš„Î±å€¼æ˜¯ 2.1
                æ•´ä¸ªWebæœ‰å‘å›¾ç»“æ„ï¼Ÿ & -
                    æ˜¯ä¸ªè´è¶ç»“(bowtie)å½¢ 
                    åˆ†åˆ«æ˜¯INã€ OUTå’ŒSCCå’Œç®¡é“
                    è¿˜æœ‰ä¸€äº›ä¸èƒ½ä» IN åˆ°è¾¾æˆ–è€…ä¸èƒ½åˆ°è¾¾ OUTçš„ç½‘é¡µæ„æˆçš„æ‰€è°“å·é¡»ï¼ˆ tendril , tubes , disconnect pagesï¼‰
            ä½œå¼Šé—®é¢˜ï¼š 
                å‡ ç§æ–¹æ³•ï¼Ÿ & -
                    å°†é‡å¤çš„è¯è®¾ç½®æˆå’ŒèƒŒæ™¯ä¸€æ ·çš„é¢œè‰²
                    æ ¹æ®ä¸åŒçš„è®¿é—®è¯·æ±‚ï¼ˆwebé‡‡é›†å™¨å’Œç”¨æˆ·ï¼‰ï¼Œä½œå¼Š Web æœåŠ¡å™¨ä¼šè¿”å›ä¸åŒçš„ç½‘é¡µç»“æœ

                    ::
                        set the color of the words the same as background 
                        use different ways to answer IR systems and Users 
                æ“ä½œç½‘é¡µå†…å®¹æ¥è¾¾åˆ°åœ¨æŸäº›å…³é”®è¯çš„æœç´¢ç»“æœä¸­æ’åè¾ƒé«˜çš„ç›®çš„
                ä¸€äº›è€ç»ƒçš„ä½œå¼Šè€…è¿˜ä¼šé‡‡ç”¨ä¸€äº›æ‰‹æ®µå’ŒæŠ€å·§ï¼Œæ¯”å¦‚ï¼Ÿ 
                    å°†é‡å¤çš„è¯è®¾ç½®æˆå’ŒèƒŒæ™¯ä¸€æ ·çš„é¢œè‰²
                å¾ˆå¤šç½‘é¡µå†…å®¹çš„å»ºè®¾è€…éƒ½æœ‰å•†ä¸šåŠ¨æœºï¼Œå› æ­¤ä»–ä»¬å¸Œæœ›é€šè¿‡æ“ä½œæœç´¢å¼•æ“çš„ç»“æœæ¥è·ç›Š
                åœ¨å¾ˆå¤š Web æœç´¢å¼•æ“ä¸­ï¼Œæœ‰å¯èƒ½å¯ä»¥é€šè¿‡ä»˜è´¹æ¥å°†è‡ªå·±çš„ç½‘é¡µæ”¾å…¥åˆ°æœç´¢å¼•æ“çš„ç´¢å¼•ä¸­ï¼Œ
                    è¿™ä¸ªæ¨¡å‹ç§°ä¸ºä»˜è´¹æ”¶å½•ï¼ˆ paid inclusionï¼‰ã€‚
                å¯¹äºæ˜¯å¦å…è®¸ä»˜è´¹æ”¶å½•ã€ä»˜è´¹æ˜¯å¦ä¼šå½±å“æœç´¢å¼•æ“çš„æ’åç»“æœï¼Œä¸åŒçš„æœç´¢å¼•æ“ä¼šæœ‰ä¸åŒçš„æ”¿ç­–
                æ ¹æ®ä¸åŒçš„è®¿é—®è¯·æ±‚ï¼ˆwebé‡‡é›†å™¨å’Œç”¨æˆ·ï¼‰ï¼Œä½œå¼Š Web æœåŠ¡å™¨ä¼šè¿”å›ä¸åŒçš„ç½‘é¡µç»“æœ 
                æ›´å¤æ‚çš„ä½œå¼ŠæŠ€æœ¯è¿˜åŒ…æ‹¬æ“çºµä¸ç½‘é¡µç›¸å…³çš„å…ƒæ•°æ®åŠæŒ‡å‘ç½‘é¡µçš„é“¾æ¥ç­‰
                ä»–ä»¬ä¹‹é—´çš„æ–—äº‰å°†æ°¸ä¸åœæ­¢
                ç ”ç©¶é¢†åŸŸé‡Œä¹Ÿå‡ºç°äº†ä¸€ä¸ªè¢«ç§°ä¸ºå¯¹æŠ—å¼ä¿¡æ¯æ£€ç´¢ï¼ˆ adversarial information retrievalï¼‰çš„å­é¢†åŸŸ **
                æœ€æ—©å¤§è§„æ¨¡ä½¿ç”¨é“¾æ¥åˆ†ææ–¹æ³•çš„æœç´¢å¼•æ“ä»ç„¶æ˜¯ Googleï¼šé˜²æ­¢ä½œå¼Š
            å¹¿å‘Šç»æµæ¨¡å‹
                Web çš„äº¤äº’æ€§ä½¿å¾— CPCä»˜è´¹æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œç”¨æˆ·çš„ç‚¹å‡»å¯ä»¥è¢«ç½‘ç«™è®°å½•å’Œç›‘æ§ï¼Œç„¶åæ ¹æ®è®°å½•çš„æƒ…å†µå°±å¯ä»¥ç»™å¹¿å‘Šå•†å¯„å»è´¦å•ã€‚
                å¯¹äºç”¨æˆ·çš„æŸ¥è¯¢ï¼Œæœç´¢å¼•æ“ä¼šå°†â€œ çº¯â€ æœç´¢ç»“æœï¼ˆé€šå¸¸ä¹Ÿè¢«ç§°
                    ä¸ºåŸºäºç®—æ³•çš„æœç´¢ç»“æœï¼‰ä½œä¸ºä¸»è¦ç»“æœè¿”å›ç»™ç”¨æˆ·ï¼ŒåŒæ—¶èµåŠ©æœç´¢ç»“æœåœ¨ç®—æ³•ç»“æœçš„å³ä¾§ç‹¬ç«‹
                    å¹¶æœ‰åŒºåˆ«æ€§åœ°æ˜¾ç¤ºå‡ºæ¥ã€‚
                é£æœºå¹¿å‘Šçš„ç¼ºä¹ä¹Ÿåæ˜ äº†å‡ ä¹æ²¡æœ‰äººä¼šåœ¨ Web ä¸Šå‡ºå”® A320 é£æœºè¿™ä¸ªäº‹å®
                ä¸€é—¨è¢«ç§°ä¸º SEMï¼ˆ Search Engine Marketingï¼Œæœç´¢å¼•æ“è¥é”€
                æ¯”å¦‚ï¼Œä¸€ä¸ªå¿ƒæœ¯ä¸æ­£çš„å¹¿å‘Šå•†å¯èƒ½ä¼šè¯•å›¾é€šè¿‡é‡å¤ç‚¹å‡»ï¼ˆä½¿ç”¨æœºå™¨ç‚¹å‡»ç”Ÿæˆå™¨ï¼‰å…¶ç«äº‰è€…çš„èµåŠ©æœç´¢å¹¿å‘Šæ¥è€—å°½å…¶å¹¿å‘Šé¢„ç®—
                ä½†æ˜¯ç ´åå½¢å¼ï¼Ÿ & -
                    å…¶ä¸­ä¸€ç§è¢«ç§°ä¸ºåƒåœ¾ç‚¹å‡»ï¼ˆ click spamï¼‰  

            ç”¨æˆ·ä½“éªŒ
                ä¸€ç³»åˆ—çš„ç ”ç©¶ç»“æœéƒ½è¡¨æ˜ï¼Œ Web æœç´¢ä¸­çš„å¹³å‡æŸ¥è¯¢å…³é”®è¯ä¸ªæ•°å¤§æ¦‚æ˜¯ 2 åˆ° 3 ä¸ª
                googleçš„ç”¨æˆ·ä½“éªŒç‰¹ç‚¹ï¼š & -
                    å…³æ³¨ç›¸å…³æ€§ï¼Œç‰¹åˆ«æ˜¯æ’åé å‰çš„ä¸€äº›ç»“æœçš„æ­£ç¡®ç‡è€Œä¸æ˜¯å¬å›ç‡
                    ç”¨æˆ·ä½“éªŒè¦è½»é‡çº§ï¼Œä¹Ÿå°±æ˜¯è¯´æŸ¥è¯¢é¡µé¢å’Œè¿”å›ç»“æœé¡µé¢åº”è¯¥ç®€æ´æ•´é½ï¼Œ
                    å¹¶ä¸”è¿™äº›é¡µé¢ä¸ŠåŸºæœ¬æ²¡æœ‰å›¾åƒæˆåˆ†ï¼Œè€Œåº”è¯¥å‡ ä¹å…¨æ˜¯æ–‡æœ¬å†…å®¹

                    lightweight 
                    no image on the result page , it is clean and pretty 
                    no too many ads on the page 
                æ™®é€šçš„ Web æœç´¢æŸ¥è¯¢ä¼¼ä¹å¯ä»¥åˆ†æˆå“ªä¸‰å¤§ç±»ï¼Ÿ  & -
                    (i) ä¿¡æ¯ç±»ï¼ˆ informationalï¼‰ï¼› 
                        ç”¨æˆ·è¾“å…¥ä¿¡æ¯ç±»æŸ¥è¯¢çš„ç›®çš„å¾€å¾€æ˜¯æƒ³ä»å¤šä¸ªä¸åŒçš„ç½‘é¡µä¸­æŠ½å–ä¿¡æ¯
                    (ii) å¯¼èˆªç±»ï¼ˆ navigationalï¼‰ï¼› 
                        è¿™ç§æƒ…å†µä¸‹ï¼Œç”¨æˆ·æœ€æœŸæœ›çš„ç»“æœå°±æ˜¯æ±‰èèˆªç©ºå…¬å¸çš„ä¸»é¡µå‡ºç°åœ¨æœç´¢ç»“æœçš„ç¬¬ä¸€ä¸ªä½ç½®ã€‚
                    (iii) äº‹åŠ¡ç±»ï¼ˆ transactionalï¼‰ã€‚
                        äº§å“è´­ä¹°ã€æ–‡ä»¶ä¸‹è½½æˆ–è¿›è¡Œé¢„è®¢ç­‰ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæœç´¢å¼•æ“åº”è¯¥åœ¨ç»“æœä¸­åˆ—ä¸¾å‡º
                        ä¸€äº›æœåŠ¡ï¼Œå®ƒä»¬èƒ½å¤Ÿæä¾›ä¸Šè¿°äº‹åŠ¡å¤„ç†çš„äº¤äº’æ¥å£
                    åˆ†ç±»å¹¶ä¸å®¹æ˜“ï¼Œè€Œç±»åˆ«ä¿¡æ¯ä¸ä»…å¯ä»¥ç”¨äºæ§åˆ¶åŸºäºç®—æ³•çš„æœç´¢ç»“æœï¼Œä¹Ÿå¯ä»¥ç”¨äºæŸ¥è¯¢å’ŒèµåŠ©æœç´¢ç»“æœçš„åŒ¹é…å½“ä¸­
                    è™šçº¿ä¸‹é¢çš„éƒ¨åˆ†å±äºæœç´¢å¼•æ“çš„å†…éƒ¨ç»“æ„ã€‚
            Webè§„æ¨¡ä¼°è®¡
                ä¼°è®¡å‡ºæŸä¸ªæœç´¢å¼•æ“çš„ç´¢å¼•è§„æ¨¡å æ•´ä¸ªWebçš„æ¯”ä¾‹ä»ç„¶æ˜¯éå¸¸å›°éš¾çš„ï¼Œè¿™æ˜¯å› ä¸ºWebä¸­å­˜åœ¨æ— æ•°çš„åŠ¨æ€ç½‘é¡µ
                æ¯æ¬¡æœç´¢è¿‡ç¨‹å½“ä¸­ä¸ä¸€å®šä¼šè°ƒç”¨æ‰€æœ‰çš„ç´¢å¼•
                ä¸€ä¸ªç½‘ç«™çš„æ·±å±‚é¡µé¢å¯èƒ½ä¼šè¢«ç´¢å¼•ï¼Œè€Œåœ¨ä¸€èˆ¬çš„
                    Web æœç´¢ä¸­å¹¶ä¸ä¼šè¢«è¿”å›ã€‚ä½†æ˜¯ï¼Œå¦‚æœç”¨æˆ·é™å®šåœ¨è¯¥ç½‘ç«™å†…æœç´¢ï¼ˆå¤§å¤šæ•°æœç´¢å¼•æ“éƒ½æä¾›äº†é™
                    å®šåœ¨æŸä¸ªç½‘ç«™è¿›è¡Œæœç´¢çš„åŠŸèƒ½ï¼‰æ—¶ï¼Œåˆ™è¯¥é¡µé¢ä¼šè¢«è¿”å›
                å¤šç±»é¡µé¢ï¼Œå› æ­¤æ— æ³•é‡‡ç”¨å•ä¸€æŒ‡æ ‡æ¥è¡¡é‡ç´¢å¼•è§„æ¨¡
                ä¸€ç§æ–¹æ³• & - NEXT exp
                    åˆ©ç”¨å…³äºE1 å’ŒE2 æ˜¯Webä¸­ç‹¬ç«‹å‡åŒ€éšæœºæŠ½æ ·å­é›†çš„å‡è®¾è¿›è¡Œå¼•æ“è§„æ¨¡ä¼°è®¡ 
                    å‡åŒ€éšæœºé€‰å‡ºä¸€ä¸ªç½‘é¡µç”¨æ¥è¿›è¡Œæµ‹è¯•æ˜¯éå¸¸å›°éš¾çš„
                    ::
                        count (More or less public) private pages
                        use the approch called â€œmark and recaptureâ€
                            ï¼ˆused in ecology to estimate an animal population's sizeï¼‰
                        let f be the number of first crawl , s second and b both of them 
                        ...
            webæŠ“å–æ¦‚è¿°
                ä»Webä¸­æ”¶é›†ç½‘é¡µï¼Œé‡‡é›†çš„ç›®æ ‡æ˜¯å°½å¯èƒ½é«˜æ•ˆåœ°é‡‡é›†æ›´å¤šæ•°ç›®çš„æœ‰ç”¨é¡µé¢
                åŒæ—¶è·å¾—è¿æ¥è¿™äº›é¡µé¢çš„é“¾æ¥ç»“æ„
                Webå¤æ‚æ€§çš„æ ¹æºåœ¨äºå…¶åˆ›å»ºçš„æ— åè°ƒæ€§
                Webé‡‡é›†å™¨ï¼ˆ web crawlerï¼‰
                åŠŸèƒ½
                    é²æ£’æ€§ï¼Œé‡‡é›†å™¨å¿…é¡»è¦èƒ½ä»è¿™ç±»é™·é˜±ä¸­è·³å‡ºæ¥ã€‚å½“ç„¶ï¼Œè¿™äº›é™·é˜±å€’ä¸ä¸€å®šéƒ½æ˜¯æ¶æ„çš„ï¼Œæœ‰æ—¶å¯èƒ½æ˜¯ç½‘ç«™è®¾è®¡ç–å¿½æ‰€å¯¼è‡´çš„ç»“æœ
                    æ§åˆ¶è®¿é—®é¢‘ç‡ï¼šè®¾è®¡é‡‡é›†å™¨æ—¶å¿…é¡»è¦éµå®ˆè¿™äº›ä»£è¡¨ç¤¼è²Œæ€§çš„è®¿é—®ç­–ç•¥
                    åˆ†å¸ƒå¼ï¼šé‡‡é›†å™¨åº”è¯¥å¯ä»¥åœ¨å¤šæœºä¸Šåˆ†å¸ƒå¼è¿è¡Œ
                    å¯æ‰©å±•æ€§ï¼šåœ¨å¢åŠ é¢å¤–çš„æœºå™¨å’Œå¸¦å®½çš„æƒ…å†µä¸‹ï¼Œé‡‡é›†å™¨çš„æ¶æ„åº”è¯¥å…è®¸å®ç°é‡‡é›†ç‡çš„æé«˜
                    æ–°é²œåº¦ï¼šåœ¨å¾ˆå¤šåº”ç”¨ä¸­ï¼Œé‡‡é›†å™¨éƒ½å¤„äºè¿ç»­å·¥ä½œçŠ¶æ€ï¼ŒæŠ“å–æ–°çš„åè®®ç­‰
                å®ç°
                    æ•´ä¸ªé‡‡é›†è¿‡ç¨‹å¯ä»¥çœ‹æˆæ˜¯ Web å›¾çš„éå†è¿‡ç¨‹
                    åœ¨æŠ“å–é«˜è´¨é‡ç½‘é¡µçš„åŒæ—¶ï¼Œé‡‡é›†å™¨è¦æ»¡è¶³åˆ†å¸ƒå¼ã€è§„æ¨¡å¯æ‰©å±•ã€é«˜æ•ˆã€ç¤¼è²Œæ€§ã€é²æ£’æ€§åŠåŠŸèƒ½å¯æ‰©å±•ç­‰ä¸€ç³»åˆ—è¦æ±‚ã€‚
                    Mercator é‡‡é›†å™¨
                        éœ€è¦åœ¨æ¯ç§’ä¹‹å†…æŠ“å–å‡ ç™¾ä¸ªç½‘é¡µ
                        äº”ä¸ªæ¨¡å— & 
                            DNS è§£ææ¨¡å—--ç”¨äºåˆ†æipåœ°å€
                            *fatchæ¨¡å—--åˆ©ç”¨ http åè®®è¿”å›æŸä¸ª URL å¯¹åº”çš„ç½‘é¡µ
                            URL æ± --å­˜æ”¾ç­‰å¾…é‡‡é›†çš„URL
                                å½“æŸä¸ª URL åŠ å…¥åˆ° URL æ± æ—¶ï¼Œå®ƒä¼šè¢«åˆ†é…ä¸€ä¸ªä¼˜å…ˆçº§ï¼ŒåŸºäºè¿™ä¸ªä¼˜å…ˆçº§æ¥å†³å®šå…¶æœ€ç»ˆå‡ºåˆ—çœŸæ­£è¿›è¡Œé‡‡é›†çš„æ—¶æœº
                            åˆ†æï¼ˆ parseï¼‰æ¨¡å—ï¼šä»é‡‡é›†åˆ°çš„ç½‘é¡µä¸­æŠ½å–æ–‡æœ¬åŠé“¾æ¥
                            URL å»é‡æ¨¡å—ï¼šç¡®å®šæŸä¸ªæŠ½å–å‡ºçš„é“¾æ¥æ˜¯å¦å·²åœ¨ URL æ± ä¸­æˆ–è€…æœ€è¿‘æ˜¯å¦å·²æŠ“è¿‡
                        çº¿ç¨‹æ‰§è¡Œä¸Šè¿°çš„æµç¨‹å›¾ï¼Œå…¶å¯ä»¥è¿è¡Œåœ¨å•ä¸ªè¿›ç¨‹ä¸­ï¼Œæˆ–è€…åˆ†å¼€åˆ°åˆ†å¸ƒå¼ç³»ç»Ÿçš„ä¸åŒèŠ‚ç‚¹çš„å¤šä¸ªè¿›ç¨‹å½“ä¸­
                        ç»´æŒä¸€ä¸ªrobots.txtæ–‡ä»¶çš„ç¼“å­˜ åŸå› ï¼Ÿ &
                            åœ¨çœŸæ­£æŠ“å–ç½‘é¡µä¹‹å‰è¿›è¡Œé‡‡é›†é™åˆ¶æ£€æµ‹ä¸ç®¡æ˜¯è¿™å‡ å¤©è¿˜æ˜¯ä¹‹åå‡ å¤©å¯¹å…¶è¿›è¡Œè®¿é—®
                    åˆ†å¸ƒå¼é‡‡é›†
                        å¦‚ä½•åˆ’åˆ†ï¼Ÿ 
                        1.åœ°ç†ä½ç½®åˆ’åˆ†--æ¯”å¦‚ï¼Œåœ°ç†ä½ç½®åœ¨æ¬§æ´²çš„é‡‡é›†å™¨ä¸»è¦å…³æ³¨æ¬§æ´²åŸŸåä¸‹ç½‘ç«™çš„é‡‡é›†ã€‚å½“ç„¶ï¼Œè¿™ç§åšæ³•å¹¶ä¸å®Œå…¨å¯é ï¼Œ
                            åŸå› åŒ…æ‹¬ï¼š Internet ä¸Šæ•°æ®åŒ…çš„ä¼ è¾“è·¯çº¿å¹¶ä¸ä¸€å®šåæ˜ åœ°ç†ä½ç½®çš„é‚»è¿‘æ€§ã€‚ &
                            å¹¶ä¸”ï¼Œåœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œä¸»æœºåŸŸåå¹¶ä¸æ€»æ˜¯åæ˜ å…¶å®é™…çš„ç‰©ç†ä½ç½®
                        2.ä½¿ç”¨ä¸»æœºåˆ’åˆ†å™¨ï¼ˆ host splitterï¼‰å°†é€šè¿‡è¿‡æ»¤æ£€æµ‹çš„ URL åˆ†é…åˆ°ä¸åŒçš„é‡‡é›†èŠ‚ç‚¹ä¸Šå» &
                            é‡å¤æ£€æµ‹æ¨¡å—ä¼šå¾ˆå¤æ‚ï¼š
                            å¿…é¡»è¦åŸºäºæŒ‡çº¹æˆ– shingle é›†åˆçš„æŸäº›æ€§è´¨å¯¹å®ƒä»¬è¿›è¡ŒèŠ‚ç‚¹åˆ’åˆ†
                            æ–‡æ¡£å˜åŒ–--å°†æ–‡æ¡£çš„æŒ‡çº¹æˆ– shingle è¿åŒ URL æœ¬èº«ä¸€èµ·æ”¾å…¥ URL æ± ä¸­
            webä¸­çš„é‡å¤é—®é¢˜
                ç™¾åˆ†ä¹‹40çš„ç½‘é¡µæ˜¯é‡å¤çš„ï¼šæ¯”å¦‚ä¿¡æ¯åº“çš„å¤šä»½é•œåƒ
                solution1 :è®¾è®¡æŒ‡çº¹æ¯”å¦‚64ä½
                    é—®é¢˜ï¼šå¯èƒ½åªæ˜¯æœ€åä¸¤ä¸ªå­—ç¬¦ä¸åŒ
                solution 2:k shinglingæŠ€æœ¯ &
                    æ­å æ£€æµ‹ï¼Œå’Œngramç±»ä¼¼ï¼Œè§p362
                    shingling ç®—æ³•ï¼šå¦‚æœè®¡ç®—ä¸¤ä¸ªç½‘é¡µç›¸äº’ç±»ä¼¼ï¼Œå°±å¯ä»¥å»æ‰å…¶ä¸­ä¸€ç¯‡ï¼Œä¸è¿›è¡Œç´¢å¼•
                    ç®—æ³•æœ‰ä¸åŒç‰ˆæœ¬ï¼Œè¿½æ±‚æ•ˆç‡çš„æœ€å¤§åŒ–ï¼Œå› ä¸ºæ–‡ç« æ•°ç›®å¤ªå¤šäº†
                    å¦‚ä½•ç¼©å‡è®¡ç®—ï¼Ÿ
            ---------------------------------------------------------------------------------------------------------
            IR on the web vs. IR in general 
                ä¸‰ç‚¹ä¸åŒ & 
            Big Picture
            Size of the WEB
                what should be counted &
                    å››ä¸ªä¸œè¥¿
                    Count them if they can be accessed by a large number of people
                How to find all Web pages? &
                    1.simply crawl the complete Web and count its number of pages
                        This doesnâ€™t work due to the Webâ€™s enormous size 
                    2.better approach : mark and recapture 
                        ç”Ÿæ€ä¸­ç§ç¾¤ä¼°ç®—
                        use two crawl,first and second 
                    In 2005, the Web has been estimated to contain at least 11.5 billion pages
                    now around 60 billion pages
            Web IR
                Differences from traditional IR &
                a bow tie 
                    wie oben 
                æ›´å¤šçš„ä¿¡æ¯éœ€è¦ 
                    wie oben 
                    Navigational and transactional
                    Difficult problem: How can the search engine tell what the user need or intent for a particular query is?
                    æŒ‰ç…§ç”¨æˆ·æ„å›¾ä¿®æ”¹æœç´¢ç»“æœ
                        åå¯¹ï¼šAvoid the filter bubbleï¼š æ–‡åŒ–æ€æƒ³æ³¡æ³¡
                ç½‘ç»œæœç´¢çš„å…·ä½“å½¢å¼ç‰¹ç‚¹ & 
                    Use short queries (average < 3)
                        Donâ€™t want to spend a lot of time on composing a query
                    Only look at the first couple of results
                è¯„ä»·
                    Classic IR relevance
                    Trust, duplicate elimination, readability, loads fast, no popups
                    On the web, precision is more important than recall
                ç½‘ç»œæ–‡æ¡£ç‰¹ç‚¹
                    ä¸»è¦çš„å››ä¸ªç‰¹æ€§ & -
                        1.å„ç§æ ¼å¼çš„æ–‡æ¡£éƒ½æœ‰
                            Most (truly) dynamic content : is ignored by web spiders -- 
                                itâ€™s too much to index it all
                            Unstructured (text, html), 
                            semi-structured (html, xml),
                            structured/relational (databases)
                        2.å¤šè¯­è¨€æ€§ 
                            Documents in a large number of languages
                            Queries in a large number of languages
                            Donâ€™t return English results for a Japanese query
                            query/document languages are Frequent mismatches

                            ç‰¹æ®Šçš„æŸ¥è¯¢ï¼šCross-language information retrieval (CLIRï¼‰ è·¨è¯­è¨€æŸ¥è¯¢
                        3.é‡å¤æ€§ 
                            see before
                            Todayâ€™s search engines eliminate duplicates very effectively
                        4.å¯ä¿¡åº¦
                            Hoaxes abound éª—å±€
            Crawling
                ä¸¤ä¸ªç‰¹ç‚¹ï¼Ÿ & 
                    should be distributed, scalable, efficient, polite, robust

                    HÃ¶flichkeit--
                        um eine Ãœberlastung des Web-- Servers zu vermeiden
                    Robustheit--
                        gegen UngÃ¼ltige/unvollstÃ¤ndige HTMLDokumente
                        Netzwerkprobleme (z.B. hohe Latenzzeit / niedrige Bandbreite des Servers)
                pptä¸­ä¾‹å­ & - ç†è§£ï¼Œç»™å›¾ç”»å‡ºæ¥è·¯å¾„
                    é¡µé¢çš„å®½åº¦æœ‰é™æœç´¢ 
                æœ‰å“ªäº›æŒ‘æˆ˜ï¼Ÿï¼š & -
                    å››ä¸ª
                        Skalierbarkeitå¯æ‰©å±•æ€§
                            Mehr Rechner erhÃ¶hen LeistungsfÃ¤higkeit
                        AktualitÃ¤tå®æ—¶æ€§
                        Verteiltes System
                        QualitÃ¤tsbewusstsein
            Ads
                åæ¥ï¼šæœç´¢ä¸å¹¿å‘Šä¸¥æ ¼åˆ†å¼€ï¼šStrict separation of search results and search ads
                How are ads ranked  & - exp
                    rank based on bid price AND relevance(two things)
                    Other ranking factors: 
                        location, time of day, loading speed of landing page
                    å‚æ•°ï¼š bid CTR ad rank rank paid p47
                Googleâ€™s second price auction 
                    æ¬¡é«˜ä»·æ‹å– & -è§£é‡Š and calcuã€‚
                        è®¡ç®— 
                        p47
                é—®é¢˜ï¼š & -
                    ä¸»è¦çš„ä¸¤ä¸ªé—®é¢˜ 
                    Keyword arbitrage
                        we can buy a keyword on google 
                        å°†æµé‡é‡å®šå‘åˆ°æ”¯ä»˜æ¯”è°·æ­Œå¤šå¾—å¤šçš„ç¬¬ä¸‰æ–¹
                    Violation of trademarks
                        The search term â€œgeicoâ€ on Google was bought by competitorsä¾µæƒ
            Duplication Detection
                //
                    
                            solution1 :è®¾è®¡æŒ‡çº¹æ¯”å¦‚64ä½
                            solution 2:k shinglingæŠ€æœ¯ 
                25-40% of the web is duplicate (mirrors - e.g. Wikipedia, unix man pages, SPAM sites)
                we should use Near-duplicates
                åŸå› ï¼š & -
                    ç”¨æˆ·ä½“éªŒ 
                    è‡ªèº«æ€§èƒ½Conserve resources: 
                        reduced index size - less memory, faster computations, etc.
                True semantic similarity (similarity in content) is too difficult to compute
                å‡ ç§åŸºæœ¬çš„åšæ³• & -
                    Hashing
                    ç¼–è¾‘è·ç¦» 
                    shinglingç®—æ³•ï¼Œé€šè¿‡åˆ†å‰²è½¬æ¢æˆé›†åˆï¼Œç„¶åè®¡ç®—Jaccard coefficient
                        ç”¨äºè®¡ç®—ä¸¤ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼åº¦ï¼Œä¾‹å¦‚ï¼Œç”¨äºç½‘é¡µå»é‡
                        -r 
                            Summarize each document in a short sketchï¼Œand then Estimate the similarity based on the sketches
                            look at the percentage of min-hashes that agree **
                            with different hash functions(doc and hash table )
                            
                            use MinHash
                                åŸå› ï¼Ÿ & - NEXT 
                                calcu &
                                    ä½¿ç”¨ä¸‰å…ƒç»„ï¼Ÿ ï¼Ÿï¼Ÿ NEXT
                                    Store the minimum hash: {143}
                                    Sim(A,B)=ç›¸åŒhashä¸ªæ•°/æ€»çš„ä¸ªæ•°
                                    å…¶å®ä¹Ÿæ˜¯åœ¨è®¡ç®—Jaccard coefficient
                            a sketch for a document is a collection of minimum hashes
                            use 84 hash functions
                            Summarized each page in 672 bytes (84*8 byte values)
                        Super Shingles
                            -r 
                                Doing all pairwise comparisons still too expensive
                                package the Shingles into super Shingles : doc - s-Shingle table  


                                å†æ¬¡å“ˆå¸Œ **
                                    Group sketch into non overlapping super-shingles 
                                    Hash each super-shingle {1011, 6543, ..., 7327}
                                    ä¾‹å­ & - p65
                                        Declare doc 2 and doc 3 to be 2-similar
                                In practice - store the above table sorted by different columns
                                    Only compare against neighboring rows
                                    Store the super shingle table sorted by columns
                        å¯¹äºä¸‹é¢çš„å¦‚ä½•å»é‡? & - exp
                                some Open Problems 
                                Flash, Ajax, and other not easily indexable content
                                Obtaining text from raw HTML is not as easy as it sounds
            SPAM detection
                åƒåœ¾é“¾æ¥åˆ†æ
                You have a page that will generate lots of
                    revenue for you if people visit it
                It damages the search engineâ€™s reputation
                Google no longer gives good rankings to
                    pages employing this technique
                å¸¸è§çš„SpamæŠ€æœ¯ &x2
                    plus oben+ 
                        Misleading meta-tags, excessive repetition
                        Doorway page
                    Landing Page 
                    Serve fake content to search engine spider
                    Link spam 
                        Put these links on pages with high (or at least non-zero) PageRank
                åSpam
                It can also be a legitimate business â€“ which is called SEO
                Google bombs **
                The war against spam
                and there is one thing called ::Webmaster Guidelines & -
                lack of central access control! & -
                Major search engines have guidelines for webmasters
                Ignore these guidelines at your own risk
                Once a search engine identifies you as a spammer, 
                    all pages on your site may get low ranks (or
                    disappear from the index entirely) **
                There is often a fine line between spam and legitimate SEO **
        ppt-14-PageRank und HITS ç¬¬ 21 ç«  é“¾æ¥åˆ†æ --over --c 
            é“¾æ¥ç»“æ„ä¿¡æ¯åœ¨ Web æœç´¢ç»“æœæ’åºä¸­çš„ä½¿ç”¨--é“¾æ¥åˆ†æ
            é“¾æ¥åˆ†æç»“æœå·²ç»æˆä¸º Web æœç´¢å¼•æ“åœ¨è®¡ç®—æŸä¸ªç½‘é¡µçš„ç»„åˆå¾—åˆ†ä¸­çš„ä¸€ä¸ªå› å­
            Web æœç´¢ä¸­é“¾æ¥åˆ†ææ€æƒ³çš„æœ€æ—©èµ·æºäºå¼•æ–‡åˆ†æé¢†åŸŸï¼Œåè€…åœ¨å¾ˆå¤šæ–¹é¢ä¸ä¸€ä¸ªè¢«ç§°ä¸ºæ–‡çŒ®
                è®¡é‡å­¦ï¼ˆ bibliometricsï¼‰çš„é¢†åŸŸæœ‰äº¤å‰ã€‚è¿™äº›å­¦ç§‘è¯•å›¾é€šè¿‡åˆ†ææ–‡çŒ®ä¹‹é—´çš„å¼•ç”¨æ¨¡å¼æ¥é‡åŒ–å­¦æœ¯
                è®ºæ–‡çš„å½±å“åŠ›ã€‚
            é“¾æ¥åˆ†æçš„æ„ä¹‰ & -
                Web ä¸Šçš„é“¾æ¥åˆ†ææ–¹æ³•ä¹ŸæŠŠè¶…é“¾æ¥çœ‹æˆæ˜¯ä¸€ä¸ªç½‘é¡µå¯¹å¦ä¸€ä¸ªç½‘é¡µçš„æƒå¨åº¦çš„è®¤å¯
                find pages with high authority
            ä»…ä»…ç®€å•åœ°é€šè¿‡å…¥é“¾æ¥çš„æ•°ç›®æ¥è¡¡é‡ç½‘é¡µçš„è´¨é‡æ˜¯ä¸å¤Ÿé²æ£’çš„ã€‚
                åƒåœ¾é“¾æ¥
            å¦‚ä½•é€‰æ‹©ä¸‹ä¸€ä¸ªé‡‡é›†ç½‘é¡µï¼Ÿ
            é“¾æ¥åˆ†æçš„ç ”ç©¶ä¸»è¦åŸºäºä¸¤ä¸ªåŸºæœ¬ç›´è§‰
                å»æ‰è¿™äº›â€œ å†…éƒ¨â€ çš„é“¾æ¥
            å› æ­¤ï¼Œ Web ç½‘é¡µæœ¬èº«æºå¸¦çš„è¯é¡¹å’Œç”¨æˆ·ç”¨äºæè¿°åŒä¸€ç½‘é¡µçš„è¯é¡¹ä¹‹é—´å¾€å¾€å­˜åœ¨ç€ä¸€å®šçš„å·®å¼‚
            ä¸¾ä¾‹ & -
                å¾ˆå¤šæŒ‡å‘www.ibm.comçš„é“¾æ¥ä¸Šçš„é”šæ–‡æœ¬éƒ½åŒ…å«å•è¯computerï¼Œè¿™ä¸ªäº‹å®å°±å¯ä»¥ä¸ºWebæœç´¢å¼•æ“æ‰€ä½¿ç”¨ 
            æ¯”å¦‚ï¼Œé”šæ–‡æœ¬ä¸­çš„è¯é¡¹å°±å¯ä»¥ä½œä¸ºç´¢å¼•ç›®æ ‡ç½‘é¡µçš„è¯é¡¹ã€‚å› æ­¤ï¼Œè¯é¡¹computerçš„å€’æ’è®°å½•è¡¨ä¸­å°±ä¼šåŒ…å«æ–‡æ¡£www.ibm.com
            åŒé¡µå†…è¯é¡¹ä¸€æ ·ï¼Œé€šå¸¸ä¹Ÿä¼šåŸºäºè¯é¢‘æ¥è®¡ç®—é”šæ–‡æœ¬è¯é¡¹çš„æƒé‡ **
            é‚£äº›åœ¨å¤šä¸ªé”šæ–‡æœ¬ä¸­é«˜é¢‘å‡ºç°çš„è¯é¡¹ï¼ˆå¦‚Webé”šæ–‡æœ¬ä¸­æœ€æ™®éçš„è¯é¡¹æ˜¯Clickå’Œhereï¼‰ä¼šå—åˆ°æƒ©ç½š
            æŸä¸ªç½‘ç«™å¯ä»¥é€šè¿‡æ„é€ å…·æœ‰è¯¯å¯¼æ€§çš„é”šæ–‡æœ¬æ¥æŒ‡å‘è‡ªå·±ï¼Œä»è€Œæé«˜åœ¨æŸäº›æŸ¥è¯¢è¯é¡¹ä¸Šçš„æ’å
            åœ¨webæ£€ç´¢å·¥å…·æ„å»ºè¿‡ç¨‹ä¸­ï¼Œéœ€è¦å¾ˆå¤šä½œå¼Šæ£€æµ‹å·¥ä½œ
            å¾ˆå¤šå­¦è€…è¿˜å¯¹æœ‰æ•ˆçª—å£çš„å¤§å°è¿›è¡Œäº†ç ”ç©¶--ä½¿ç”¨å¤šå°‘é”šæ–‡æœ¬åˆé€‚ï¼Ÿ 
            webæœç´¢å¼•æ“ä¸åŒäºä¼ ç»Ÿæ–‡æ¡£é›†ï¼Ÿ 
            é™æ€PageRank
                formua & 
                    transport probability alpha .
                    the prob. of transporting from i to j  is:: alpha /n + ..  
                åœ¨éšæœºæ¸¸èµ°è¿‡ç¨‹ä¸­è®¿é—®è¶Šé¢‘ç¹çš„ç½‘é¡µä¹Ÿè¶Šé‡è¦
                å¯¹ Webå›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹èµ‹ä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„åˆ†å€¼ï¼Œè¿™ä¸ªåˆ†å€¼è¢«ç§°ä¸º PageRank
                    ç»™å®šæŸ¥è¯¢ï¼Œwebå¼•æ“ä¼šç»¼åˆå„ç§æŒ‡æ ‡ï¼š
                ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå‚è€ƒ 6.3 èŠ‚ï¼‰ã€è¯é¡¹é‚»è¿‘åº¦ï¼ˆ 7.2.2 èŠ‚ï¼‰åŠ PageRank ç­‰
                å‡è®¾å†²æµªè€…ä»¥1/Næ¦‚ç‡è·³å‘å…¶ä»–ä»»æ„ä¸€ä¸ªç½‘é¡µã€‚å½“ç„¶ï¼Œå†²æµªè€…ä¹Ÿä»¥ 1/Nçš„æ¦‚ç‡è·³åˆ°å…¶å½“å‰ä½ç½®
                è¿‡ç¨‹å‡è®¾ï¼š & -
                    1.å¦‚æœæ²¡æœ‰å‡ºé“¾ï¼Œéšæœºæ¸¸èµ°
                    2.å¦‚æœæœ‰å‡ºé“¾alpha:éšæœºè·³è½¬ï¼›1-alpha:ç»§ç»­è¿›è¡Œéšæœºæ¸¸èµ°
                ä»è€Œï¼Œé€šè¿‡é©¬å°”ç§‘å¤«é“¾è§„åˆ™ï¼Œå¾—çŸ¥ä»–å°±ä¼šä»¥ä¸€ä¸ªå›ºå®šçš„æ—¶é—´æ¯”ä¾‹ Ï€(v)è®¿é—®æ¯ä¸ªèŠ‚ç‚¹ v &
                +é©¬å°”ç§‘å¤«é“¾ï¼š &
                    é©¬å°”ç§‘å¤«é“¾æ˜¯ä¸€ä¸ªç¦»æ•£æ—¶é—´éšæœºè¿‡ç¨‹
                    Pijè¢«ç§°ä¸ºè½¬ç§»æ¦‚ç‡ï¼Œå®ƒä»…ä»…ä¾èµ–äºå½“å‰çš„çŠ¶æ€ iï¼Œè¿™ç§æ€§è´¨è¢«ç§°ä¸ºé©¬å°”ç§‘å¤«æ€§
                    é©¬å°”ç§‘å¤«é“¾ä¸­ï¼Œä¸‹ä¸€ä¸ªçŠ¶æ€çš„åˆ†å¸ƒä»…ä»…ä¾èµ–äºå½“å‰çš„çŠ¶æ€ï¼Œè€Œå’Œå¦‚ä½•åˆ°è¾¾å½“å‰çŠ¶æ€æ— å…³ã€‚
                    æ ¹æ®Pç”Ÿæˆçš„éšæœºçŸ©é˜µï¼Œæœ€å¤§ç‰¹å¾å€¼æ˜¯ 1
                    å›¾ä¸­é“¾æ¥ä¸Šçš„æ•°å­—ä»£è¡¨çš„æ˜¯è½¬ç§»æ¦‚ç‡
                    æœ¬è´¨æ˜¯ä¸€ç§å‘åè¿­ä»£è§„åˆ™ & -
                å›åˆ°ç½‘é¡µé—®é¢˜ï¼šå…¶ä¸­é©¬å°”ç§‘å¤«é“¾ä¸­çš„æ¯ä¸ªçŠ¶æ€å¯¹åº”ä¸€ä¸ªç½‘é¡µï¼Œ
                    è€Œæ¯ä¸ªè½¬ç§»æ¦‚ç‡ä»£è¡¨ä»ä¸€ä¸ªç½‘é¡µè·³è½¬åˆ°å¦å¤–ä¸€ä¸ªç½‘é¡µçš„æ¦‚ç‡
                æ ¹æ®ä¸Šé¢çš„è§„åˆ™ï¼Œç”±NÃ—Nçš„ä¸´è¡—çŸ©é˜µAæ¨å¯¼å‡ºé©¬å°”ç§‘å¤«é“¾çš„è½¬ç§»æ¦‚ç‡çŸ©é˜µP
                è¿™æ ·æˆ‘ä»¬åªéœ€ çŠ¶æ€åˆ†å¸ƒå’Œè½¬ç§»æ¦‚ç‡çŸ©é˜µ Pï¼Œå°±èƒ½è®¡ç®—å†²æµªè€…åœ¨ä»»ä¸€æ—¶åˆ»æ‰€å¤„çŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒ
                æˆ‘ä»¬å°† PageRank è®¾ç½®ä¸ºæ¯ä¸ªèŠ‚ç‚¹ v åœ¨ç¨³æ€ä¸‹çš„è®¿é—®é¢‘ç‡
                è®¡ç®—PageRankæƒé‡ & - p322
                    ergodic Markov chain
                    it will converge to a steady state


                    ç¬¬ä¸€ä¸ªæ¡ä»¶ä¿è¯ä»ä»»æ„ä¸¤ä¸ªçŠ¶æ€ä¹‹é—´éƒ½å­˜åœ¨éé›¶æ¦‚ç‡è½¬ç§»åºåˆ—ï¼Œè€Œç¬¬äºŒä¸ªæ¡ä»¶ä¿è¯ä¸å­˜åœ¨è¿™æ ·çš„çŠ¶æ€åˆ’åˆ†ï¼š
                        æ‰€æœ‰çš„çŠ¶æ€è½¬ç§»åªå‘ç”Ÿåœ¨ä¸¤ä¸ªåˆ’åˆ†åçš„çŠ¶æ€å­é›†ä¹‹é—´å¹¶å¾ªç¯å¾€å¤ã€‚
                    å¸¦éšæœºè·³è½¬æ“ä½œçš„æœ€åå¯ä»¥å¾—åˆ°å”¯ä¸€çš„ç¨³æ€æ¦‚ç‡åˆ†å¸ƒã€‚æŸä¸ªçŠ¶æ€çš„ç¨³æ€æ¦‚ç‡å°±æ˜¯ç›¸åº”ç½‘é¡µçš„ PageRank
                    åå¤è¿­ä»£ä¸€å®šæ¬¡æ•°ä¹‹åï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°åˆ†å¸ƒæ”¶æ•›äºä¸€ä¸ªç¨³å®šçŠ¶æ€ ** ä¾‹å­p322 & -
                    çŠ¶æ€å…·æœ‰å¯¹ç§°æ€§çš„æ—¶å€™å¯ä»¥ç›´æ¥è®¡ç®—ç¨³æ€æ¦‚ç‡åˆ†å¸ƒ **
                    ç½‘é¡µçš„ PageRank ä¸ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢æ— å…³--é™æ€è´¨é‡è¡¡é‡æŒ‡æ ‡
                    æ‰€ä»¥åªæ˜¯è¿™ä¸ªæŒ‡æ ‡ä½œä¸ºæ’åå› å­ä¹‹ä¸€
                ç‰¹ç‚¹
                ç½‘é¡µçš„ PageRank ä¸ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢æ— å…³
            é¢å‘ä¸»é¢˜çš„ PageRank
                -r 
                    use different PageRanks to calcu . when deal with different subjects .

                éç­‰æ¦‚ç‡è·³åˆ°ä¸€ä¸ªéšæœºç½‘é¡µ
                ä¸€ä¸ªä½“è‚²è¿·å¯èƒ½å¸Œæœ›æœ‰å…³ä½“è‚²ä¸»é¢˜çš„ç½‘é¡µçš„æ’åè¦é«˜äºéä½“è‚²ä¸»é¢˜çš„ç½‘é¡µ
                å¦‚æœä¸æ˜¯ä½“è‚²ç±»çš„æ–‡ç« ï¼Œä»¤å…¶PageRankä¸º 0 &
                    å®é™…ä¸Šï¼Œä½“è‚²ç›¸å…³çš„é›†åˆSæœªå¿…æœ‰ç¨³æ€
                    æ‰€ä»¥æ‰¾ä¸€ä¸ªæ¯”Sç¨å¾®å¤§ä¸€ç‚¹çš„é›†åˆYï¼Œæœ‰ç¨³æ€
                    é©¬å°”ç§‘å¤«é“¾ç¨³æ€çš„æ—¶å€™æ¯ä¸ªç½‘é¡µå…·æœ‰éé›¶å€¼
                    Y ä¸­çš„æ¯ä¸ªç½‘é¡µéƒ½æœ‰éé›¶ PageRank å€¼
                    åœ¨Yä¹‹å¤–çš„æ–‡ç« åˆ†æ•°å½’é›¶
                å„ä¸ªä¸ç”¨çš„ä¸»é¢˜ï¼šæ‰“åˆ†æ—¶å€™è°ƒç”¨ä¸åŒçš„PageRankè¿›è¡Œè®¡ç®—ï¼š &
                    æ¯”å¦‚ç§‘å­¦å®—æ•™ç­‰
                    å½“æœç´¢ç”¨æˆ·ä»…ä»…å¯¹æŸä¸ªä¸»é¢˜æ„Ÿå…´è¶£æ—¶ï¼Œé‚£ä¹ˆåœ¨å¯¹æ£€ç´¢ç»“æœæ‰“åˆ†å’Œæ’åè¿‡ç¨‹åªé¡»è°ƒç”¨ç›¸åº”ä¸»é¢˜çš„PageRankå‘é‡å€¼è¿›è¡Œè®¡ç®—
                    ç”¨æˆ·æœ‰å¯èƒ½æ˜¾å¼åœ°æ³¨å†Œäº†å…¶å…´è¶£ï¼Œæˆ–è€…ç³»ç»Ÿèƒ½å¤Ÿä»æ¯ä¸ªç”¨æˆ·çš„å†å²è¡Œä¸ºä¸­å­¦åˆ°å…¶å…´è¶£
                å¤„ç†æ··åˆçš„æƒ…å†µï¼Ÿ &
                    -r 
                        each user can have an individual PageRank on different topics 
                        we mix them with linear functions 
                    å®é™…ä¸Šä»»ä¸€ç”¨æˆ·çš„ä¸ªæ€§åŒ–PageRank éƒ½å¯ä»¥è¢«è¡¨ç¤ºæˆå¤šä¸ªé¢å‘ä¸»é¢˜çš„ PageRank çš„çº¿æ€§ç»„åˆ
                    0.6*pi1 + 0.4*pi2
                    åˆ†åˆ«æ˜¯é¢å‘ä½“è‚²å’Œ çš„ æ”¿æ²»ä¸»é¢˜ PageRank å‘é‡
                    å¦‚æœéšæœºè·³è½¬æ“ä½œçš„æ¦‚ç‡æ˜¯ 10%çš„è¯ï¼Œé‚£ä¹ˆå…¶ä¸­æœ‰ 6%æ˜¯åˆ°ä½“è‚²ç±»ï¼Œè€Œ 4%åˆ°æ”¿æ²»ç±»ç½‘é¡µ
            HITsåŸºäºæŸ¥è¯¢çš„hubæŒ‡æ•°å’Œæƒå¨æŒ‡æ•°--é’ˆå¯¹ä¿¡æ¯ç±»æ£€ç´¢
                ä¸¤ä¸ªæ’åºç»“æœåˆ—è¡¨ï¼Œå…¶ä¸­ä¸€ä¸ªåŸºäºhubå€¼ï¼Œè€Œå¦ä¸€ä¸ªåŸºäºauthorityå€¼
                ä¹‹é—´æ„æˆè‰¯æ€§å¾ªç¯ï¼Œå…±åŒç»„æˆä¸€ä¸ªWeb å­é›†
                    æ„é€ ï¼š
                        å°†æ ¹é›†åŠæŒ‡å‘æ ¹é›†ä¸­çš„ç½‘é¡µå’Œæ ¹é›†æ‰€æŒ‡å‘çš„ç½‘é¡µåŠ å…¥åˆ°åŸºæœ¬é›†ï¼ˆ base setï¼‰ä¸­
                        åˆ©ç”¨åŸºæœ¬é›†è¿›è¡Œè®¡ç®—
                        åŸå› æœ‰ä¸‰ &
                        å¯ä»¥è·å¾—è·¨è¯­è¨€çš„æ•ˆæœï¼šæ¯”å¦‚å¦‚æœæœ‰ä¸€ä¸ªè‹±æ–‡çš„ hub ç½‘é¡µæŒ‡å‘ä¸€ä¸ªæ—¥æ–‡æè¿°çš„æ—¥æœ¬å°å­¦çš„ä¸»é¡µï¼Œæœ€åè¿”å›çš„ç»“æœä¼šæœ‰ç±»ä¼¼ä¹±ç çš„æ•ˆåº”
                hub å€¼æŒ‡çš„æ˜¯è¯¥ç½‘é¡µçš„å¯¼èˆªèƒ½åŠ›ï¼Œä¹Ÿå¯ä»¥ç§°ä¸ºå¯¼èˆªåº¦ã€‚è€Œ authority å€¼æŒ‡çš„æ˜¯ç½‘é¡µçš„æƒå¨åº¦ã€‚
                è¿­ä»£ç®—æ³•çš„æ ¸å¿ƒç¯èŠ‚å°±æ˜¯ hub å€¼å’Œ authority å€¼çš„åŒé‡æ›´æ–°è¿‡ç¨‹
                è®¡ç®—æ¯ä¸ªç½‘é¡µçš„æ•°å€¼ï¼Ÿ &
                å¾ªç¯è¿­ä»£å…¬å¼ & -
                é‚»æ¥çŸ©é˜µè¡¨ç¤ºçš„å¾ªç¯å…¬å¼ &
                ä¸Šé¢é‚£ä¸ªå¼å­å°±å˜æˆçŸ©é˜µ AATçš„ç‰¹å¾æ–¹ç¨‹ï¼Œè€Œä¸‹é¢é‚£ä¸ªå¼å­åˆ™ä¼šå˜æˆçŸ©é˜µ ATA çš„ç‰¹å¾æ–¹ç¨‹
                é‚£ä¹ˆ h å’Œ a æœ€åä¼šæ”¶æ•›äºæŸä¸ªå”¯ä¸€çš„ç¨³æ€å‘é‡ï¼Œæœ€åçš„ç¨³å®šçŠ¶æ€å–å†³äºå›¾ç»“æ„ & **
                è®¡ç®—ä¼˜åŒ–ï¼Ÿ & - 
                    æ ¸å¿ƒå°±æ˜¯å˜æˆäº†æ±‚ä¸»ç‰¹å¾å‘é‡
                HITSï¼ˆ Hyperlink-Induced Topic Searchï¼Œè¶…é“¾å¯¼å‘çš„ä¸»é¢˜æœç´¢ï¼‰
            æ€»ç»“ï¼š
                é“¾æ¥åˆ†æçš„ç›®çš„ï¼šåˆ†æé“¾æ¥æƒå¨æ€§ï¼Œç»™ç½‘é¡µæ‰“åˆ†
                ä»»æ„è®¡ç®—ç‰¹å¾å‘é‡çš„ç®—æ³•éƒ½å¯ä»¥ç”¨äºè®¡ç®— hub å€¼å’Œ authority å€¼å‘é‡
                æˆ‘ä»¬å¹¶ä¸éœ€è¦è®¡ç®—è¿™äº›å€¼çš„ç²¾ç¡®å€¼ï¼Œè€Œåªéœ€è¦çŸ¥é“è¿™äº›å€¼çš„ç›¸å¯¹å¤§å°ä»¥ä¾¿èƒ½å¤Ÿè¿›è¡Œæ’åºå³å¯
                å®é™…ä¸Šå…¬å¼ï¼ˆ 21-8ï¼‰åªéœ€è¦å¤§æ¦‚ 5 æ¬¡è¿­ä»£å°±å¯ä»¥äº§ç”Ÿç›¸å½“å¥½çš„ç»“æœ
                è¿­ä»£æ›´æ–°è€Œä¸æ˜¯ç›´æ¥è®¡ç®—ï¼Œå› ä¸ºSparse
            -------------------------------------------------------------------------------------------
            Webgraph
            Linkankeré“¾æ¥æè¿°ï¼Œmaoæ–‡æœ¬
                Ankertexte
            Eigenschaften des WWW & -
                Nicht zusammenhÃ¤ngend ïƒ  unerreichbare Knoten
            é“¾æ¥åˆ†ææ–¹æ³•ï¼š PageRank å’Œ HITS
            PageRank
                Statisches QualitÃ¤tsmaÃŸ
                Why count links ? & -
                    Gute Inhalte werden hÃ¤ufiger verlinkt *** 
                    good content offen means more links 
                1.Random Surfer
                    ä¾‹å­ p14 & -
                    æœ‰å“ªäº›å›°éš¾ï¼Ÿ: & - 
                        Sackgassen
                        Anderes Problem: KreislÃ¤ufe
                2.Modellierung &
                    using stochastisch Matrix
                    calcu & -
                        æœ€åç»™å‡ºRankingç»“æœ
                    p23å…¬å¼ & -
                    bestimmten Zustand zu sein
                    Berechnung Eigenvektor
                pagerank ç‰¹ç‚¹ & -
                    Von Anfrage unabhÃ¤ngig
                    Offline Berechnung
                    ï¼ˆå…¶å®æ˜¯æ¯å‘¨è®¡ç®—ä¸€æ¬¡ï¼‰
            HITS
                ç†è§£ & -
                    ä¹Ÿæ˜¯ä¸€ç§é€æ­¥è¿›å…¥ç¨³æ€çš„ç®—æ³•ï¼Œä½†æ˜¯åªæ˜¯åœ¨åŸºæœ¬é›†åˆä¸­è¿›è¡Œ
                    å±€éƒ¨ç®—æ³•ï¼Œé’ˆå¯¹IBMè¿™æ ·å…¨æ˜¯å›¾ç‰‡çš„ç½‘é¡µ
                idea & -
                    Authorities -Haben gute Inhalte
                    Hubs -Verweisen auf gute Inhalte
                Formalisierung & -
                    Darstellung Ã¼ber Adjazenzmatrix
                    Multiplikation Matrix mit Vektorä¸€ç§æ•°å­¦æ“ä½œ **
                    Iteratives Verfahren
                    Verfahren muss nicht konvergierenä¸æ˜¯ä¸€å®šæ”¶æ•›
                    Potenzmethode (vollstÃ¤ndig) &&&&è¿™ç§æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯é—®é¢˜è½¬æ¢çš„ç¬¬äºŒä¸ªå…³é”®æ­¥éª¤ï¼šæ±‚ä¸»ç‰¹å¾å‘é‡
                    è®¡ç®— &
                å…·ä½“åº”ç”¨è¿‡ç¨‹ & -
                    1.ç»™å®šæŸä¸ªæŸ¥è¯¢ï¼ˆæ¯”å¦‚ leukemiaï¼‰ï¼Œ
                        åˆ©ç”¨æŸä¸ªæ–‡æœ¬ç´¢å¼•è·å¾—åŒ…å« leukemia çš„æ‰€æœ‰ç½‘é¡µã€‚è¿™äº›ç½‘é¡µè¢«ç§°ä¸ºæ ¹é›†ï¼ˆ root setï¼‰
                    2.æˆ‘ä»¬åˆ©ç”¨ä¸Šè¿°è¿‡ç¨‹äº§ç”Ÿçš„åŸºæœ¬é›†è¿›è¡Œ hub å€¼å’Œ authority å€¼çš„è®¡ç®—
                        ä¹‹æ‰€ä»¥å¦‚æ­¤æ„é€ åŸºæœ¬é›†çš„åŸå› åœ¨äº

                    HITS nicht auf komplettem Web berechnen, sondern auf Suchergebnissen
                    ï¼šåªæ˜¯åœ¨æœç´¢ç»“æœä¸­è¿›è¡Œæœç´¢ï¼Œä¸æ˜¯å…¨ç½‘
                    Suchterme bestimmen eine Ergebnismenge von Dokumenten ïƒ  root set (=Startmenge)
                    ï¼šæ€æ ·è¿ç”¨äºç½‘ç»œæœç´¢æ’å
                    Ãœberwindung von Sprachen
                hits ç‰¹ç‚¹ & -
                    -r 
                        goes only in the result of queries not the whole web 
                        different languages can be involved 

                    åªæ˜¯åœ¨æœç´¢ç»“æœä¸­è¿›è¡Œæœç´¢ï¼Œä¸æ˜¯å…¨ç½‘
                    Ãœberwindung von Sprachen
                é—®é¢˜ï¼š  & -
                    -r 
                        not relevant website can be involved 

                    å¯èƒ½å¼•èµ·ä¸»é¢˜ç¥¨å˜ï¼Œå› ä¸ºAufnahme nicht relevanter Seiten 
        -----æ–‡æœ¬åˆ†ç±» --ä¼¼ä¹ä¸ä»…ä»…æ˜¯æä¾›æœç´¢æœåŠ¡ï¼Œæ›´æœ‰äººå·¥æ™ºèƒ½åœ¨é‡Œé¢ï¼ˆä¸æ—¶ä¿±è¿›æ¨¡å—ï¼‰
            +
                ä¾‹å¦‚æ–‡ç« è‡ªåŠ¨åˆ†ç±»ã€é‚®ä»¶è‡ªåŠ¨åˆ†ç±»ã€åƒåœ¾é‚®ä»¶è¯†åˆ«ã€ç”¨æˆ·æƒ…æ„Ÿåˆ†ç±»ç­‰ç­‰
                æ ¹æ®ä¸€äº›å…¬å¸æè¿°é¢„æµ‹å…¬å¸æ€§è´¨ï¼Œç»™å…¶åˆ†ç±»
                æ•°æ®é›†æ˜¯11ç±»å…¬å¸çš„æè¿°æ•°æ®ï¼Œæˆ‘ä»¬è¦æ ¹æ®4774æ¡è®­ç»ƒæ•°æ®å»é¢„æµ‹2381æ¡æ•°æ®çš„ç±»åˆ«æ ‡ç­¾
                ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
                ä½¿ç”¨CHIé€‰æ‹©ç‰¹å¾ï¼ŒTFIDFè®¡ç®—ç‰¹å¾æƒé‡ï¼Œæœ´ç´ è´å¶æ–¯åˆ†ç±»çš„æ•´ä½“æµç¨‹
                    https://blog.csdn.net/liuchonge/article/details/52204218
                å¤§è‡´æ­¥éª¤ï¼š
                    ä½¿ç”¨ç»“å·´ä¸­æ–‡åˆ†è¯å·¥å…·å¯¹æ–‡æœ¬è¿›è¡Œå¤„ç†ï¼Œ    
                        å¹¶å»åœç”¨è¯å¾—åˆ°æ‰€æœ‰æ–‡æœ¬ä¸­å‡ºç°çš„è¯è¯­ã€‚
                    ä½¿ç”¨CHIä½œä¸ºç‰¹å¾é€‰æ‹©çš„ä¾æ®ç»™æ¯ä¸€ç±»æ–°é—»é€‰å‡º150ç»´çš„ç‰¹å¾
                        ï¼Œå¹¶å»é‡ã€‚è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥è·å¾—å¤§æ¦‚1000ç»´çš„ç‰¹å¾ã€‚
                    ç‰¹å¾-->æ¯ä¸ªæ–°é—»æ„é€ VSMæ¨¡å‹ï¼ˆä¸€ä¸ªç‰¹å¾çŸ©é˜µï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯ç‰¹å¾å‘é‡ ***ï¼‰
                        ç‰¹å¾æƒé‡çš„å¤šé‡ç®—æ³•ï¼šçœ‹å›¾ï¼Œä¸‰ç§
                            1.ç”¨æ˜¯å¦å‡ºç°è¡¨è¾¾
                                æœ€ç®€å•
                            2.ç”¨tf
                            3.ä½¿ç”¨TFIDFæ–¹æ³•è®¡ç®—å„ç‰¹å¾çš„æƒé‡å¾—åˆ°è¡¨ç¤ºè¯¥æ–‡æœ¬çš„ç‰¹å¾å‘é‡
                    ç°åœ¨å°±å¯ä»¥æ–¹ä¾¿ä½¿ç”¨KNN/SVMç­‰æ–¹æ³•åˆ†ç±»çš„æ•°æ®
                æ–‡æœ¬çš„ç›¸ä¼¼åº¦è®¡ç®—ä¼šæœ‰è¿›ä¸€æ­¥çš„åº”ç”¨ï¼Œæ¯”å¦‚æ–‡æœ¬çš„åˆ†ç±»ã€èšç±»ç­‰ ***
        ppt-15 ä¸€äº›åŸºç¡€  --over 
            Klassifikation als Aufgabe im IR
                Herleiten von ğ›¾æ¥æº & 
                    ï‚§ Manuell
                    ï‚§ Erlernen aus Beispielen
                Beispiele Textklassifikation
                    åœ¨åƒåœ¾é‚®ä»¶å¤„ç†ä¸­ï¼Œæœ‰å“ªäº›åŸºæœ¬çš„å¤„ç†æ–¹æ³• & p7 
                        maybe we will let you deside if it is spam 
                        check the keyword or the metadata 
                    Ressortzuteilung von Nachrichten
                    Erkennung von Sprache / Zeichensatz
                    Sentimentanalyse
                    Who wrote which Federalist papers?
                    Male or female author?  
                        50 features to distinguish male-authored texts
                        from female-authored texts
                    Positive or negative movie review?
                    What is the subject of this article??
                the formal expression & 
                    Einfach Zuweisung
                    Mehrfache Zuweisung
                        Mehrfachklassifikation bzw. Ãœberschneidende Kategorien
                questions about Maschinelles Lernen &
                    1.how function this method ?use pic to explain
                    2.about overfitting problem (speak more than one min)
                Ergebnisse Eva.
                    richtig oder nicht ? & 
                        explain it about 0.5 min .
                        Korrekte Klassifikationen
                            ï‚§ d aus c1 als c1 klassifiziert
                            ï‚§ d aus c2 als c2 klassifiziert
                        Falsche Klassifikation
                            ï‚§ d aus c1 als c2 klassifiziert
                            ï‚§ d aus c2 als c1 klassifiziert
                    1.using Confusion Matrix & 
                        rem. pic.
                        Verwendung von EvaluationsmaÃŸen wie Recall, Precision und F1 mÃ¶glich
                    2.Confusion Matrix erweitern p27 pic & 
                        Recall und Precision nicht als GesamtmaÃŸ anwendbar
                        calcu . with 
                            Accuracy 
                                korrekten Ergebnisse Ã¼ber allen vorgenommenen Klassifikationen 
                                sum of diag numbers / sum of all numbers  **
                                calcu Beispiel p28 &
                            Error Rate 
                                calcu 
                            Kritik an Accuracy &
                                if one class has more articals ,  
                                and thus the total acc will be strongly infact by it 
                    3.cross validation 
                        exp & 
                            1.randomly part the dataset to k categraies 
                            2.choose one cat. for evaluation and the others for traning .
                            3.repeat it for n times with diff. choise .
                                n-times k-fold cross-validation ïƒ  
                                    typischer Wert: 10-times 10-fold cross-validation
            æ•°æ®è°ƒæ•´ï¼Œç‰¹å¾æŠ½å–ï¼ˆåˆ†ç±»çš„ç¬¬ä¸€æ­¥ï¼‰    
                Klassenungleichgewicht
                    three methods & 
                        Undersampling
                            ï‚§ Ausgleich unterschiedlicher HÃ¤ufigkeiten 
                                zwischen den Klassen durch Eliminierung von
                                Beobachtungen der grÃ¶ÃŸeren Klasse L
                        Oversampling
                            ï‚§ VervielfÃ¤ltigung von Beobachtungen der kleineren Klasse S
                        Hybrid-Verfahren
                            ï‚§ Mischform zwischen Under- und Oversampling

                    Undersampling
                        https://zhuanlan.zhihu.com/p/34782497
                        1.use Undersampling-Rate,éšæœºæ¬ é‡‡æ ·
                            Imbalance-Rate & rario between big one and small one 
                            å‡å°‘å¤šæ•°ç±»æ ·æœ¬æ•°é‡æœ€ç®€å•çš„æ–¹æ³•ä¾¿æ˜¯éšæœºå‰”é™¤å¤šæ•°ç±»æ ·æœ¬
                        2.NearMissæ–¹æ³•
                            æ˜¯åˆ©ç”¨è·ç¦»è¿œè¿‘å‰”é™¤å¤šæ•°ç±»æ ·æœ¬çš„ä¸€ç±»æ–¹æ³•ï¼Œå®é™…æ“ä½œä¸­ä¹Ÿæ˜¯å€ŸåŠ©kNN
                            å…·ä½“ &
                                NearMiss-1ï¼šåœ¨å¤šæ•°ç±»æ ·æœ¬ä¸­é€‰æ‹©ä¸æœ€è¿‘çš„3ä¸ªå°‘æ•°ç±»æ ·æœ¬çš„å¹³å‡è·ç¦»æœ€å°çš„æ ·æœ¬ã€‚
                                NearMiss-2ï¼šåœ¨å¤šæ•°ç±»æ ·æœ¬ä¸­é€‰æ‹©ä¸æœ€è¿œçš„3ä¸ªå°‘æ•°ç±»æ ·æœ¬çš„å¹³å‡è·ç¦»æœ€å°çš„æ ·æœ¬ã€‚
                                NearMiss-3ï¼šå¯¹äºæ¯ä¸ªå°‘æ•°ç±»æ ·æœ¬ï¼Œé€‰æ‹©ç¦»å®ƒæœ€è¿‘çš„ç»™å®šæ•°é‡çš„å¤šæ•°ç±»æ ·æœ¬ã€‚
                            æ¯”è¾ƒ &
                                NearMiss-1å’ŒNearMiss-2æ–¹æ³•çš„æè¿°ä»…æœ‰ä¸€å­—ä¹‹å·®ï¼Œä½†å…¶å«ä¹‰æ˜¯å®Œå…¨ä¸åŒçš„ï¼š
                                    NearMiss-1è€ƒè™‘çš„æ˜¯ä¸æœ€è¿‘çš„3ä¸ªå°‘æ•°ç±»æ ·æœ¬çš„å¹³å‡è·ç¦»ï¼Œæ˜¯å±€éƒ¨çš„ï¼›
                                    NearMiss-2è€ƒè™‘çš„æ˜¯ä¸æœ€è¿œçš„3ä¸ªå°‘æ•°ç±»æ ·æœ¬çš„å¹³å‡è·ç¦»ï¼Œæ˜¯å…¨å±€çš„ã€‚
                                è®ºæ–‡ä¸­æœ‰å¯¹è¿™å‡ ç§æ–¹æ³•çš„æ¯”è¾ƒï¼Œå¾—åˆ°çš„ç»“è®ºæ˜¯NearMiss-2çš„æ•ˆæœæœ€å¥½
                                    ï¼Œä¸è¿‡è¿™ä¹Ÿæ˜¯éœ€è¦ç»¼åˆè€ƒè™‘æ•°æ®é›†å’Œé‡‡æ ·æ¯”ä¾‹çš„ä¸åŒé€ æˆçš„å½±å“
                        3.Condensed Nearest Neighbors (CNN)
                            ä¸€ç§åŸºäºèšç±»çš„æ–¹æ³• *
                            NEXT
                            Durch Zufallskomponente kann die erzeugte Untermenge stark variieren
                        4.Underbagging
                            æ¬ é‡‡æ ·è¢«æ•´åˆåˆ°AdaBoost.M1ä¸­ã€‚RUSBoostè¢«è¯æ˜æ˜¯æ¯”AdaC2ï¼Œ
                                SMOTEBoostï¼ŒMSMOTEBoostï¼ŒUnderBaggingï¼Œ
                                EasyEnsembleï¼ŒBalanceCascadeæ›´å¥½ï¼Œæ›´å¿«ï¼Œ
                                æ›´ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› æ­¤æˆä¸ºä»åæ–œè®­ç»ƒæ•°æ®ä¸­å­¦ä¹ çš„å¯è¡Œé€‰æ‹©ã€‚ 
                            NEXT
                            Mit steigender Anzahl der Modelle umso wahrscheinlicher, dass jede Beobachtung in zumindest
                            eines der Modelle einflieÃŸt und somit nicht komplett verworfen wird
                    Oversampling
                        VergrÃ¶ÃŸerter Trainingsdatensatz
                        1.use ratio to enlarge 
                        2.SMOTE-Verfahren (Synthetic Minority Oversampling Technique)
                            NEXT
                                æ™®é€šçš„è¿‡é‡‡æ ·ä¼šä½¿å¾—è®­ç»ƒé›†ä¸­æœ‰å¾ˆå¤šé‡å¤çš„æ ·æœ¬ã€‚ 
                                äº§ç”Ÿoverfittingé—®é¢˜
                                SMOTEçš„å…¨ç§°æ˜¯Synthetic Minority Over-Sampling Techniqueï¼Œè¯‘ä¸ºâ€œäººå·¥å°‘æ•°ç±»è¿‡é‡‡æ ·æ³•â€ã€‚
                                é€šè¿‡feature space
                                ä½¿ç”¨çº¿æ€§å˜æ¢ï¼Ÿ
                            DistanzmaÃŸ: euklidische Distanz bzw. Gower-Distanz fÃ¼r kategorische Attribute
                        ++can be more 
                Feature-Scaling
                    +
                        why two values should be likely :åŸºæœ¬æ€æƒ³ & -
                            when x1 :[0-2000]
                                x2: [1-5]
                            feature map ä¼šå˜æˆæ¤­åœ†ï¼Œå¯¹åç»­å¤„ç†ä¸å¥½ï¼šï¼šæ±‚æ•°å€¼çš„æ—¶å€™ä¼šæ¥å›è·³åŠ¨ 
                        ä¸€èˆ¬æˆ‘ä»¬é€‰æ‹©å°†featureçš„valueè½åœ¨ [âˆ’1,1], 
                            ä½†æ˜¯æˆ‘ä»¬ä¸éœ€è¦ä¸¥è‹›çš„è¦æ±‚ä¸€å®šæ»¡è¶³[âˆ’1,1]ï¼Œå…¶å®å·¦è¾¹ç•Œåœ¨[âˆ’3,âˆ’13]ï¼Œå³è¾¹ç•Œåœ¨[13,3] å°±å¯ä»¥äº†
                    methods & 
                        1.Standard Scaler
                            æ ‡å‡†åŒ–
                            å¤„ç†ä»¥åæ˜¯N(0,1)
                        2.MinMax Scaler
                            Skaliert auf den Bereich 0 bis 1 bzw. -1 bis 1 falls negative Werte vorhanden sind
                            Sensitiv gegenÃ¼ber AusreiÃŸern-å¯¹å¼‚å¸¸æ•°æ®æ•æ„Ÿï¼ˆæ¯”å¦‚æœ‰ä¸€ä¸ªå¾ˆå¤§ï¼‰
                        3.Robust Scaler
                            wird der Interquartilsabstand å››åˆ†ä½æ•°(engl. interquartile range) verwendet
                            Dadurch robuster gegenÃ¼bern AusreiÃŸern
                            calcu ?? NEXT
                Feature-Auswahl
                    -r
                        based on frequncy
                        MI formular & calcu. & p65
                        with the help of Chi-square test calcu. & p69
                    ä¸»è¦ç›®çš„ &
                        Text collections have a large number of features
                        it can Eliminates noise features and avoid overfitting
                        ç‰¹å¾ä¸ªæ•°è¶Šå¤šï¼Œå®¹æ˜“å¼•èµ·â€œç»´åº¦ç¾éš¾â€ï¼Œæ¨¡å‹ä¹Ÿä¼šè¶Šå¤æ‚ï¼Œå…¶æ¨å¹¿èƒ½åŠ›ä¼šä¸‹é™ã€‚
                        ç‰¹å¾é€‰æ‹©èƒ½å‰”é™¤ä¸ç›¸å…³ï¼ˆirrelevantï¼‰æˆ–äº¢ä½™ï¼ˆredundantï¼‰çš„ç‰¹å¾ï¼Œ
                            ä»è€Œè¾¾åˆ°å‡å°‘ç‰¹å¾ä¸ªæ•°ï¼Œæé«˜æ¨¡å‹ç²¾ç¡®åº¦ï¼Œå‡å°‘è¿è¡Œæ—¶é—´çš„ç›®çš„
                    Utility measures A(t,c), three different approaches &
                        -r
                            not all features are suitable for classification purpose
                            only this subset as features in text classification

                        ï‚§ Frequency: ğ´ ğ‘¡, ğ‘ = ğ‘(ğ‘¡, ğ‘)
                            spec. in IR : Selecting terms that are most common in the class(document )
                            but: May have no specific information (such as, Monday, Tuesday â€¦)
                        ï‚§ Mutual information: ğ´ ğ‘¡, ğ‘ = ğ¼(ğ‘ˆğ‘¡; ğ¶ğ‘)
                            è¿™ç§ç”±äºè®­ç»ƒé›†çš„å¶ç„¶æ€§å¯¼å‡ºçš„ä¸æ­£ç¡®çš„æ³›åŒ–ç»“æœç§°ä¸ºè¿‡å­¦ä¹ ï¼ˆoverfittingï¼‰ã€‚è§£å†³è¿™ä¸ª
                            è¯é¡¹çš„å­˜åœ¨ä¸å¦ç»™ç±»åˆ«cçš„æ­£ç¡®åˆ¤æ–­æ‰€å¸¦æ¥çš„ä¿¡æ¯é‡
                            é€‰æ‹©æœ€å¤§çš„kä¸ªé¡¹ç›®
                            é€‰å‡ºçš„è¯æ±‡å¯¹ç±»åˆ«çš„åˆ¤å®šèµ·åˆ°è‰¯å¥½ä½œç”¨
                            formular &
                            calcu. & p65
                        ï‚§ The ğœ’2 test: ğ´ ğ‘¡, ğ‘ = ğœ’2(ğ‘¡, ğ‘)
                            åœ¨ç»Ÿè®¡å­¦ä¸­ï¼ŒÏ‡ 2ç»Ÿè®¡é‡å¸¸å¸¸ç”¨äºæ£€æµ‹ä¸¤ä¸ªäº‹ä»¶çš„ç‹¬ç«‹æ€§
                            å¦‚æœä»–ä»¬æ˜¯ç›¸å…³çš„åˆ™è¯´æ˜ä¿¡æ¯æ˜¯æœ‰ç”¨çš„
                            formular &
                            calcu. & p69
                        +åéªŒæ¦‚ç‡
                                https://blog.csdn.net/u011508640/article/details/72815981
                                å°¤å…¶æ˜¯å½“åéªŒåˆ†å¸ƒæ²¡æœ‰ä¸€ä¸ªç®€å•çš„è§£æå½¢å¼çš„æ—¶å€™æ›´æ˜¯è¿™æ ·ï¼š
                                    åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒåéªŒåˆ†å¸ƒå¯ä»¥ä½¿ç”¨ Markov chain Monte Carlo æŠ€æœ¯æ¥æ¨¡æ‹Ÿï¼Œ
                                    ä½†æ˜¯æ‰¾åˆ°å®ƒçš„æ¨¡çš„ä¼˜åŒ–æ˜¯å¾ˆå›°éš¾æˆ–è€…æ˜¯ä¸å¯èƒ½çš„
                                æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMaximum likelihood estimation, ç®€ç§°MLEï¼‰å’Œ
                                    æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡ï¼ˆMaximum a posteriori estimation, ç®€ç§°MAPï¼‰
                                    æ˜¯å¾ˆå¸¸ç”¨çš„ä¸¤ç§å‚æ•°ä¼°è®¡æ–¹æ³•
                                ä½ æœ‰å¤šå¤§æŠŠæ¡èƒ½ç›¸ä¿¡ä¸€ä»¶è¯æ®ï¼Ÿï¼ˆhow much you can trust the evidenceï¼‰
                                è´å¶æ–¯å…¬å¼ï¼šåšåˆ¤æ–­çš„æ—¶å€™ï¼Œè¦è€ƒè™‘æ‰€æœ‰çš„å› ç´ ã€‚ 
                                    è€æ¿éª‚ä½ ï¼Œä¸ä¸€å®šæ˜¯ä½ æŠŠä»€ä¹ˆå·¥ä½œæç ¸äº†ï¼Œå¯èƒ½åªæ˜¯ä»–ä»Šå¤©å‡ºé—¨å‰å’Œå¤ªå¤ªåµäº†ä¸€æ¶ã€‚
                                    ä¸€ä¸ªæœ¬æ¥å°±éš¾ä»¥å‘ç”Ÿçš„äº‹æƒ…ï¼Œå°±ç®—å‡ºç°æŸä¸ªè¯æ®å’Œä»–å¼ºçƒˆç›¸å…³ï¼Œ
                                        ä¹Ÿè¦è°¨æ…ã€‚è¯æ®å¾ˆå¯èƒ½æ¥è‡ªåˆ«çš„è™½ç„¶ä¸æ˜¯å¾ˆç›¸å…³ï¼Œä½†å‘ç”Ÿæ¦‚ç‡è¾ƒé«˜çš„äº‹æƒ…
                                +MAP 
                                    æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ˜¯æ±‚å‚æ•°Î¸, ä½¿ä¼¼ç„¶å‡½æ•°P(x0|Î¸)æœ€å¤§ã€‚æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡åˆ™æ˜¯æƒ³æ±‚Î¸ä½¿P(x0|Î¸)P(Î¸)æœ€å¤§ã€‚
                                        æ±‚å¾—çš„Î¸ä¸å•å•è®©ä¼¼ç„¶å‡½æ•°å¤§ï¼ŒÎ¸è‡ªå·±å‡ºç°çš„å…ˆéªŒæ¦‚ç‡ä¹Ÿå¾—å¤§
                                    MAPå…¶å®æ˜¯åœ¨æœ€å¤§åŒ–P(Î¸|x0)
                                    å¯ä»¥å‡ç¼“å®éªŒçš„ä¸ç¨³å®šæ€§
                                    æ›´å¥½çš„ä¼°è®¡ï¼Ÿ 
                                MLEæ˜¯æŠŠå…ˆéªŒæ¦‚ç‡P(Î¸)è®¤ä¸ºç­‰äº1ï¼Œå³è®¤ä¸ºÎ¸æ˜¯å‡åŒ€åˆ†å¸ƒã€‚
                                é€šè¿‡äº’ä¿¡æ¯å»å™ªåï¼Œç”±äºåªä¿ç•™äº†æœ‰æ•ˆçš„ç‰¹å¾ï¼ŒåéªŒæ¦‚ç‡åŠ å¤§ï¼Ÿï¼Ÿï¼Ÿ
                        +
                                https://blog.csdn.net/BigData_Mining/article/details/81279612
                                ä»¥ä¸Šä¸‰ç§ä¸åŒçš„è§’åº¦è¯´æ˜: ä»ä¸€ä¸ªäº‹ä»¶è·å¾—å¦ä¸€
                                    ä¸ªäº‹ä»¶çš„å¹³å‡äº’ä¿¡æ¯éœ€è¦æ¶ˆé™¤ä¸ç¡®å®šåº¦,ä¸€æ—¦æ¶ˆé™¤äº†ä¸ç¡®å®šåº¦,å°±è·å¾—äº†ä¿¡æ¯ã€‚
        ppt-16--Rule-based Text Classification --over 
            +
                é€šè¿‡ä¸€ç³»åˆ—è§„åˆ™â€œå¦‚æœã€‚ã€‚ã€‚å°±ã€‚ã€‚ã€‚â€ï¼Œæ¥è¿›è¡Œåˆ†ç±»
                å†³ç­–æ ‘å¯ä»¥è½¬æ¢æˆåŸºäºè§„åˆ™çš„åˆ†ç±»å™¨
                è§„åˆ™æŒ‰ç…§ä¼˜å…ˆçº§ä¸€æ¬¡æ’åˆ—---->å†³ç­–åˆ—è¡¨
                è§„åˆ™æ’åºæ¨¡å¼
                    ï¼ˆ1ï¼‰åŸºäºè§„åˆ™æ’åºï¼šè§„åˆ™çš„åˆ†ç±»èƒ½åŠ›
                    ï¼ˆ2ï¼‰åŸºäºç±»çš„æ’åºï¼šç›¸åŒç±»åˆ«çš„è§„åˆ™æ’åœ¨ä¸€å—
                rule evaluation è§„åˆ™è¯„ä¼°
                    1.å¦‚æœä¿¡æ¯å¢ç›Šä¸ç†æƒ³ï¼Œä¸¢å¼ƒè¯¥è§„åˆ™
                    2.å’Œå†³ç­–æ ‘çš„post-pruningåä¿®å‰ªç±»ä¼¼
                        reduced error pruningï¼š
                        ç¡®å®šä¸€æ¡è§„åˆ™
                        åœ¨ä¿®å‰ªä¹‹å‰å’Œä¿®å‰ªåï¼Œåˆ†åˆ«è®¡ç®—æ¯”è¾ƒåœ¨éªŒè¯é›†ä¸Šçš„é”™è¯¯ç‡
                        å¦‚æœé”™è¯¯ç‡å˜é«˜ï¼Œä¸¢å¼ƒè¯¥è§„åˆ™
                é¡ºåºè¦†ç›–
                    æ­¥éª¤ï¼š
                    ï¼ˆ1ï¼‰start from an empty rule
                ç”±å†³ç­–æ ‘ç”Ÿæˆï¼šC4.5rules
            
            ---
            Regelbasierte Systeme
                Form: &
                    Regeln haben Wenn-Dann Form 
                Formale Darstellung &
                Bezug zu Boolschem IR
                    Ati æè¿°ï¼Ÿï¼Ÿ
            Manuell erstellte Regeln
                exp & p10
                    ::
                        simple quick and efficient 
                        can be used to filter emails ..
            Evaluation von Regeln
                Grundlegende MaÃŸe
                    ä¸¤ä¸ªæŒ‡æ ‡ å…¬å¼ &
                    accuracy
                    Coverage
                        å¤ªå°ï¼šï¼šoverfitting,Regel zu speziell,rule too special 
                        match too many test data , also not good 

                    è®¡ç®— & p15
                    wir mochten hohe Accuracy und niedriger Coverage-Wert
                Drei QualitÃ¤tsmaÃŸe -- ç›®çš„åªæœ‰ä¸€ä¸ªï¼Œæ€»ç»“å‰é¢ä¸¤ä¸ªæˆä¸ºä¸€ä¸ªæŒ‡æ ‡
                    formular &
                    p21 ?? NEXT
                    ï‚§ Laplace-MaÃŸ 
                        åˆ†å­åˆ†æ¯çš„ä¸€ç§å¸¦æœ‰smoothingçš„ç»“åˆæ–¹å¼
                    ï‚§ Likelihood Quotienten Test
                        ä½¿ç”¨æ¦‚ç‡æŠ€å·§ï¼Œåˆ†æˆä¸¤ç§ç±»åˆ«
                    ï‚§ FOIL's Information Gain
                        è®¡ç®—æ”¹è¿›ï¼Œç›¸å¯¹äºåŸæœ‰çš„rule
                    eg. & p26
            Sequential Covering
                NEXTNEXTNEXT
                    åŸºäºFOIL_Gain
                    This sequential covering algorithm is one of the most widespread approaches to learning disjunctive sets of rules
                    åœ¨è®­ç»ƒé›†ä¸Šæ¯å­¦åˆ°ä¸€æ¡è§„åˆ™ï¼Œå°±å°†è¯¥è§„åˆ™è¦†ç›–çš„è®­ç»ƒæ ·ä¾‹å»é™¤ï¼Œç„¶åä»¥å‰©ä¸‹çš„è®­ç»ƒæ ·ä¾‹ç»„æˆè®­ç»ƒé›†é‡å¤ä¸Šè¿°è¿‡ç¨‹
                    http://www.cse.unsw.edu.au/~cs9417ml/Rule/sequential.html
                    ä»Dä¸­åˆ é™¤coveræ‰çš„termï¼šï¼šç¬¬ä¸€æ¬¡çš„æ—¶å€™åˆ é™¤äº†ğ‘ˆğ‘šğ‘ ğ‘ğ‘¡ğ‘§ 
                idea &
                    use graddy search , depth first search 
                Abbruchkriterium fÃ¼r learnRule
                    1.Laplace-Mass begain to decrease 
                    2. keine Accuracy von 1 
                    3. all documents has been removed 
                formular &
                ï¼šï¼šeg p33 ***
                Nachbearbeitung
            Aber:
                Aufbau und Pflege regelbasierter Klassifikationssysteme ist aufwÃ¤ndig und teuer
        ppt-17--NaÃ¯ve Bayes Text Classification --over 
            ---
            Probabilistische Klassifikation
                +
                ï‚§ Idee
                ï‚§ Lernen / SchÃ¤tzen von Wahrscheinlichkeiten
                Zuordnung eines Dokuments zur wahrscheinlichsten Kategorie
                idea with formular & 
                why NaÃ¯ve &
                    it assumps that there is no relationship between terms 
                why not Naive &
                    very robost against concept drift 
                        (VerÃ¤nderung der Definition einer Klasse Ã¼ber die Zeit)
                    very quick runs 
            Multinomial NaÃ¯ve Bayes
                +corpus 
                    direct proportion to 
                    inverse proportion 
                    They are often in direct proportion to their wealth.
                        - the value of .. is in direct proportion to the number of ..
                    

                
                å¯¹äºå¤šä¸ªdocçš„formular & 
                Ã„hnlichkeit zu LM (Vorlesung 12) ,exp &
                Lernphase 
                    exp & 
                        ::
                            calcu. the probability of cj 
                            calcu. the p. of tk given cj 
                                if the term appears in the docu.    

                    Klassifikationsfunktion & formular 
                    eg & 
                
                -r 
                    similar to LM 
                    mianly beased on conditional probability 
                    ä¸»è¦ä½¿ç”¨åéªŒæ¦‚ç‡è®¡ç®—
                    ç”¨è¯æ±‡é¢‘ç‡è¿›è¡Œä¼°è®¡ï¼Œæ¡ä»¶æ¦‚ç‡

                +
                    NB-Klassifikatoren::steht fur NaÃ¯ve Bayes 
                    Average Accuracy zwischen 0,81 und 0,84æ¯”åŸºäºè§„åˆ™çš„å¥½å¾ˆå¤š
                    æœ´ç´ è´å¶æ–¯çš„ç‹¬ç«‹æ€§å‡è®¾å¾ˆå‚»å¾ˆå¤©çœŸï¼Œtoo simpleï¼Œsometimes naiveï¼Œæ‰€ä»¥é¢„æµ‹ç²¾åº¦å¾€å¾€ä¸æ˜¯å¾ˆé«˜
                    å¦‚æœæ²¡æœ‰ç‹¬ç«‹å‡è®¾ï¼Ÿä¹Ÿå¯ä»¥è®¡ç®—ï¼Ÿ
            Bernoulli NaÃ¯ve Bayes
                +

                    ä¸ç”¨é¢‘ç‡ï¼Œè€Œæ˜¯Termæ˜¯å¦å‡ºç°è¯„ä»·è®¡ç®—:NBâ€™s main strength is its efficiency
                    å› æ­¤è¿™é‡Œä½¿ç”¨çš„ä¸»è¦æ˜¯dfï¼Œæ–‡æ¡£é¢‘ç‡
                    æˆ‘è¿™æœ‰çš„ç”¨å…¬å¼è®¡ç®—ï¼Œå…¶ä»–ä¸å‡ºç°çš„åº”è¯¥æ˜¯ä¹˜ä»¥åå‘æ¦‚ç‡ ** è¿™å°±æ˜¯è¢«åŠªåŠ›ï¼ˆæƒ³æƒ³åˆ†å¸ƒä¹Ÿæ˜¯è¿™æ ·ï¼‰
                    ä¸æ˜¯ä¹˜ä»¥ç³»æ•°çš„å…³ç³»ï¼Œåº•å±‚æ¨¡å‹ä¸ä¸€æ ·
                    â€œTerm vorhandenâ€ oder â€œTerm nicht vorhandenâ€ als 
                        Ergebnis der Beobachtung eines Dokumentes
                        Bernoulli-Verteilung
                    Klassifikationsfunktionï¼Ÿ **
                    Anmerkungen
                    Aktuelle Anwendung
                        è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ 
                idea with formuar  &
                Klassifikationsfunktion &
                Beispiel &
                Anmerkungen &
                    -r 
                        NBâ€™s main strength is its efficiency
                            uses binary term occurrence features 
                            Multinomial Naive Bayes is in a sense more complex model 
                        suitable for short doc.
                        It has the benefit of explicitly modelling the absence of terms
                        
                    FÃ¼r kurze Dokumente geeignet
                    Sinnvoll bei negativem Zusammenhang zwischen Termen und Kategorien
                    Etwa bei positiven oder negativen Ergebnisse von medizinischen 
                        Tests um einen Krankheitsbefund zu erstellen
            Aktuelle Anwendung &
                CLIR-System
        ppt-18--Vector Space Text Classification   --over
            +
                mit CosinusmaÃŸ
                Rocchio Klassifikator
                    -r 
                        Assign category to the nearest centroid
                        nearst centroid and far from others 

                        

                    åŸºæœ¬æ€è·¯
                        1.ç®€å•å¾—åªè®¡ç®—ä¸­å¿ƒ
                        2.æˆ–è€…è€ƒè™‘æ­£åä¾‹å­ï¼šä¸­å¿ƒçš„ä½ç½®ä¸æ­£ä¾‹å­è¿‘è€Œä¸åä¾‹è¿œ
                    ç¼ºç‚¹ï¼šï¼š
                        å®ƒè®¤ä¸ºä¸€ä¸ªç±»åˆ«çš„æ–‡æ¡£ä»…ä»…èšé›†åœ¨ä¸€ä¸ªè´¨å¿ƒçš„å‘¨å›´ï¼Œå®é™…æƒ…å†µå¾€å¾€ä¸æ˜¯å¦‚æ­¤
                        ä¸å¯ä»¥å®¹é”™--ä¸‡ä¸€æœ‰ä¸€ä¸ªé”™è¯¯çš„å‘¢ï¼ï¼è‡´å‘½

                        -r
                            there will be sth. wrong , when the classes are not convex  
                                but in practice , classes are rarely distributed in this way 
                                like spheres with similar radii
                            it is not robost with noise 

                    å¸¸å¸¸è¢«ç”¨æ¥åšç§‘ç ”ä¸­æ¯”è¾ƒä¸åŒç®—æ³•ä¼˜åŠ£çš„åŸºçº¿ç³»ç»Ÿï¼ˆBase Lineï¼‰
                    ç”¨äºç›¸å…³åé¦ˆï¼š
                        å‡å®šæˆ‘ä»¬è¦æ‰¾ä¸€ä¸ªæœ€ä¼˜æŸ¥è¯¢å‘é‡q ï¼Œ
                        å®ƒä¸ç›¸å…³æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦æœ€å¤§ä¸”åŒæ—¶åˆå’Œä¸ç›¸å…³æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦æœ€å°
                    0,86 und 0,87
                    Segmentierung des Vektorraumes = Voronoi-Diagramm
                kNN 
                    å¦‚æœä¸€ä¸ªæ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„kä¸ªæœ€ç›¸ä¼¼(å³ç‰¹å¾ç©ºé—´ä¸­æœ€é‚»è¿‘)
                        çš„æ ·æœ¬ä¸­çš„å¤§å¤šæ•°å±äºæŸä¸€ä¸ªç±»åˆ«ï¼Œåˆ™è¯¥æ ·æœ¬ä¹Ÿå±äºè¿™ä¸ªç±»åˆ«ï¼Œ
                        å…¶ä¸­Ké€šå¸¸æ˜¯ä¸å¤§äº20çš„æ•´æ•°
                    KNNç®—æ³•ä¸­ï¼Œæ‰€é€‰æ‹©çš„é‚»å±…éƒ½æ˜¯å·²ç»æ­£ç¡®åˆ†ç±»çš„å¯¹è±¡ã€‚
                        è¯¥æ–¹æ³•åœ¨å®šç±»å†³ç­–ä¸Šåªä¾æ®æœ€é‚»è¿‘çš„ä¸€ä¸ª
                        æˆ–è€…å‡ ä¸ªæ ·æœ¬çš„ç±»åˆ«æ¥å†³å®šå¾…åˆ†æ ·æœ¬æ‰€å±çš„ç±»åˆ«ã€‚
                    ä¾èµ–äºk
                    KNNé€šè¿‡ä¾æ®kä¸ªå¯¹è±¡ä¸­å ä¼˜çš„ç±»åˆ«è¿›è¡Œå†³ç­–ï¼Œ
                        è€Œä¸æ˜¯å•ä¸€çš„å¯¹è±¡ç±»åˆ«å†³ç­–ã€‚è¿™ä¸¤ç‚¹å°±æ˜¯KNNç®—æ³•çš„ä¼˜åŠ¿
                    Varianten
            ---
            Klassifikation im Vektorraum
                Vektorraum:vector space
                ç”»å›¾ & p6 
                exp &
                    Aside: 2D/3D graphs can be misleading 
                    ::
                        the vectors are high dimentional vectors 
                        compare them to measure similarity between them.
            Rocchio Klassifikator
                rocchio classification algorithm
                Rocchio relevance feedback is designed to
                    distinguish only two classes, relevant and nonrelevant.
                ç½—åŸºå¥¥ï¼ï¼
                exp idea &
                    Kategorie des nÃ¤chstgelegenen Zentroiden zuweisen
                    è§‚å¯Ÿå¾—åˆ°çš„ç‰¹ç‚¹ï¼šliegen untereinander rÃ¤umlich dicht
                        beisammen und gleichzeitig zu den anderen Kategorien weiter entfernt
                    Zentroiden liegen im Zentrum der Trainingsdokumente zu einer Kategorie
                    ::
                        it uses centroids to calcu. 
                            the center means :vector average
                            one centroid means one class 
                        boundaries?
                            points with equal distance from two centroids
                            are boundaries of classes .
                formular & p13 
                Beispiel & 
                Probleme bei Rocchio &
                    FlÃ¤che des Voronoi-Diagramms sind konvex
                    Probleme bei
                        ï‚§ Unterschiedlicher GrÃ¶ÃŸe
                        ï‚§ Unterschiedlicher Dichte
                        ï‚§ Verschlungene / â€verzahnteâ€œ Formen
                    Bereits Fehler auf Trainingsdaten
                    å®ƒè®¤ä¸ºä¸€ä¸ªç±»åˆ«çš„æ–‡æ¡£ä»…ä»…èšé›†åœ¨ä¸€ä¸ªè´¨å¿ƒçš„å‘¨å›´ï¼Œå®é™…æƒ…å†µå¾€å¾€ä¸æ˜¯å¦‚æ­¤
                Segmentierung des Vektorraumes = Voronoi-Diagramm
            KNN
                +å‚è€ƒæ‰‹å†™æ•°å­—è¯†åˆ«ç¨‹åº 
                æ˜¯ä¸Šé¢æ–¹æ³•çš„å˜ç§æ‰©å±• Ã—Ã—
                process & 
                    Zu Dokument k nÃ¤chstgelegenen Dokumente auswÃ¤hlen und deren Kategorien betrachten
                    am hÃ¤ufigsten vorkommt wird dem zu klassifizierenden Dokument zugewiesen

                    ::
                        the result class will be assigned to the class
                            most common among its k nearest neighbors
                        we find the k nearest neighbors, an then , find the most commen class among them 

                Beispiel &
                1NN: Voronoi-Diagramm mit mehreren Zellen
                Wahl von k? &
                    å°çš„æ—¶å€™æ˜¯ä¸€ä¸ªç±»åˆ«ï¼Œå¢åŠ ä»¥åæ˜¯å¦ä¸€ä¸ªï¼Œä¸ç¨³å®š
                    å…¶ä¸­Ké€šå¸¸æ˜¯ä¸å¤§äº20çš„æ•´æ•°
                    ç”»å›¾
                    åœ¨åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬å–ä¸€ä¸ªè¾ƒå°çš„kå€¼ï¼Œé€šå¸¸é‡‡ç”¨äº¤å‰éªŒè¯æ³•æ¥é€‰å–æœ€ä¼˜çš„kå€¼ã€‚

                    ::
                        the result may not stable with k 
                            when k is small , the reuslt is one , when it larger , be an other .
                        Normally , k is no larger than 20 
                        use cross validation to choose the value of k 

                å˜ç§ & 
                    nennen Sie zwei
                    -r 
                        we can set weight to the neighbors 
                        use the raw vector , instead of normalize them.

                    1.Gewichtung der Nachbarn
                    2.Keine Operationen auf Vektoren
        ppt-19--SVMåˆ†ç±»æ•°æ®  --over 
        +å‘¨æœºå™¨å­¦ä¹ 
        +åŸºäºç¥ç»ç½‘ç»œçš„åˆ†ç±»
        +
            Skalarprodukts
            Mehrklassenprobleme
            Iteratives Verfahren
                Sonst bei jeder Iteration mindestens ein Fehler ïƒ  Gewichtsvektor und Bias mÃ¼ssten erneut
                    angepasst werden
                Lernrate: beeinflusst nur LÃ¤nge von w
                Duale Darstellung
                Kein SVM
                åªæ˜¯ä¸€ç§è¿­ä»£æ–¹æ³•äº§ç”Ÿè¶…å¹³é¢ï¼Œç”¨ä¸€ä¸ªæ¡ä»¶ï¼Œç”¨ä¸€å®šçš„å­¦ä¹ é€Ÿç‡è¿›è¡Œè¿™ä¸ªè¿‡ç¨‹
                Wiederholung der DurchlÃ¤ufe bis alle Beispiele richtig klassifiziert werden
                è€ƒè™‘å¯¹å¶é—®é¢˜
            SVM 
                die mÃ¶glichst viel Abstand zu den Datenpunkten aus den 
                    verschiedenen Kategorien lÃ¤sst
                StÃ¼tzvektoren
                Duale Formulierung
                +äºŒæ¬¡è§„åˆ’
                    ä¼˜åŒ–(æœ€å°åŒ–æˆ–æœ€å¤§åŒ–)å¤šä¸ªå˜é‡çš„äºŒæ¬¡å‡½æ•°ï¼Œå¹¶æœä»äºè¿™äº›å˜é‡çš„çº¿æ€§çº¦æŸ
                    ä¸€ç§æ•°å­¦é—®é¢˜
                    äºŒæ¬¡è§„åˆ’æ˜¯ä¸€ç§ç‰¹æ®Šçš„éçº¿æ€§è§„åˆ’
                    æ ¹æ®ä¼˜åŒ–ç†è®ºï¼Œä¸€ä¸ªç‚¹xæˆä¸ºå…¨å±€æœ€å°å€¼çš„å¿…è¦æ¡ä»¶æ˜¯æ»¡è¶³Karush-Kuhn-Tuckeræ¡ä»¶ï¼ˆKKTï¼‰ã€‚
                    å½“f(x)æ˜¯å‡¸å‡½æ•°æ—¶ï¼ŒKKTæ¡ä»¶ä¹Ÿæ˜¯å……åˆ†æ¡ä»¶
                    å½“äºŒæ¬¡è§„åˆ’é—®é¢˜åªæœ‰ç­‰å¼çº¦æŸæ—¶ï¼ŒäºŒæ¬¡è§„åˆ’å¯ä»¥ç”¨çº¿æ€§æ–¹ç¨‹æ±‚è§£ã€‚
                    å¦åˆ™çš„è¯ï¼Œå¸¸ç”¨çš„äºŒæ¬¡è§„åˆ’è§£æ³•æœ‰ï¼š
                        å†…ç‚¹æ³•(interior point)ã€
                        active setå’Œå…±è½­æ¢¯åº¦æ³•ç­‰ã€‚
                    å‡¸é›†äºŒæ¬¡è§„åˆ’é—®é¢˜æ˜¯å‡¸ä¼˜åŒ–é—®é¢˜çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚
                    å¯¹äºä¸€èˆ¬é—®é¢˜ï¼Œå¸¸ç”¨çš„æ–¹æ³•æœ‰å¾ˆå¤šï¼ŒåŒ…æ‹¬
                        interior point,å†…ç‚¹æ³•
                        active set å†…ç‚¹æ³•
                        augmented Lagrangian
                        conjugate gradient,
                        gradient projection,
                        extensions of the simplex algorithm
                    è®¡ç®—å¤æ‚æ€§
                        å½“Qæ­£å®šæ—¶ï¼Œç”¨æ¤­åœ†æ³•å¯åœ¨å¤šé¡¹å¼æ—¶é—´å†…è§£äºŒæ¬¡è§„åˆ’é—®é¢˜ã€‚
                        å½“Qéæ­£å®šæ—¶ï¼ŒäºŒæ¬¡è§„åˆ’é—®é¢˜æ˜¯NPå›°éš¾çš„ï¼ˆNP-Hardï¼‰ã€‚
                        å³ä½¿Qåªå­˜åœ¨ä¸€ä¸ªè´Ÿç‰¹å¾å€¼æ—¶ï¼ŒäºŒæ¬¡è§„åˆ’é—®é¢˜ä¹Ÿæ˜¯NPå›°éš¾çš„
                +å˜ç§
                    è½¯é—´éš”åˆ†ç±»å™¨
                    https://www.jianshu.com/p/8a499171baa9
                    é’ˆå¯¹å™ªå£°é—®é¢˜
                    ä½¿ç”¨svmåº“æ—¶éœ€è¦è°ƒæ•´ä¸å°‘å‚æ•°ï¼Œä»¥é«˜æ–¯æ ¸ä¸ºä¾‹ï¼Œæ­¤æ—¶è‡³å°‘éœ€è¦è€ƒè™‘çš„å‚æ•°æœ‰Cå’ŒÏƒ
                        ï¼Œå…¶ä¸­Cä¸ºè®­ç»ƒæ ·æœ¬è¯¯å·®é¡¹çš„æƒ©ç½šç³»æ•°ï¼Œè¯¥å€¼å¤ªå¤§æ—¶ï¼Œä¼šå¯¹è®­ç»ƒæ ·æœ¬æ‹Ÿåˆå¾ˆå¥½ï¼ˆbiaså°ï¼‰
                        ï¼Œä½†å¯¹æµ‹è¯•æ ·æœ¬æ•ˆæœå°±è¾ƒå·®(varianceå¤§)
                +è§£å†³æ›²çº¿åˆ†ç±»é—®é¢˜ï¼š
                    å¦‚ä½•ç†è§£ï¼Ÿ ï¼Ÿï¼Ÿ pptï¼Ÿï¼Ÿ
                    æ”¯æŒå‘é‡æœºé¦–å…ˆåœ¨ä½ç»´ç©ºé—´ä¸­å®Œæˆè®¡ç®—ï¼Œç„¶åé€šè¿‡æ ¸å‡½æ•°å°†è¾“å…¥ç©ºé—´æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ï¼Œ 
                        æœ€ç»ˆåœ¨é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­æ„é€ å‡ºæœ€ä¼˜åˆ†ç¦»è¶…å¹³é¢ï¼Œä»è€ŒæŠŠå¹³é¢ä¸Šæœ¬èº«ä¸å¥½åˆ†çš„éçº¿æ€§æ•°æ®åˆ†å¼€ &
                    Kernel Trick & è§£é‡Š æ ¸æŠ€å·§ï¼Ÿ
                        çº¦æŸé—®é¢˜ä¸­å†…ç§¯Ï•iâ‹…Ï•jçš„è¿ç®—ä¼šéå¸¸çš„å¤§ä»¥è‡³äºæ— æ³•æ‰¿å—ï¼Œå› æ­¤é€šå¸¸æˆ‘ä»¬ä¼šæ„é€ ä¸€ä¸ªæ ¸å‡½æ•°
                        ä¸€èˆ¬å¾ˆéš¾æ„é€ å‡ºå®Œå…¨ç¬¦åˆè¾“å…¥ç©ºé—´çš„æ ¸å‡½æ•°ï¼Œå› æ­¤æˆ‘ä»¬å¸¸ç”¨å¦‚ä¸‹å‡ ç§å¸¸ç”¨çš„æ ¸å‡½æ•°æ¥ä»£æ›¿è‡ªå·±æ„é€ æ ¸å‡½æ•°
                        æ ¸å‡½æ•°çš„é€‰æ‹©åœ¨SVMç®—æ³•ä¸­å°±æ˜¾å¾—è‡³å…³é‡
                    å‡ ç§å¸¸ç”¨çš„æ ¸å‡½æ•°
                        çº¿æ€§æ ¸å‡½æ•°
                            æˆ‘ä»¬é€šå¸¸é¦–å…ˆå°è¯•ç”¨çº¿æ€§æ ¸å‡½æ•°æ¥åšåˆ†ç±»ï¼Œçœ‹çœ‹æ•ˆæœå¦‚ä½•ï¼Œå¦‚æœä¸è¡Œå†æ¢åˆ«çš„ &
                        å¤šé¡¹å¼æ ¸å‡½æ•°
                            çš„å‚æ•°å¤šï¼Œå½“å¤šé¡¹å¼çš„é˜¶æ•°æ¯”è¾ƒé«˜çš„æ—¶å€™ï¼Œæ ¸çŸ©é˜µçš„å…ƒç´ å€¼å°†è¶‹äºæ— ç©·å¤§æˆ–è€…æ— ç©·å°ï¼Œè®¡ç®—å¤æ‚åº¦ä¼šå¤§åˆ°æ— æ³•è®¡ç®—
                            ç»´æ•°æ˜¯å‚æ•°parameter in sklearn learn
                        é«˜æ–¯
                            è€Œä¸”å…¶ç›¸å¯¹äºå¤šé¡¹å¼æ ¸å‡½æ•°å‚æ•°è¦å°‘ï¼Œå› æ­¤å¤§å¤šæ•°æƒ…å†µä¸‹åœ¨ä¸çŸ¥é“ç”¨ä»€ä¹ˆæ ¸å‡½æ•°çš„æ—¶å€™ï¼Œä¼˜å…ˆä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°
                            generate radial areas around training points
                        é‡‡ç”¨sigmoidæ ¸å‡½æ•°
                            æ”¯æŒå‘é‡æœºå®ç°çš„å°±æ˜¯ä¸€ç§å¤šå±‚ç¥ç»ç½‘ç»œ ï¼Ÿï¼Ÿ 
                    å‚æ•°
                        C:tradeoff between how smooth the decision boundary is and how well it classifies examples
                        Gamma: kernel coefficient for rbf, poly, and sigmoid
        ---
        Klassifikation Ã¼ber Hyperebene
            Darstellung Hyperebene mit Ebenengleichung 
                -r 
                    If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes
                    hyperplane

                formular &
                clacu p7 &
            Vorgehen bei mehr als zwei Kategorien ?Multiclass SVMs?
                exp & p10
                    ::
                        1.Build a classifier for each class
                        2.Given the test document, apply each classifier separately.
                        3.Assign the document to the class with the maximum score
                        example ??
            
        Trainingsphase:Perzeptron--Iteratives Verfahren von Hyperebene
            verfahren exp &
                eg. Wiederholung der DurchlÃ¤ufe bis alle Beispiele 
                    richtig klassifiziert werden
                Gewichtsvektor und Bias mÃ¼ssten erneut angepasst werden bei Fehlern.
                :: 
                    0.start from w=0 and b=0
                    1.use the training set to update the value of w,b
                    2.repeat until all docs. are classified 

                    +the perceptron is an algorithm for supervised learning 
                        of binary classifiers
                    https://en.wikipedia.org/wiki/Perceptron#Multiclass_perceptron
            algo p15 & 
            calcu. p16 & 
            Perzeptron erlernt nur eine Hyperebene
            learn rate:
                control the relationship between weight vector and bias 
        SVM
            mÃ¶glichst viel Abstand
            ï‚§ Linearer Fall
                Einfach Verfahren &
                    formular p30 & -
                    Duale Formulierung & -
                        mit dem Gradientenverfahren lÃ¶sbar
                        LÃ¶sung dieses Problems = Lagrange-Multiplikatoren Î± i
                        Trainingsdokument ist ein StÃ¼tzvektor ???? NEXT
                    Beispiel p33 &
                    SensitivitÃ¤t
                Erweitertes Optimierungsverfahren
                    EinfÃ¼hrung einer Schlupfvariableï¼šï¼šå¼•å…¥æ¾å¼›å˜é‡
                    slack variable
                    formular &
                    Einfluss von C exp & -
                        darstellung mit ein Bild 
                        Je grÃ¶ÃŸer C je stÃ¤rker werden falsch
                            klassifizierte Trainingspunkte bestraft
                        ::
                            The parameter C is a regularization term, 
                                which provides a way to control overfitting
                            in our course , it should be small .
                            small  c has a better effect , Robust against failure
                            
            ï‚§ Kernel Trick--Nichtlineare Entscheidungsgrenzen
                key idea &
                    åœ¨é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­æ„é€ å‡ºæœ€ä¼˜åˆ†ç¦»è¶…å¹³é¢ï¼Œä»è€ŒæŠŠå¹³é¢ä¸Šæœ¬èº«ä¸å¥½åˆ†çš„éçº¿æ€§æ•°æ®åˆ†å¼€
                    ::
                        saparate the calsses in high dimentional .
                        It will be easier to do classification .
                types of kernel  & 
                    mehr als 4 
                        RBF Kernel--GauÃŸâ€˜scher Kernel
                        Polynomial Kernel
                        Sigmoid Kernel
                        Cosinus Similarity Kernel
                        Chi-squared Kernel




