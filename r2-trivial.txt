

    ---
    some calculations
        calculations
            do intersection for two postings & -
            pseudocode for skiping list &
            Positional index usage & example p38
            â€œProximityâ€ intersection algo  &
            calcu Soundex & -  Example::Soundex of HERMAN H650 p41
            Successor Variety  &calcu - -e READABLE p28
            ç®€å•è®¡ç®— ubung & 
            ç¼–è¾‘è·ç¦»calcu & 
            Jaccard coefficient to Scoringï¼Œhow? & Query q [ides of March] doc1:"Caesar died in Marchâ€:ïƒ  JACCARD(q, d1) = 1/6
            tf idf calcu & - P28
            tf idf calcu & - ä»p41å¼€å§‹è®¡ç®—
                tf idf algo p46 & - NEXT
            Heapsâ€™ law mean? formular &
            Zipfâ€™s law mean? formular &
                Bytewise compression  & calcu - p34
            åŸºäºå—çš„æ’åºç´¢å¼•æ–¹æ³•Blocked Sort-Based Indexing (BSBI) idea & -  algo & - 
            å†…å­˜å¼å•éæ‰«æç´¢å¼•æ„å»ºæ–¹æ³•Single-Pass In-Memory Indexing (SPIMI) algo & - 
            ch08 p27è®¡ç®— & -
            ch08 æˆ‘çš„ç½‘ä¸Šç…§ç‰‡ä¾‹å­ &è®¡ç®— - NEXT
            Precision and Recall
                è®¡ç®—p10 & -
            Accuracy ï¼šï¼šTP TN å ç”¨çš„æ¯”ä¾‹ 
                å…¬å¼ & -
                calcu &
            p28è®¡ç®— & - ubungä¸­æœ‰å¯¹åº”çš„é¢˜ç›® &
            pptä¸Šè®¡ç®— & p37 - ç»™å®šä¸¤ä¸ªåˆ¤æ–­ï¼Œæ±‚Kappaç»Ÿè®¡
                ç½‘é¡µä¸­é‚£ä¸ªè®¡ç®— & - NEXT
            è®¡ç®— çŠ¶æ€æ£€ç´¢å€¼ Retrievalstatuswert & calcu 
            è®¡ç®—ä¾‹å­ & - p165 ä¾‹12-2
            è¾“å…¥ä¸ºTasse Kanneçš„é‚£ä¸ªè®¡ç®— & - NEXT
            ubungä¸­ &
            ch13 æ¬¡é«˜ä»·æ‹å– & and calcuã€‚ p47
            è®¡ç®—PageRankæƒé‡ & - p322 p23å…¬å¼ & -
            Multinomiales NaÃ¯ve Bayes ï¼šidea & formuar  & calcu &
            ch19 calcu. p16 & 

    ---          
    Ubungen (will be merged
        improtant concepts from Ubung 
            u1 
                intersection algorithm x AND y and x OR y algo &
                How should the Boolean query x AND NOT y be handled &
                    Why is the naive evaluation of this query normally very expensive
                what can we achieve p6 &
                    x AND NOT y    ,  x OR NOT y    x+y    , N 
                For a conjunctive query, is processing postings lists in order of size guaranteed to be optimal? &
            u2
                The tokenization of a document is a trivial task
                stemming can lowers precision
                stemming increase or changed recall
                    // how affects the stemmer precision and recall and vocabulary size  &
                Stemming decreases the size of the vocabulary.
                Stemming should be applied to the documents, but not to the queries  
                The postings list of a stop word is usually longer than the postings list of a non-stop word  

                // what other approaches to stemming x3 &
                What are the respective Levenshtein editing operations x3 types of operations
            u3
                The Jaccard coefficient works well for ranking documents  
                Rare terms are less informative than frequent terms. 
                    // which words are more informasive ? &
                The inverted document frequency (idf) has no effect on the ranking for one-term queries  ! &
                There is exactly one way to calculate the tf-idf weights 

                What minimal and maximal values can the following variables have ? -
                Consider the case of a query term that is not in the set of indexed terms,what do we do ?
                What is the ğ‘–ğ‘‘ğ‘“ of a term that occurs in every document?
            u4  
                Zipf's Law states that the ith most frequent term has a collection frequency proportional to 1Î¤ğ‘–
                Heaps' Law assumes that the vocabulary can grow infinitely !!!
                Elias Gamma Coding is a technique for dictionary compression
                Front Coding is a technique for dictionary compression
                Large document collections contain many frequent and few rare terms !!!
                    // contains few frequent items and many rare items  &
                Gamma coding cannot encode the number zero. Is this a problem for compressing postings lists

            u5
                External sorting algorithms are used to sort lists which do not fit in main memory
                It is impossible to calculate the top-k documents for a query without completely reading 
                    the postings list of each query term   
                        // the idea of NRA ? + calcu. the topk docs. without completely reading the postings list 
                Why can the MapReduce algorithm as described in the lecture not be run in this case? &?? next 
                For optimal load balancing, the inverters in MapReduce must get segmented postings
                    files of similar sizes. For a new collection, the distribution is unknown , what should we do ? &
            ---   
            u6 &
                a) The key measure for a search engine is user happiness.
                b) The ğ¹Î² measure combines both, precision and recall, into one number.
                c) The 11-point interpolated average precision projects the precision-recall curve to a single number.
                d) The Mean Average Precision (MAP) is yet another measure for evaluating the result of one query.
                e) Pooling means experts manually judge the relevance of each document in the collection.
                f) The kappa value is 1 if two judges always agree and 0 if they never agree.
                g) Dynamic result summaries can be constructed efficiently from the positional inverted index

                How many relevant documents does the
                    result contain and at which ranks are they?
                kappa measure formua -
            u7 &
                a) The main motivation for relevance feedback and query expansion is to decrease recall.
                b) The basic idea of the Rocchio algorithm is to move the query vector towards the vectors of
                relevant documents and away from the vectors of irrelevant documents.
                c) Relevance feedback is widely available in web search engines, because users can easily
                understand it and are willing to â€œtuneâ€ their query.
                d) Pseudo-relevance feedback can lead to query drift. !!!
                e) In thesaurus-based query expansion, we add terms to a query which are semantically related to
                the query terms specified by the user.
                f) Thesauri can only be built manually, there is no way to build them automatically.

                In Rocchioâ€˜s algorithm, what weight setting for ğ›¼, ğ›½, ğ›¾ does a â€œFind pages like this oneâ€ search
                    correspond to? &
                Why is positive feedback likely to be more useful than negative feedback to an IR system? &
                Why might only using one non-relevant document be more effective than using several? !!! &
            u8
                a) Probabilistic IR is about estimating the probability that a document is relevant to a query.
                b) The odds of an event ğ´ is defined as ğ‘‚ ğ´ = ğ‘ƒ(ğ´)Î¤ğ‘ƒ(ğ´Ò§) .
                c) The classical Binary Independence Retrieval (BIR) model takes term frequencies into account.
                d) The BIR model assumes that terms appear independently from each other in the documents.
                e) The cluster-hypothesis states, term distributions differ between relevant and irrelevant documents.
                f) The system Okapi was a much simpler predecessor of the BIR Model.

                in RSV calcu. :
                    what can be omitted in order to save time? &
                    Which of the calculations done so far have to be done anew for the new query? &
                    The classical BIR model does not consider term frequencies.
                        Is there a way to take these into account
                        as well?
                derived the formulas for the Retrieval Status Value:  
                    We made the assumption that terms appear independent from each other in a document
                        Provide an example of two terms for which this assumption is likely to hold and not .
                    
            u9 & 
                a) A statistical language model (LM) assigns probabilities to strings of symbols from some alphabet.
                b) For every unigram LM, cats hunt mice and mice hunt cats have the same probability.
                c) Language models are completely unrelated to Markov chains.

                d) We use LMs in IR like this: (step 1) we derive a LM from each document, (step 2) we rank all
                    documents by the probability that their LM generates the query.
                e) From a vocabulary containing ğ‘‰ terms we can construct approximately ğ‘‰ ğ‘› ğ‘›-grams.

                f) A document containing ğ‘‘ tokens contains approximately ğ‘‘ ğ‘› ğ‘›-grams.
                    // contains d vocabularies not tokens
                g) In practice, language models in IR consider ğ‘›-grams with ğ‘› â‰¥ 3, i.e., at least tri-grams.
                h) The shorter the query, the more important is smoothing
                    // longer ? or no relationship ??
            u10
                a) The size of the web can easily be determined by crawling the web and counting the pages.
                b) In the context of web search, information needs are the only subclass of user needs.
                c) Shingling is a technique for detecting near duplicates of web pages.

                d) The average out-degree of all web pages is higher than their average in-degree.
                    why -- may be equal 
                    rem !!! &
                e) The PageRank algorithm ranks web pages by the number of occurrences of the query terms.
                f) The PageRank of a web page is query-dependent.

                g) The HITS-algorithm provides two scores per web page: an authority score and a hub score.
                h) The HITS-scores of a web page are query-dependen

                what happends as Î± becomes close to 1? &
            u11  
                a) Classification is about assigning a document to one (or more) out of several predefined categories.
                b) The accuracy of a classifier is always one minus its error rate.
                c) When using supervised learning for classification, we first train a classifier on unlabeled
                    documents.
                d) Overfitting means that a classifier is too general.
                e) With ğ‘›-times ğ‘˜-fold cross-validation, we partition a set of ğ‘› labeled documents into ğ‘˜ (nearly)
                    equally-sized subsets and for each subset ğ‘†ğ‘– we train a classifier on the documents in ğ‘†ğ‘– and
                    evaluate its performance by applying it to the documents in ğ‘†ğ‘–.
                f) Given a set of labeled documents, the sequential covering algorithm determines a set of rules for
                    rule-based text classification
                g) With probabilistic classification, a document ğ‘‘ is assigned to category ğ‘ğ‘– if the probability ğ‘ƒ ğ‘ğ‘– ğ‘‘)
                    is the maximum of the probabilities ğ‘ƒ ğ‘ğ‘˜ ğ‘‘) for all categories ğ‘ğ‘˜.
                h) Feature selection can reduce the training time of a classifier, but it cannot improve its quality.
                i) The Rocchio approach for vector-based classification assumes that the document vectors in each
                    category are close to each other, but distant from the document vectors in the other categories.
                j) The ğ‘˜NN classifier assigns a doc. ğ‘‘ to the ğ‘˜ categories whose centroid vectors are closest to ğ‘‘Ô¦.
                k) The idea of support vector machines (SVMs) is to separate the vector space using an optimal
                    hyperplane and to assign a document ğ‘‘ to one of two classes depending on whether ğ‘‘Ô¦ lies on the
                    one or on the other side of the hyperplane.
                l) The SVM approach can only be applied to linearly separable datasets

                difference between  Bernoulli and multinomial ? &
                    
        answer to ubung // or can ref. to the answer docs 
            u1 
                calculate (NOT y) first as a new postings list, which takes O(N) time. 
                    where N is the total number of documents in the collection
                bounded by the number of documents 
                not optimal , just a gready method 
                
            u2
                t t t t f t 
            u3
                f f t f 
            u4  
                see ubung pdf 
            u5
                partition by docid as well as term for very frequent terms
                using a machine, to find the distribution of terms starting from various alphabets

            u6
            u7 
                But, in practice, it has been shown to be most useful for increasing recall 
                might not be conveyed properly to the IR system which results in low precision output

            u8
            u9
            u10
            u11   
                multinomial model uses term freq. where Bernoulli uses document freq. to calcu. the probability.

                multinomial calcu. the probability by the number of occurrences of the in the collection 
                The Bernoulli model , on the other hand , just calculates the probability by the number of
                    documents which contain the word 

    ---                
    key concepts
        ----lev 1 Basic Part + Boolean  
            freq. error
                Stemming,wildcard queries,phrase quries,Spelling Correction x3
            ch1 -
                was ist IR? &
                what are unstructured text (usually text)? & -  
                what is data, knowledge and Information &
                IR system architecture::structure of IR system  ç¤ºæ„å›¾ p15  & -
                usage of IR Systems &  
                compare IR and databank & -  x6

            ch2 -
                Why is grep not the solution & p6 -- x3
                To answer the query with help of Term-document incident matrix ï¼Ÿ example & - p8
                why not used? & -
                why inverted ? Inverted Index & 
                what is posting list? & - p11
                posting list å»ºç«‹è¿‡ç¨‹ 3ä¸ªæ­¥éª¤ &
                dicå’Œpostingå¦‚ä½•å­˜å‚¨ &
                What is the best order for processing this query? & 
                how to skip ? &
                why skip lists are not used now  ? & x2
                Westlawç‰¹æ€§ & x2
                how to use Bi-word index to do Phrase Queries &
                Phrase Queries methods x3
                key idea of Extended Bi-words &
                why Bi words not used ? &
                what is Positional index &
                how can position index be used to Proximity search & 
                    advantages of positional index ::
                        1. doesnt require much space 
                        2. can do proximity search   
                what is the Combination scheme of those two  & -
                What are â€œgoodâ€ bi-words & -
            
            ch3
                how to determine lan in one text? & -
                what is & 
                    Term,Morphem,Inflection ,
                    Derivation,Kompositum,Noun Phrase (NP)
                promblems for reading words in IR? & *5
                what is &
                    Grammatical markings(POS Tags),Stemming,
                    Lemmatization,Case Folding,Stop words
                key idea of Sondex & 
                ways of Stemming & x3
                Diceâ€™s coefficient and Jaccard diff. &
                Successor Variety &
                    advantages of it & x2
                n-gram stemmers:  & ::statistics statistical 0.8
                Porter's Stemmer explain & -
                Stemming?or not &
                Dangerâ€™s of stemming &
                Part-of-Speech Tagging what is &
                how can we do Part-of-Speech Tagging & x2
                edit distance & 

            ch4
                è¯å…¸çš„æ•°æ®ç»“æ„what datastructures can be used for dictionaries &
                é€šé…ç¬¦æŸ¥è¯¢::Wildcard queries  x3 & - ä¾‹å­ï¼šï¼š[hel*o] look up  
                Wildcard queries vs Phrase Queries &
                why not used 
                permuted index and kgram æ¯”è¾ƒ & -
                    why not used ?
                Spelling Correction ä¸‰ç§ &
                it was expensive , so how to solve this ? 
                Issues when using Spelling correction  & Problems 
            
        ----lev 2 Basic Rank
            ch5
                compare: rank or not & -
                Why rank & x1
                Jaccard coefficient to Scoringï¼Œhow? & 
                    problems: & -
                    https://datascience.stackexchange.com/questions/15862/how-to-compute-the-jaccard-similarity-in-this-example-jaccard-vs-cosine
                Cosine similarity and Jaccard similarity differ.
                what is tf idf &
                tf-idf weighting , how & 
                tf-idf formular & -
                tf-idf compare & - those two 
                under what condition weight is 0? & x2
                The vector space models before and now  & x3
                why Euclidean distance is a bad idea & -
                differ. weghts &
                    Inc.Itn 
                    Itn.bnn
                    åˆ†åˆ«ä»£è¡¨å•¥æ„æ€ï¼Œå…¬å¼åˆ—å‡ºæ¥   
            ch6
                why compression & x2 -
                what is Lossy vs. lossless compression & - -
                Heapsâ€™ law mean?
                the meaning for IR systems &
                zipf Law mean ï¼Ÿ
                Dictionary as a string & ä¸»è¦saveçš„æ˜¯å“ªé‡Œçš„ç©ºé—´
                why 3B here needed &
                Dictionary as a string with blocking & ä¸»è¦saveçš„æ˜¯å“ªé‡Œçš„ç©ºé—´
                ä¸ºå•¥blockä¸èƒ½å¤ªå¤§ & 
                how to Tradeoff &
                Front coding & 
                how to store gaps & x2
                Variable length code & 
                Variable byte code  &
                Gamma Codes for gap encoding & calcu -p39 13 å››ä¸ªæ­¥éª¤ &
                Gamma:some advantages & - explain - x2
                Gamma seldom used in practice & - explain
                
                
            ch7
                External Memory Sort &
                ç¼©å†™çš„å«ä¹‰ &
                åŸºäºå—çš„æ’åºç´¢å¼•æ–¹æ³•
                    Blocked Sort-Based Indexing (BSBI) idea & -  algo & - 
                å†…å­˜å¼å•éæ‰«æç´¢å¼•æ„å»ºæ–¹æ³•
                    Single-Pass In-Memory Indexing (SPIMI) algo & - 
                Cachingç­–ç•¥ x2 &
                zwei Arten von Rechnerâ€Knoten :  & - two types of nodes 
                advantages or dis- of distributed index construction & - NEXT
                Mathods of Distributing &
                why can parallel? & - 
                MapReduce Architecture & - NEXT 
                Delta-Index: how it works ? & when put the delta index to Disk: &
            
            ch8
                Term-at-a-Time (TAAT): and DAAT &
                NRA ï¼š No Random Accesses function explain & -
                    ç»“æŸæ¡ä»¶ &
                    p29å…¬å¼ & - x5 mink,unseen,worst,best,high(t)

            ch9
                how to Measure the happiness ? & - p5  explain x4 2+2
                Precision and Recall
                    å…¬å¼ p9 & -
                F-Measure Precision/recall tradeoff
                    formula & - p13
                Cost-based Measure & calcu
                Accuracy ï¼šï¼šTP TN å ç”¨çš„æ¯”ä¾‹ 
                    å…¬å¼ & -
                    calcu &
                Macro average (precision) and Micro average (precision) 
                    p19 calcu & å…¬å¼æœ‰é—®é¢˜? è²Œä¼¼æ²¡æœ‰ 
                PR curve å‘ˆé”¯é½¿å½¢çš„åŸå› ? & -
                what are included in benchmarks & x3
                Kappaç»Ÿè®¡ why ? &
                Poolingç­–ç•¥--è¯„ä»·æ‰€æœ‰æ–‡æ¡£å·¥ä½œé‡æ˜¯å¾ˆå¤§çš„ï¼Œå¯ä»¥åªè¯„ä»·æ£€ç´¢çš„ &
                A/B Testing &
                what to descreption in summaries &
                ç”ŸæˆåŠ¨æ€æ‘˜è¦çš„ç›®æ ‡æ˜¯é€‰å‡ºæ»¡è¶³å¦‚ä¸‹æ¡ä»¶çš„ç‰‡æ®µ &  p49


        ----lev 3 Ranking system
            ch10
                four different examples with picture & - for user feedback -next
                idea of Rocchio Algorithm to do feedback & 
                    centroid , opt. query formular &
                    æœ‰å›¾æ¥è§£é‡Šè¿™ä¸ªå…¬å¼ & -ï¼š
                Used in practice &
                    what effect parameter has & -
                Positive versus negative feedback & -
                When Does Relevance Feedback Work? & ??
                Evaluation of feedback:how many times is enough to do that &
                problemsï¼Œä¸èµ·ä½œç”¨çš„åŸå›   & - x3
                ä¸ºç›¸å…³åé¦ˆé—®é¢˜ &
                but it workd well? &
                what can we do to é—´æ¥ç›¸å…³åé¦ˆ &
                æŸ¥è¯¢æ‰©å±•(Query expansion) methods & x3 
                åŸºäºç»Ÿè®¡å­¦çš„ç»“æœï¼Œç›¸å…³æ€§çŸ©é˜µ
                    calcu & - NEXT p48 

            ch11
                äº‹ä»¶çš„ä¼˜åŠ¿ç‡ï¼ˆ oddsï¼‰ & f
                çŠ¶æ€æ£€ç´¢å€¼ Retrievalstatuswert ï¼ŒRetrieval status value &    NEXT TUI 
                    å…¬å¼
                    æ„ä¹‰ & 
                Test auf UnabhÃ¤ngigkeit formula &
                æ¦‚ç‡æ£€ç´¢æ¨¡å‹ idea &
                äºŒå€¼ç‹¬ç«‹æ¨¡å‹BIR & å…¨ç§°
                Okapi formular & calcu & 
                æ–‡æ¡£è¯„åˆ†çºµå‘æ¯”è¾ƒï¼š & -
                æ¦‚ç‡IRä¼˜ç¼ºç‚¹ & - NEXT explain x2 x2 

            ch12
                è¯­è¨€æ¨¡å‹ idea & -
                what is language of LM &
                åŒåœˆèŠ‚ç‚¹å¯¹åº”çš„æ˜¯ &
                æè¿°æœ‰é™è‡ªåŠ¨æœº EA & - explain
                è®¡ç®— P(Katzen fangen MÃ¤use) & - 
                äºŒå…ƒè¯­è¨€æ¨¡å‹ä¸ºå•¥ä¸ç”¨ & - exp
                kann man LM durch Markow-Ketten darstellen & - Horizontï¼ˆå‘å‰çœ‹çš„è§†é‡ï¼‰?
                retrival function p20å…¬å¼ & -
                Jelinek-Mercer Smoothing & -
                Multiplikation kleiner Werte FÃ¼hrt zu Rechenungenauigkeiten & - how to handle &
                LMs vs. BIR x1
                    è”ç³» & - NEXT
                    åŒºåˆ« & - NEXT
                LMs vs. Vector-Space-Modell
                    è”ç³» & - NEXT
                    åŒºåˆ« & - NEXT

            ch13  web basic , +++++++++
                ä¸ºä»€ä¹ˆ Webçº·ç¹æ‚ä¹±ã€å˜åŒ–è¿…é€Ÿ & x1
                web IRç‰¹ç‚¹ & x3
                é™æ€é¡µé¢ä¸åŠ¨æ€  & - exp  
                magure the size of the web & Webè§„æ¨¡ä¼°è®¡ &
                æ•´ä¸ªWebæœ‰å‘å›¾ç»“æ„ï¼Ÿ & - x6
                ä½œå¼Šé—®é¢˜ï¼šå‡ ç§æ–¹æ³•æ’åä½œå¼Šï¼Ÿ & - x2
                googleçš„ç”¨æˆ·ä½“éªŒç‰¹ç‚¹ï¼š & -  x3
                æ™®é€šçš„ Web æœç´¢æŸ¥è¯¢ä¼¼ä¹å¯ä»¥åˆ†æˆå“ªä¸‰å¤§ç±»ï¼Ÿ  & -
                Mercator é‡‡é›†å™¨ äº”ä¸ªæ¨¡å— &
                pptä¸­ä¾‹å­ & - ç†è§£ï¼Œç»™å›¾ç”»å‡ºæ¥è·¯å¾„
                webä¸­çš„é‡å¤é—®é¢˜ & x2
                ç½‘ç»œæœç´¢çš„å…·ä½“å½¢å¼ç‰¹ç‚¹ & 
                ç½‘ç»œæ–‡æ¡£ç‰¹ç‚¹ï¼Œä¸»è¦çš„å››ä¸ªç‰¹æ€§ & -
                Crawlingï¼Œä¸¤ä¸ªç‰¹ç‚¹ï¼Ÿ & 
                How are ads ranked  & - exp x2+n
                æ¬¡é«˜ä»·æ‹å– & and calcuã€‚ p47 --second price auction 
                    Anybody can participate and bid on keywords
                ä¸»è¦çš„ä¸¤ä¸ªé—®é¢˜ 
                    Keyword arbitrage
                    Violation of trademarks
                å¸¸è§çš„SpamæŠ€æœ¯ & x2


            ch14
                é“¾æ¥åˆ†æçš„æ„ä¹‰ & - 
                PageRankè¿‡ç¨‹å‡è®¾ï¼š & - x2
                é¢å‘ä¸»é¢˜çš„ PageRank:ç®€å•åšæ³• &
                å¤„ç†æ··åˆçš„æƒ…å†µï¼Ÿ &
                pagerank ç‰¹ç‚¹ & - x2
                HITSå¾ªç¯è¿­ä»£å…¬å¼ & - 
                    è®¡ç®—ä¼˜åŒ–ï¼Ÿ & - è¶…é“¾å¯¼å‘çš„ä¸»é¢˜æœç´¢
                    idea & - 
                    é—®é¢˜ï¼š  & - x1
                    hits ç‰¹ç‚¹ & - x2

        ----lev 4 ML classification
            ch15  
                åœ¨åƒåœ¾é‚®ä»¶å¤„ç†ä¸­ï¼Œæœ‰å“ªäº›åŸºæœ¬çš„å¤„ç†æ–¹æ³• & p7  x2
                Maschinelles Lernenï¼Œhow it works  &ç”»å›¾ 
                *æ€»ä½“ï¼šåˆ†ç±»çš„è¯„ä»· & x3 + 2
                    basic 
                        Accuracy and  Error Rate
                        Recall, Precision and F1 
                        formula & -
                    erweitern
                        Confusion Matrix  p27 pic ç”»å›¾ 
                        cross validation x3
                *Undersampling , oversampling,Hybrid-Verfahren & -- deal with data skew
                ä¸‹é‡‡æ ·æ–¹æ³•è¯´å‡ºåå­—å°±å¯ä»¥ & x4
                Oversampling & x2 è¯´å‡ºåå­—ï¼Œç®€ç§°ï¼Œå…¨ç§°
                Feature-Scaling x3
                *Feature-Auswahl x3 &

            ch16 -ç®—æ³•ç»†èŠ‚ï¼Œå¦‚ä½•ç»§ç»­ä¼˜åŒ– 
                Form der Regeln  &
                *Evaluation von Regeln
                    Grundlegende MaÃŸe,ä¸¤ä¸ªæŒ‡æ ‡ å…¬å¼ &,è®¡ç®— & p15
                        å¤ªå¤§å¤ªå°ï¼Ÿ
                    Drei QualitÃ¤tsmaÃŸe ,formular &,p21 ?? NEXT
                        combine the two values 
                        eg. & p26
                *Sequential Covering,
                    idea &,
                    formular &,eg p33 &&&,
                    *ç®—æ³• p30 
                    Nachbearbeitung x2+ x2
            ch17 -å…¬å¼æœ‰ç‚¹è´¹åŠ²
                *è´å¶æ–¯åˆ†ç±»ï¼š
                    why NaÃ¯ve & x1
                    why not Naive & x2
                Multinomiales NaÃ¯ve Bayes ï¼šidea & formuar  & calcu & 
                    ä¸‰ä¸ªå±‚çº§çš„å…¬å¼ 
                        *ç¬¬ä¸€å±‚æ¬¡-
                        åŸºæœ¬cj d, p ï¼Œ tk cj ä¼°è®¡ï¼Œ gamma (d) 
                        how to de smoothingï¼Ÿ
                    +ä»£ç å¯¹åº” & æ•°ç»„...
                    +MLå®é™…ä¸Šå­˜å‚¨çš„æ˜¯ä»€ä¹ˆï¼Œå­¦ä¹ çš„æ˜¯å•¥
                    +term freq =1 çš„æ—¶å€™å°±æ˜¯Bernoulli NaÃ¯ve Bayesï¼Ÿ false ! why 
                Bernoulli NaÃ¯ve Bayes ï¼š idea & formuar  & 
                    ä¹Ÿæ˜¯ä¸‰ä¸ªå±‚æ¬¡
                    smoothingï¼Ÿ why + 2 in the denominator 
                    Anmerkungen & x2 advantages 
                Aktuelle Anwendung & x1
            ch18
                *Klassifikation im Vektorraum
                    Vektorraum:vector space
                    exp &
                Rocchio Klassifikator 
                    idea & 
                    formuar  & 
                    *Probleme bei Rocchio & x2
                kNN 
                    process & x2 
                    *Wahl von k? & x2-3
                    å˜ç§ &  x2
                
            ch19 -svm ç»†èŠ‚
                Darstellung Hyperebene mit Ebenengleichung ::p9
                Vorgehen bei mehr als zwei Kategorien ? Multiclass SVMs?,exp & p10 x3steps
                    ::when there are more than two classes x2
                *Trainingsphase:Perzeptron
                    what is R ,ita &
                    verfahren exp & x3steps
                    if learn rate is large ? &
                    formula of perceptron &
                        alpha factor in the formular what is that &
                    dual form &
                    what have we learned ? &
                SVM
                    *SVM Linearer Fall
                        è·ç¦»çš„è¡¨è¾¾å¼ &
                        Einfach Verfahren &
                            formular p30 & -
                                it can be solved with Lagrange multiplier or quadratic  programming
                                    -Well-studied solution algorithms
                                why larger than 1 ? (constraint formula )
                            //Duale Formulierung & -
                        Beispiel p33 &
                        

                    *Erweitertes Optimierungsverfahren
                            using slack variable in SVMï¼šï¼šå¼•å…¥æ¾å¼›å˜é‡
                                formular & p38
                                Einfluss von C exp & -
                                    tradeoff parameter (chosen by cross-validation)
                                what is m :: number of slacks?
                                condition ? &

                    *è¯´åå­—-Kernel Trick--Nichtlineare Entscheidungsgrenzen
                        key idea &
                        kernel types & x4
                            RBF Kernel--GauÃŸâ€˜scher Kernel
                            Polynomial Kernel
                            Sigmoid Kernel
                            Cosinus Similarity Kernel
                            Chi-squared Kernel
                        how to choose ? 10k
        
        ---
    
    ---
    Lan training
        base::
            generic storage...
                similar. general
            math 
                calculus..
                Introduction to Calculus
                    regular polyhedra 
                    Sets, Functions, Graphs and Limits
                    Elementary Set Theory, Subsets, Set Operations, Coordinate Systems
                    The Exponential and Logarithmic Functions
                    The Hyperbolic Functions
                    Symmetry of Functions, Translation and Scaling of Axes,
                    Equations of Lines
                    Parabola, Ellipse, Hyperbola
                    Conic Sections in Polar Coordinates
                Differential Calculus
                    The Derivative of
                    take the derivative of both sides 
                    And then we take the derivative.!!!!
                        -r now , we are going to take the derivativ 
                Integral Calculus
                    Integrate 
                    Table of Integrals
                    I will integrate the function on that region !!!!
                        -r now , we are going to integrate this function on the rigion 
                    Method of Partial Fractions
                    The Definite Integral
                    Improper Integrals
                    Cylindrical Coordinates
                applications of 
                    Spring-mass System
                    Damping Forces
                    Mechanical Resonance

        ch1 
            Information retrieval 
                is the task, 
                        given a set of documents and a user query, 
                        finding the relevant documents
                        // find the relevant docs , according to the collection and the queries 
                Automated information retrieval systems are used to reduce
                    what has been called information overload
                        // why should we gen. IR system : there are huge amount informaiton that we should manage 
                An IR system is a software that provide access to books, 
                    journals and other documents, stores them and manages the document.
                        // we can access to these books , using the system 
                Web search engines are the most visible IR applications. 
                    // Medical Imaging is the most visible application in CG 
                information retrieval process begins when
                Queries are formal statements of information needs,
                search strings in web search engines
                with different degrees of relevancy.
                opposed to classical SQL queries of a database
                compute a numeric score on how well each object in the database matches the query
                    // the key idea of our sys. is to compute a numeric score to ranking the doc. 
                        so as to provide a very quick access
                The process may then be iterated if the user wishes to refine the query
                Now the world has changed, and hundreds of millions of people engage
                    in information retrieval every day when they use a web search engine or search their email
                    web search engine:: pronu.
                

            unstructured text
                does not have a pre-defined data model
                    // has no data model , when it compared to data in databank 
                typically text-heavy
                an ambiguous word/term/statement 
                This results in irregularities and ambiguities that make it 
                    difficult to understand using traditional programs 
                80-90% of all potentially usable business information may originate in unstructured form
                Other sources have reported similar or higher percentages of unstructured data
                majority of that will be unstructured
                    // xmajority of data that in our daily life are unstructured data 
                The earliest research into business intelligence focused in on unstructured textual data
                    // at the very begining , ir was used into business intelligence
                the extraction and classification of unstructured text
                Approaches in medicine and biomedical research
                biomedical documents include self-organizing map 
                    approaches for identifying topics among documents
            

        ch2 
            what is a model 
                Mathematical models are used to study the properties of 
                    the process, draw conclusions, make predictions
                derived from .. 

            boolean re. is something that very precise
            Boolean model of information retrieval (BIR)
                The BIR is based on Boolean logic and classical set theory 
                the user's query are conceived as sets of terms. 
                    //  bag of words model 
                Retrieval is based on whether or not the documents contain the query terms. 
                    // has ignore the term freq. and just check if the term is included in the doc.
            why inverted 
                postings file or inverted file
                    a mapping from content
                    named in contrast to a forward index, which maps from documents to content
                        // 
                            it is named in contrast to forward index 
                            forward index maps froms documents to content 
                            inverted maps from words to documents 
                    The purpose of an inverted index is to allow fast full-text searches
                        //
                            to enable fast full text research , we generate an inverted index for the collection 
                        It is the most popular data structure used in document retrieval systems
            Phrase Queries
                A word query is a query on a word or phrase
                    // a phrase can be 2 or more words :: the wto black albama ...
                search for documents containing an exact sentence or phrase
                searching for a certain string of text
                using quotation marks (") around a specific phrase to 
                    indicate that you want to search for instances of that search query
                    //
                        there should be quotation marks around this string 


        ch3 
            a morpheme is the smallest meaningful unit in a language
                // meaningful unit 
            stemmer 
                handling of word endings by reducing the words to their word roots  
                    // find the root of words 
                Stemming broadens our results to include both word roots and word derivatives
                        to improve recall
                    // stemmers are used to increse recall in ir sys.
                A stemming algorithm is an algorithm that converts a word to a related form
                conversion of plurals to singulars. 
                It is the most effective and widely used
                it can only be applied to text in the English Language
                    // the porter stemmer
                the user specifies a word in a query but only a variant of this word is present in a relevant document
                This problem can be partially overcome with the substitution of stems for the words
                A stem is the portion of a word that is left after the removal of its affixes (i.e., prefixes and suffixes)
                improving retrieval performance
                affix removal, table lookup, successor variety
                    determination of morpheme boundaries 
                and n-grams.
                might require considerable storage space
                reduce the size of indexing files
                In summary, the successor variety stemming process has 3 parts:
                develop a stemmer that required little or no human processing.
                    // the systems are considered to requiring little human processing 
                    // sys should do little interaction with human 
                it facilitates the construction of an attractive user interface
                to test the correctness of our work
                    // to check if it is correct 
                we have achieved an 80% level of correctness
                Several advantages of the Successor Variety Algorithm can be observed
                    --  without the need to use a dictionary
                    -- it is basically (domain independent)
                Another advantage is that it can be used in several domains; it is basically (domain independent).
                National Computer Conference and Exhibition
            Part-of-Speech 
                The part of speech indicates how the word functions in 
                    meaning as well as grammatically within the sentence.
                    // find the function of the words we are trying to tag in the sentance .
            
        ch4 
            encyclopedia
            Wildcard character
                a wildcard character is a kind of placeholder represented by a single character
                an asterisk 
                can be interpreted as a number of literal characters or an empty string
                used in Regular expressions
                doc* matches doc and document but not dodo
            k gram
                an n-gram is a contiguous sequence of n items from a given sample of text or speech
                n-grams may also be called shingles
                a type of probabilistic language model
                n-gram models are widely used in statistical natural language processing.
            Spaceâ€“time tradeoff
                an algorithm or program trades increased space usage with decreased time
                Using stored knowledge or encoding stimuli reactions as "instincts" 
                    in the DNA avoids the need for "calculation" in time-critical situations
                More specific to computers, look-up tables have been implemented since the very earliest operating systems
            Spelling Correction
                spell checker :: ot check the spell of a word if it is correct
                levenstein distance 
                k gram 
                Context-sensitive process 
                a software feature that checks for misspellings in a text.
                n-grams, to recognize errors instead of correctly-spelled words
                with phonetic information
                Gorin wrote SPELL in assembly language, for faster action
                available on mainframe computers
                PCs with sufficient memory.
                For example, the concatenation of "snow" and "ball" is "snowball
                string theory, string concatenation is a primitive notion.
            Recreational mathematics

        ch5  
            Jaccard with k-Grams
                Jaccard similarity
                    //
                        k-Grams is just a simple concept and can be used in many algos. 
                        Jaccard measures simularity of tow sets using k gram 
                is a statistic used for comparing the similarity and diversity of sample sets. 
                divided by the size of the union of the sample sets
                If A and B are both empty, we define J(A,B) = 1.
                Intersection over Union
                ratio of the size of the symmetric difference 
                Weighted Jaccard similarity and distance
                Probability Jaccard similarity and distance
                Tanimoto similarity and distance
                Paul Jaccard
                Cosine similarity is for comparing two real-valued vectors, 
                Jaccard similarity is for comparing two binary vectors (sets).



        ch6  
            zipf Law 

        ch7 
        ch8 
        ch9 
            Kappa statistic
                a statistic which measures inter-rater agreement
                it is conceptually simpler to evaluate disagreement between two raters
                The seminal paper introducing kappa as a new technique was published by Jacob Cohen 
                between two raters only,cannot measure the aggrement amoung more than two raters 
                Fleiss' kappa
                The Fleiss kappa, however, is a multi-rater generalization of Scott's pi statistic
                Kappa is also used to compare performance in machine learning
                Statistical significance
                type of intraclass correlation
